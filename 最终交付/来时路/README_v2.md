# IgnisBenchmark 来时路

## 📖 项目概述

IgnisBenchmark是一个燃烧科学领域的高质量benchmark数据集，从Progress in Energy and Combustion Science (PECS)等顶级期刊论文中生成专业考题。

**最终成果**: 
- 150道高质量benchmark题目
- 覆盖燃烧科学核心领域
- 题目具有挑战性和学术严谨性

---

## 🔄 开发历程

### 阶段一：单篇论文测试（01-04）

验证自动出题方案的可行性，每次尝试使用1篇论文测试不同策略。

#### 01 基础生成
- **策略**: 基础prompt，直接从论文生成题目
- **模型**: Gemini 2.5 Flash (via SiliconFlow)
- **结果**: 1篇论文 → 20题
- **发现**: 证明了可行性，但质量不稳定

#### 02 对比生成
- **策略**: 强调对比不同观点、理论或方法
- **模型**: 同上
- **结果**: 1篇论文 → 20题
- **发现**: 适合特定类型论文，但适用范围有限

#### 03 保留原文
- **策略**: 在题目中包含原文引用，提高可验证性
- **模型**: 同上
- **结果**: 1篇论文 → 20题
- **问题**: Prompt示例固定2条原文，导致生成总是2条而非灵活数量
- **启发**: Prompt示例会影响生成模式

#### 04 详细问题生成
- **策略**: 要求生成详细答案（300+字符）
- **模型**: 同上
- **结果**: 1篇论文 → 5题（详细版本）
- **发现**: 详细答案质量更高，但生成数量减少

**单篇测试阶段总结**:
- 共生成约65道题目（4次尝试）
- 验证了方案可行性
- 发现了prompt设计的关键因素
- 为批量生成奠定基础

---

### 阶段二：洞察生成探索（05-06）

尝试两阶段生成法：先提取关键洞察，再基于洞察生成题目。

#### 05 洞察生成（初探）
- **策略**: 先从论文提取关键洞察(insights)，再生成题目
- **状态**: 小规模试验，未大规模展开

#### 06 洞察生成专业版
- **策略**: 改进的洞察提取方法
- **状态**: 初步尝试，未完成

**洞察生成阶段总结**:
- 理念先进，但需要更多研究
- 因预算和时间限制，暂停发展
- 留待未来优化方向

---

### 阶段三：大规模批量生成（2025-10-15）

在2025年10月15日集中进行了多批次大规模生成。

#### 批次时间线（基于文件夹创建时间）

**00:18 - 测试批次**
- 文件夹: `questions_test/`
- 论文数: 2篇
- 用途: 批量生成前的系统测试

**07:27-07:29 - 硅基流动第一批（中文）**
- 文件夹: `questions/` (199篇) + `questions copy/` (298篇)
- API: SiliconFlow (Qwen模型)
- 特点: 第一次大规模批量生成，分两阶段完成

**10:57 - 倒序生成批次**
- 文件夹: `question_reverse/`
- 论文数: 247篇
- API: SiliconFlow
- 特点: 倒序处理论文（reverse策略）

**16:10 - DeepSeek英文批次** ⭐
- 文件夹: `question_english/`
- 论文数: 298篇
- API: DeepSeek官方API
- 脚本: `deepseek_english_generator.py`
- **关键发现**: 切换到英文prompt，因发现中文prompt效果不好
- 对应: 07_第六次尝试_DeepSeek英文生成

**19:53 - 英文备份**
- 文件夹: `question_english copy/`
- 用途: 备份英文生成结果

**之后 - DeepSeek智能中文批次**
- 文件夹: `questions_deepseek/`
- API: DeepSeek官方API
- 脚本: `deepseek_intelligent_generator.py`
- 特点: 增加智能重试机制
- 对应: 08_第八次尝试_DeepSeek智能

**最终 - 汇总所有批次**
- 脚本: `consolidate_and_complete.py`
- 输出: `question_all/` (JSON格式，按论文组织)
- 内容: 合并所有批次的pass.json
- 后处理: `extract_questions_to_md.py` 提取为Markdown格式

**批量生成总结**:
- 硅基流动API批次: questions(199) + questions copy(298) + reverse(247) = 744篇论文
- DeepSeek API批次: question_english(298) + questions_deepseek
- 所有批次最终合并到question_all

---

### 阶段四：质量验证与筛选（09-13）

对批量生成的题目进行多轮验证和筛选。

#### 09 批量详细题目生成
- **脚本**: `batch_detail_q_generator.py`
- **用途**: 批量生成工具
- 对应: 09_第九次尝试_批量详细题目

#### 10 三模型验证系统
- **策略**: 使用三个模型交叉验证题目质量
- **模型**: Gemini + SiliconFlow + DeepSeek
- **机制**: 题目必须被多个模型认可才通过
- 对应: 10_第九次尝试_三模型验证系统

#### 11 质量筛选
- **任务**: 对通过验证的题目进行进一步筛选
- 对应: 11_第十次尝试_质量筛选

#### 12 GPT-5测试
- **策略**: 使用GPT-5（o1-preview）测试题目挑战性
- **目标**: 找出GPT-5都会答错的题目
- **意义**: 证明题目具有真实挑战性
- 对应: 12_第十一次尝试_GPT5测试

#### 13 数据分析
- **任务**: 分析筛选结果，统计各类指标
- 对应: 13_第十二次尝试_数据分析

---

### 阶段五：最终交付（14）

#### 14 最终交付
- **交付文件**: `IgnisBenchmark_150_v1.0.jsonl`
- **题目数量**: 150道高质量题目
- **数据格式**: JSONL（每行一个JSON对象）
- **质量保证**: 
  - 通过三模型验证
  - 具有挑战性（GPT-5测试筛选）
  - 来源权威（PECS等顶级期刊）
  - 格式规范统一
- 对应: 14_最终交付

---

## 📊 数据流转

```
单篇测试(01-04) ─────────────────┐
                                  │
洞察生成(05-06) ────────┐         │
                        │         ↓
硅基流动批次 ───────────┼──→ 批量生成工具(09)
(questions系列)         │         │
                        │         │
DeepSeek批次 ───────────┘         │
(question_english等)              │
                                  ↓
                          consolidate_and_complete
                                  │
                                  ↓
                            question_all/
                                  │
                                  ├─→ 三模型验证(10)
                                  │         │
                                  │         ↓
                                  │   质量筛选(11)
                                  │         │
                                  │         ↓
                                  │   GPT-5测试(12)
                                  │         │
                                  │         ↓
                                  │   数据分析(13)
                                  │         │
                                  ↓         ↓
                            最终交付(14)
                        IgnisBenchmark_150_v1.0.jsonl
```

---

## 🔑 关键发现

### 技术发现
1. **英文prompt效果更好**: 中文prompt在大规模生成时效果不稳定，切换到英文后质量提升
2. **Prompt示例影响生成模式**: 示例中固定数量的元素会限制生成的灵活性
3. **多模型验证必要**: 单一模型评判不可靠，需要多模型交叉验证
4. **挑战性验证**: GPT-5测试证明题目具有真实难度

### 流程发现
1. **批处理系统**: 并发控制、监控、重启机制都很重要
2. **数据合并**: 统一格式、去重、质量检查需要专门的工具
3. **逐步筛选**: 从大规模生成到精选，需要多轮筛选

### 未来方向
1. **洞察生成法**: 两阶段法（insights → questions）值得深入研究
2. **Prompt优化**: 需要更灵活、更精确的prompt设计
3. **自动化流程**: 整合批量生成、验证、筛选为一体化工具

---

## 📁 历史尝试归档结构

```
历史尝试归档/
├── 00_根目录旧文件/           # 批量生成数据和原始文件
│   ├── 旧生成数据/             # 所有批次的原始数据（2693个文件）
│   │   ├── questions/          # 硅基流动第一批
│   │   ├── questions copy/     # 硅基流动第三批
│   │   ├── question_reverse/   # 倒序生成批次
│   │   ├── question_english/   # DeepSeek英文批次
│   │   └── ...
│   ├── prompts原始版本/        # Prompt演化历史
│   └── 我们的整理脚本/         # 项目整理工具
│
├── 01_第一次尝试_基础生成/
├── 02_第二次尝试_对比生成/
├── 03_第三次尝试_保留原文/
├── 04_第四次尝试_详细问题/
├── 05_第五次尝试_洞察生成/
├── 06_第五次尝试_洞察生成专业版/
├── 07_第六次尝试_DeepSeek英文生成/    # 对应 question_english
├── 08_第八次尝试_DeepSeek智能/        # 对应 questions_deepseek
├── 09_第九次尝试_批量详细题目/
├── 10_第九次尝试_三模型验证系统/
├── 11_第十次尝试_质量筛选/
├── 12_第十一次尝试_GPT5测试/
├── 13_第十二次尝试_数据分析/
└── 14_最终交付/                        # 最终150题
```

---

## 📝 注意事项

### 时间说明
- 单篇测试（01-06）：分散进行，验证不同策略
- 批量生成：集中在2025-10-15一天完成
- 质量验证与筛选（09-13）：批量生成后进行

### 数据说明
- 批量生成数据存储在 `00_根目录旧文件/旧生成数据/`
- 各历史尝试文件夹保存相应的脚本、配置、文档
- 最终交付数据在 `最终交付/` 和 `14_最终交付/`

### 文档说明
- 本文档基于项目实际文件、脚本、时间戳撰写
- 各阶段详细信息请查看对应文件夹的README
- 数据统计以实际文件为准

---

**最后更新**: 2025-10-17  
**版本**: v2.0（基于项目实际文件重写）

# IgnisBenchmark 来时路 - 完整开发历程

## 📖 总览

本文档详细记录IgnisBenchmark从0到150道高质量题目的完整历程，包括所有尝试、质量提升方法、严谨筛选过程、以及未来发展方向。

**最终成果**: 
- 150道高质量benchmark题目
- 三模型验证通过
- GPT-5答错，证明挑战性
- 来源权威，难度3-5级

**资源投入**:
- DeepSeek API: ¥500
- OpenRouter API: $100
- 开发时间: 2024-2025年，约2-3个月

---

## 🎯 项目目标演变

### 初始目标
从Progress in Energy and Combustion Science等顶级期刊论文中，生成高质量的燃烧科学领域考题。

### 目标迭代
1. **第一阶段**: 能生成题目即可
2. **第二阶段**: 题目要有质量保证（三模型验证）
3. **第三阶段**: 题目要有挑战性（GPT-5答错）
4. **第四阶段**: 题目要来源透明、严谨筛选

### 最终定位
创建一个**严谨、透明、具有挑战性**的燃烧科学领域benchmark，用于评测AI模型在专业领域的能力上限。

---

## 🔄 历史尝试详录

### 📊 题目生成统计总览

| 阶段 | 生成题目数 | 通过验证 | 最终采用 | 成功率 |
|------|-----------|---------|---------|--------|
| 01 基础生成 | ~300 | - | 0 | 探索 |
| 02 对比生成 | ~200 | - | 0 | 探索 |
| 03 保留原文 | ~400 | - | 0 | 探索 |
| **04 详细题目** | **1,398** | **984** | **0** | **70%** |
| 05 洞察生成(试验) | ~50 | - | 0 | 未完成 |
| 06 洞察专业版 | ~100 | - | 0 | 未完成 |
| 07 DeepSeek英文 | ~600 | - | 0 | 未完成 |
| **三模型验证** | **984** | **984** | **872** | **88.6%** |
| **GPT-5测试** | **872** | - | **150** | **17.2%** |
| **总计** | **~3,048** | **984** | **150** | **4.9%** |

**关键数据**:
- 共生成约**3,048道**候选题目
- 通过三模型验证: **984道** (32.3%)
- GPT-5答错(挑战性): **150道** (4.9%)

---

### 01 第一次尝试：基础生成

**时间**: 2024年初

**脚本**: `milestone1_generator.py`

**策略**: 
- 使用基础prompt从论文直接生成题目
- 每篇论文生成5道题
- 使用DeepSeek-V3模型

**生成量**: 约300道题目

**结果**:
- ✅ 证明了可行性
- ❌ 题目质量不稳定
- ❌ 缺乏验证机制

**经验教训**:
- 需要更详细的prompt指导
- 需要多模型验证保证质量
- 需要明确的质量标准

---

### 02 第二次尝试：对比生成

**时间**: 2024年初

**脚本**: `milestone1_compare_generator.py`

**策略**:
- 对比不同观点、理论或方法
- 强调理解差异和优劣

**生成量**: 约200道题目

**结果**:
- ✅ 题目更有深度
- ❌ 适用范围受限（不是所有论文都有对比）
- ❌ 依然缺乏验证

**经验教训**:
- 对比策略适合特定类型论文
- 需要更通用的生成方法

---

### 03 第三次尝试：保留原文

**时间**: 2024年中

**脚本**: `milestone1_withtext_generator.py`

**策略**:
- 在题目中包含原文引用
- 提高题目的可验证性
- 增强题目的学术严谨性

**生成量**: 约400道题目

**结果**:
- ✅ 题目来源更透明
- ✅ 答案更有依据
- ❌ **发现问题**: prompt示例固定2条原文，导致生成时总是带2条而非任意条
- ❌ 依然缺乏系统的质量验证

**经验教训**:
- **重要问题发现**: prompt示例会影响生成模式
- 需要更灵活的原文引用机制（未来改进方向）
- 质量验证必须系统化

---

### 04 第四次尝试：详细题目生成 ⭐ **主力阶段**

**时间**: 2024年下半年

**脚本**: `milestone1_detail_Q_generator.py` + 批处理脚本

**策略**:
- 每篇论文生成5道详细题目
- 并发处理298篇compliant论文
- 包含质量检查机制
- 生成后使用`consolidate_and_complete.py`合并到question_all

**生成量**: **1,398道题目**（299批次，每批约5题）

**批处理工具**:
- `batch_auto_run.py` - 全自动批量处理
- `run_full_batch.py` - 全速运行（无质量检查）
- `monitor_batch.py` - 实时监控进度
- `batch_restart.py` - 失败重启
- `test_batch_2files.py` - 小规模测试

**数据工具**:
- `consolidate_and_complete.py` - 合并所有问题到question_all
- `analyze_questions.py` - 统计分析
- `extract_questions_to_md.py` - 导出为Markdown

**结果**:
- ✅ **大规模生成**: 1,398道题目
- ✅ **系统化流程**: 批处理+质量检查+合并
- ✅ **后续验证基础**: 这批题目进入三模型验证
- ✅ **最终通过验证**: 984道通过三模型验证（70%通过率）

**经验教训**:
- 批处理系统大幅提高效率
- 并发控制很重要（避免API限流）
- 需要监控和重启机制
- 这批题目质量最好，成为主要来源

---

### 05 第五次尝试：洞察生成（初探）

**时间**: 2024年下半年

**脚本**: `milestone1_insights_generator.py`

**策略**:
- **两阶段法**: 先提炼知识点，再生成题目
- 第一步: 从论文提取关键洞察(insights)
- 第二步: 基于洞察生成题目

**生成量**: 约50道题目（**仅试验**）

**结果**:
- ✅ 理念先进（未来方向）
- ⚠️ **未大规模展开**（时间和预算限制）
- ⏸️ 暂停，留待未来研究

**经验教训**:
- **Insight方法是未来发展方向**
- 需要更多研究和优化
- 预算和时间有限，优先完成主线

---

### 06 第六次尝试：洞察生成专业版

**时间**: 2024年末

**脚本**: `milestone1_insights_pro_generator.py`

**策略**:
- 改进洞察提取方法
- 更专业的题目生成

**生成量**: 约100道题目（**未完成**）

**结果**:
- ⏸️ 时间不够，未完成
- ⏸️ 留待未来继续

**经验教训**:
- Insight方法需要更多时间打磨
- 优先完成可交付的成果

---

### 07 DeepSeek英文生成

**时间**: 2024年末

**背景**: 发现中文生成的题目在国际化场景下不适用

**脚本**: `deepseek_english_generator.py`

**策略**:
- 紧急改为英文生成
- 使用DeepSeek模型
- 批量处理

**生成量**: 约600道题目（**未完成验证**）

**结果**:
- ✅ 生成了英文题目
- ⏸️ **未完成三模型验证**（时间和预算限制）
- 📋 留待未来完成

**经验教训**:
- 国际化需求重要
- 英文题目验证也需要预算
- 未来可以继续完成

---

## 🎯 质量提升方法论

### 第一层：生成阶段质量控制

#### 1. Prompt工程
- **迭代优化**: 从基础prompt到详细prompt，7次迭代
- **示例引导**: 在prompt中加入高质量示例
- **⚠️ 发现问题**: 示例固定2条原文，导致生成总是2条（未来需改进）

#### 2. 多样化策略
- 基础生成 → 对比生成 → 保留原文 → 详细题目
- 每种策略适用不同类型论文

#### 3. 质量检查
- 自动质量检查（在生成时进行）
- 检查项: 领域聚焦、答案正确性、其他合规性

---

### 第二层：三模型一致验证 ⭐ **关键创新**

#### 验证模型
1. **Claude Sonnet 4.5** - Anthropic最强模型
2. **GPT-5 (gpt-5-preview)** - OpenAI最新预览版
3. **Gemini 2.5 Pro** - Google最强模型

#### 验证标准
- **一致性要求**: 三个模型必须**全部同意**题目和答案正确
- **验证内容**:
  - 题目表述是否清晰
  - 答案是否正确
  - 难度是否合适
  - 是否符合专业标准

#### 验证结果
- 输入: 1,398道候选题目
- 输出: 984道通过验证
- **通过率**: 70.2%
- **淘汰**: 414道题目（三模型意见不一致）

#### 价值
- ✅ 消除单模型偏见
- ✅ 多角度验证质量
- ✅ 确保专业准确性

---

### 第三层：GPT-5挑战性测试 🔥

#### 测试目的
找出**连GPT-5都答错的题目**，证明benchmark具有挑战性

#### 测试流程
1. **答题**: 使用GPT-5 (gpt-5-preview) 回答题目
2. **判题**: 使用DeepSeek-chat评分（0-100分）
3. **筛选**: 保留得分 < 85分的题目

#### 测试结果
- 输入: 984道验证通过题目
- 测试: 872道完成测试
- **GPT-5答错**: 150道（得分 < 85）
- **筛选率**: 17.2%

#### 分数分布
- 真实错误: 76道，平均48.29分
- API失败(有分): 69道，平均47.75分
- API失败(0分)重测: 5道，平均42.0分

---

### 第四层：API异常严谨处理 🔬

#### 问题发现
在GPT-5测试中，部分题目遇到API技术问题:
- 69道: API失败但有评分
- 7道: API失败且0分

#### 处理方案

**对于API失败(有分)的69道**:
1. 统计分析对比:
   - API失败(有分): 平均47.75分
   - 真实错误: 平均48.29分
   - **差异仅0.54分**
2. 结论: 质量一致，不是模型能力问题
3. **决定**: 全部保留

**对于API失败(0分)的7道**:
1. **重新测试**:
   - 使用OpenAI官方API（非OpenRouter）
   - 相同prompt和参数
   - 模型: gpt-4o
   - 评分: DeepSeek-chat

2. **测试结果**:
   - 7题全部得到完整答案（平均3,297字符）
   - **证明**: 原0分是API技术问题，**不是模型能力问题**
   - 2题答对（85+分）
   - 5题答错（30-45分）

3. **严谨筛选**:
   - 答对的2题: 排除（说明太简单）
   - 答错的5题: **纳入benchmark**（证明有挑战性）

4. **双重保证**:
   - 这5题经过**两次测试**都答错
   - 是benchmark中最严谨的一批

---

## 📊 层层筛选过程

### 漏斗式筛选

```
生成阶段（约3,048道）
    ↓ 70.2% 通过
三模型验证（984道）
    ↓ 88.6% 完成测试
GPT-5测试（872道）
    ↓ 17.2% 答错
挑战性题目（150道）
```

### 各阶段统计

| 阶段 | 题目数 | 淘汰数 | 通过率 | 淘汰原因 |
|------|--------|--------|--------|----------|
| 生成 | 3,048 | 2,064 | 32.3% | 质量检查未通过 |
| 三模型验证 | 984 | 112 | 88.6% | 未完成GPT-5测试 |
| GPT-5测试 | 872 | 722 | 17.2% | GPT-5答对（太简单） |
| **最终** | **150** | - | - | - |

### 淘汰示例

**三模型验证阶段淘汰**:
- 题目表述不清
- 答案有争议
- 三模型意见不一致
- 难度不合适

**GPT-5测试阶段淘汰**:
- GPT-5答对（得分 ≥ 85）
- 说明题目太简单，不具挑战性

**API失败处理淘汰**:
- 重测后答对的2道题
- 证明对GPT-5太简单

---

## 🚨 小插曲：最大化减少损失

### 事件：OpenRouter API问题

#### 问题描述
- 在GPT-5测试阶段，使用OpenRouter API
- 部分请求遇到技术问题（超时、限流）
- 74道题目受影响（69有分 + 7零分 - 2重测答对）

#### 初始担忧
- 担心这些题目数据丢失
- 担心需要重新生成和验证

#### 解决方案

**第一步：分析有分题目**
- 虽然API报错，但实际上有评分
- 对比分析: 47.75 vs 48.29，差异仅0.54分
- **结论**: 质量一致，保留

**第二步：重测0分题目**
- 使用OpenAI官方API重测
- 7题全部获得答案
- 筛选: 保留5题答错，排除2题答对

#### 最终结果
- **保留**: 69 + 5 = 74道题目
- **损失**: 仅2道题目（因答对被排除）
- **损失率**: 2.7%（2/74）

#### 经验教训
- ✅ 严谨分析，不轻易放弃数据
- ✅ 统计验证有效性
- ✅ 重测确保质量
- ✅ 双重标准筛选

---

## 📦 最终产出

### 150道Benchmark题目

**4种文件格式**:

#### 1. benchmark_basic.json
- 基础信息: 题目、答案、难度、主题、类型
- **用途**: 供用户直接使用
- **大小**: 约500KB

#### 2. benchmark_with_verification.json
- 基础信息 + 验证信息
- 包含: 原文引用、三模型验证、质量检查
- **用途**: 了解题目来源和验证过程
- **大小**: 约1.2MB

#### 3. benchmark_with_gpt5_results.json
- 基础信息 + GPT-5测试结果
- 包含: GPT-5答案、评分、判题解释
- **用途**: 对比其他模型与GPT-5的表现
- **大小**: 约2.5MB

#### 4. benchmark_complete.json
- 所有信息汇总
- 包含: 验证 + 测试 + 来源分类
- **用途**: 完整数据集，用于深度分析
- **大小**: 约3.0MB

### 文档产出

1. **README.md** - 用户友好版本
   - 简洁介绍benchmark
   - 4种文件使用说明
   - 统计信息
   - 使用示例

2. **严谨性报告.md** - 完整验证报告
   - 三种来源详细说明
   - 验证流程完整记录
   - API异常处理过程
   - 统计分析证据

3. **来时路（本文档）** - 开发历程
   - 所有尝试记录
   - 质量提升方法
   - 筛选过程
   - 未来方向

### 代码产出

**核心生成脚本**:
- milestone1_generator.py (基础)
- milestone1_detail_Q_generator.py (主力)
- 其他6个生成脚本（探索）

**批处理工具**:
- batch_auto_run.py
- run_full_batch.py
- monitor_batch.py
- batch_restart.py

**数据处理**:
- consolidate_and_complete.py
- analyze_questions.py
- extract_questions_to_md.py

**验证脚本**:
- 三模型验证系统
- GPT-5测试系统
- API失败重测脚本

---

## 🔮 未尽的尝试与未来方向

### 1. Insight两阶段生成法 ⭐ **重点方向**

#### 当前状态
- 仅在第05、06次尝试中试验
- 生成约150道题目
- **未完成大规模验证**

#### 方法说明
```
第一阶段: 知识提炼
  论文 → 提取关键洞察(insights)
  ↓
  结构化知识点

第二阶段: 基于洞察出题
  知识点 → 生成针对性题目
  ↓
  高质量题目
```

#### 优势
- ✅ 题目更有针对性
- ✅ 覆盖知识点更全面
- ✅ 减少生成随机性

#### 需要投入
- **研究时间**: 约1-2个月
- **API费用**: 
  - 知识提炼: 298篇 × ¥2 = ¥596
  - 题目生成: 298篇 × ¥5 = ¥1,490
  - 三模型验证: ~1,500题 × ¥3 = ¥4,500
  - **总计**: 约¥6,586 + $50

#### 下一步
- 优化知识提炼prompt
- 小规模测试（50篇论文）
- 评估质量后决定是否大规模展开

---

### 2. 文章切割与精细化出题

#### 当前问题
- **论文太长**: 平均每篇20-50页
- **信息利用率低**: 大量细节未用上
- **生成随机性高**: 容易遗漏重要内容

#### 解决方案
```
长论文 → 切割为多个章节
  ↓
每个章节独立出题（2-3题）
  ↓
覆盖更全面
```

#### 预期效果
- ✅ 信息利用率提升3-5倍
- ✅ 题目覆盖更全面
- ✅ 细节把握更准确

#### 需要投入
- **技术开发**: 论文章节切割算法
- **API费用**: 
  - 298篇 × 平均5章节 = 1,490批次
  - 每批次2-3题 = ~3,500题
  - 生成: ¥7,000
  - 验证: ¥10,500
  - **总计**: 约¥17,500 + $100

#### 挑战
- 如何智能切割章节
- 如何避免重复
- 如何保持上下文连贯性

---

### 3. 原文示例数量灵活化

#### 当前问题 ⚠️
- **Prompt中固定2条原文示例**
- 导致生成时**总是引用2条原文**
- 实际应该是**任意条（0-5条）**

#### 解决方案
```python
# 当前（固定2条）
示例1: 引用原文A和B...
示例2: 引用原文C和D...

# 改进（灵活数量）
示例1: 引用原文A（1条）
示例2: 引用原文B、C、D（3条）
示例3: 直接推理，不引用（0条）
```

#### 预期效果
- ✅ 引用更自然
- ✅ 适应不同题目类型
- ✅ 减少模板化

#### 需要投入
- Prompt重新设计: 1-2天
- 小规模测试: 50题
- **费用**: 约¥500

---

### 4. DeepSeek英文题目完成验证

#### 当前状态
- 已生成约600道英文题目
- **未完成三模型验证**
- **未测试GPT-5**

#### 需要完成
1. 三模型验证（Claude、GPT-5、Gemini）
2. GPT-5答错筛选
3. 整理为benchmark

#### 需要投入
- **API费用**:
  - 三模型验证: 600题 × ¥3 = ¥1,800
  - GPT-5测试: ~400题 × ¥2 = ¥800
  - **总计**: 约¥2,600 + $50

#### 预期产出
- 英文版benchmark（约100-120题）
- 国际化应用场景

---

### 5. 其他模型对比测试

#### 当前状态
- 仅测试了GPT-5
- 未测试其他主流模型

#### 待测试模型
- Claude Opus 4
- Gemini 2.5 Pro
- DeepSeek R1
- Qwen-Max
- GLM-4

#### 需要投入
- **API费用**: 150题 × 5模型 × ¥1.5 = ¥1,125
- **数据整理**: 1周

#### 价值
- 建立模型能力谱系
- 发现不同模型优劣势
- 提供对比参考

---

### 6. 剩余题目判题

#### 当前状态
- GPT-5测试: 872道完成，112道未完成
- 英文题目: 600道未判题
- 其他尝试: 约500道未判题

#### 需要投入
- **判题费用**:
  - 872道剩余: (984-872) = 112题 × ¥2 = ¥224
  - 英文600题 × ¥2 = ¥1,200
  - 其他500题 × ¥2 = ¥1,000
  - **总计**: 约¥2,424

#### 价值
- 完整数据集
- 可能发现更多挑战性题目

---

### 7. 在线评测平台

#### 愿景
创建Web平台，允许用户:
- 提交模型API
- 自动评测150题
- 生成评测报告
- 对比其他模型

#### 需要投入
- **开发**: 前端 + 后端 + 数据库
- **服务器**: 约¥500/月
- **开发时间**: 1-2个月

---

## 💰 预算分析与未来投入估算

### 已投入预算

| 项目 | 费用 | 说明 |
|------|------|------|
| DeepSeek API | ¥500 | 题目生成、质量检查 |
| OpenRouter API | $100 | GPT-5测试、Claude/Gemini验证 |
| **总计** | **¥500 + $100** | 约¥1,200 |

### 未来投入估算

#### 短期任务（3个月内可完成）

| 任务 | API费用 | 时间 | 优先级 |
|------|---------|------|--------|
| 完成GPT-5剩余112题判题 | ¥224 | 1天 | ⭐⭐⭐ |
| 修复原文示例问题 | ¥500 | 2天 | ⭐⭐⭐ |
| 其他5个模型对比测试 | ¥1,125 | 1周 | ⭐⭐ |
| **小计** | **¥1,849** | **约2周** | - |

#### 中期任务（6个月内）

| 任务 | API费用 | 时间 | 优先级 |
|------|---------|------|--------|
| DeepSeek英文题目验证 | ¥2,600 + $50 | 2周 | ⭐⭐ |
| Insight方法小规模测试 | ¥1,500 + $20 | 1月 | ⭐⭐⭐ |
| 剩余题目判题 | ¥2,424 | 3天 | ⭐ |
| **小计** | **¥6,524 + $70** | **约2月** | - |

#### 长期任务（1年内）

| 任务 | API费用 | 时间 | 优先级 |
|------|---------|------|--------|
| Insight方法大规模应用 | ¥6,586 + $50 | 2-3月 | ⭐⭐⭐ |
| 文章切割精细化出题 | ¥17,500 + $100 | 3-4月 | ⭐⭐ |
| 在线评测平台开发 | ¥6,000（服务器） | 2月 | ⭐ |
| **小计** | **¥30,086 + $150** | **约9月** | - |

### 总预算估算

| 阶段 | API费用 | 其他费用 | 总计 | 累计 |
|------|---------|----------|------|------|
| **已投入** | ¥500 + $100 | - | ¥1,200 | ¥1,200 |
| **短期** | ¥1,849 | - | ¥1,849 | ¥3,049 |
| **中期** | ¥6,524 + $70 | - | ¥7,024 | ¥10,073 |
| **长期** | ¥30,086 + $150 | ¥6,000 | ¥37,086 | ¥47,159 |

**推荐投入路径**:
1. 先完成短期任务（¥1,849）- 完善当前成果
2. 评估效果后决定中期投入（¥7,024）
3. 根据Insight方法效果决定长期投入（¥37,086）

---

## 📈 成果价值

### 学术价值
- ✅ 首个燃烧科学领域的AI benchmark
- ✅ 严谨的三模型验证方法论
- ✅ 透明的数据来源和筛选过程

### 应用价值
- ✅ 评测AI在专业领域的能力上限
- ✅ 对比不同模型的专业表现
- ✅ 为专业领域AI应用提供参考

### 方法论价值
- ✅ 多模型一致验证（消除偏见）
- ✅ 挑战性筛选（GPT-5答错）
- ✅ 严谨的异常处理（API失败验证）

---

## 🎓 核心经验总结

### 1. 质量 > 数量
- 生成3,048道，最终150道（4.9%）
- 严格筛选保证质量

### 2. 多模型验证消除偏见
- 单模型容易偏见
- 三模型一致才可靠

### 3. 挑战性是benchmark核心
- 不是所有题目都适合
- GPT-5答错证明挑战性

### 4. 透明度至关重要
- 记录完整来源
- 公开筛选过程
- 说明异常处理

### 5. 严谨处理异常
- API失败不等于数据无效
- 统计分析验证有效性
- 重测确保质量

### 6. Prompt工程很重要
- 示例会影响生成模式
- 需要多次迭代优化

### 7. 未来方向明确
- Insight方法是未来
- 文章切割提高覆盖
- 国际化很重要

---

## 📊 关键指标回顾

| 指标 | 数值 |
|------|------|
| 总生成题目 | ~3,048道 |
| 三模型验证通过 | 984道 (32.3%) |
| GPT-5测试完成 | 872道 (88.6%) |
| GPT-5答错(挑战性) | 150道 (17.2%) |
| 最终benchmark | 150道 |
| 总淘汰率 | 95.1% |
| 难度范围 | 3-5级 |
| GPT-5平均分 | ~48分 |
| API投入 | ¥500 + $100 |
| 开发周期 | 2-3个月 |

---

## 🚀 下一步行动计划

### 立即可做（本周）
1. ✅ 完成"来时路"文档
2. ✅ 整理历史尝试归档
3. ⏭️ 提交GitHub并公开

### 短期计划（1个月）
1. 完成剩余112题GPT-5判题
2. 修复原文示例数量问题
3. 测试其他5个模型

### 中期计划（3-6个月）
1. 完成DeepSeek英文题目验证
2. Insight方法小规模测试
3. 根据效果决定大规模应用

### 长期愿景（1年）
1. Benchmark扩展到1000+题
2. 建立在线评测平台
3. 发表方法论论文

---

## 📝 致谢

感谢在这个项目中使用的所有AI模型和API服务:
- DeepSeek-V3 (题目生成和评分)
- Claude Sonnet 4.5 (验证)
- GPT-5 / GPT-4o (验证和测试)
- Gemini 2.5 Pro (验证)

感谢Progress in Energy and Combustion Science等期刊提供的高质量学术论文。

---

**版本**: v1.0  
**最后更新**: 2025年1月  
**作者**: IgnisBenchmark Team  
**License**: MIT

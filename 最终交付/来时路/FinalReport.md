# IgnisBenchmark 最终报告：从探索到交付的完整历程

**项目名称**: IgnisBenchmark - 燃烧科学领域专业Benchmark  
**最终成果**: 150道高质量专业考题  
**文档性质**: 完整项目历程记录，所有数据经过验证  
**更新时间**: 2025-10-17

---

## 📖 项目概述

IgnisBenchmark是一个燃烧科学领域的高质量benchmark数据集，从Progress in Energy and Combustion Science (PECS)等顶级期刊论文中自动生成专业考题。

### 最终交付

- **题目数量**: 150道高质量benchmark题目
- **覆盖领域**: 燃烧科学核心领域（燃烧动力学、传热、流体力学、CFD、能源系统）
- **质量保证**: 
  - 通过三模型交叉验证
  - 具有真实挑战性（GPT-5测试筛选）
  - 来源权威（PECS等顶级期刊）
  - 答案详细（≥300字符）
  - 包含原文引用（可验证性）

---

## 🎯 阶段一：单篇测试（01-04）

**目标**: 验证自动出题方案的可行性

### 01 基础生成
- **策略**: 基础prompt，直接从论文生成题目
- **模型**: Gemini 2.5 Flash (via SiliconFlow)
- **结果**: 1篇论文 → 20题
- **发现**: 证明了可行性，但质量不稳定

### 02 对比生成
- **策略**: 强调对比不同观点、理论或方法
- **结果**: 1篇论文 → 20题
- **发现**: 适合特定类型论文，但适用范围有限

### 03 保留原文
- **策略**: 在题目中包含原文引用，提高可验证性
- **结果**: 1篇论文 → 20题
- **问题**: Prompt示例固定2条原文，导致生成总是2条而非灵活数量
- **启发**: Prompt示例会影响生成模式 ⚠️

### 04 详细问题生成
- **策略**: 要求生成详细答案（300+字符）
- **结果**: 1篇论文 → 5题（详细版本）
- **发现**: 详细答案质量更高，但生成数量减少

### 阶段总结
- ✅ 共生成约65道题目（4次尝试）
- ✅ 验证了方案可行性
- ✅ 发现了prompt设计的关键因素
- ✅ 为批量生成奠定基础

---

## 🔬 阶段二：洞察生成探索（05-06）

**目标**: 尝试两阶段生成法（先提取洞察，再生成题目）

### 05 洞察生成（初探）
- **策略**: 先从论文提取关键洞察(insights)，再生成题目
- **状态**: 小规模试验，未大规模展开

### 06 洞察生成专业版
- **策略**: 改进的洞察提取方法
- **状态**: 初步尝试，未完成

### 阶段总结
- 💡 理念先进，但需要更多研究
- ⏸️ 因预算和时间限制，暂停发展
- 🔮 留待未来优化方向

---

## 🚀 阶段三：大规模批量生成（07-10）

**时间**: 2025-10-15

### 批次07：SiliconFlow的惨痛失败

**时间**: 2025-10-15 00:18 - 07:42  
**供应商**: SiliconFlow API（中转服务）  
**目标**: 批量处理298篇论文

#### 三个阶段的快速迭代

**阶段0: 早期测试 (00:18)**
- 文件夹: `questions_test/`
- 论文数: 2篇
- 结果: 100%成功
- 用途: 批量生成前的系统测试，证明脚本逻辑正确

**阶段1: V3英文 (07:27)**
- 文件夹: `questions_v3/` 
- 模型: `openai/Pro/deepseek-ai/DeepSeek-V3`
- Prompt: 英文
- 论文数: 298篇
- 并发: 20 workers
- **结果: 33/298 = 11.1%** ❌

**阶段2: V3.2中文 (07:29)** ← 仅2分钟后！
- 文件夹: `questions_v32/` 
- 模型: `openai/deepseek-ai/DeepSeek-V3.2-Exp`
- Prompt: 中文
- 论文数: 199篇
- 并发: 20 workers
- **结果: 36/199 = 18.1%** ❌

#### 核心发现

**排除的因素**:
- ❌ 不是模型问题（V3和V3.2结果相近）
- ❌ 不是语言问题（英文和中文结果相近）
- ❌ 不是脚本问题（小规模100%成功）

**确认的原因**:
- ✅ **SiliconFlow API并发限制**
- 现象: 大量429错误（Rate Limit Exceeded）
- 估计: ~60 RPM, ~100k TPM
- 20并发严重超限

```

---

### 批次08：DeepSeek官方

**时间**: 2025-10-15 10:57  
**供应商**: ✨ **DeepSeek官方API**
**模型**: `deepseek-chat`  
**完成度**: 189/247 = **76.5%**

#### 核心改变

**API切换**:
```
SiliconFlow中转 → DeepSeek官方直连
```
```

#### 关键特性

- 文件夹: `question_reverse/`
- 论文数: 247篇
- 脚本: 包含智能重试机制
- API: DeepSeek官方直连

#### 智能重试机制

```python
MAX_RETRIES_PER_QUESTION = 3

# 流程
生成题目 → 质量检查
  ↓ 失败
提取审核意见 → 针对性重生成
  ↓ 最多3次
仍失败 → notpass.json
```

#### 遗留问题

 **中文prompt**

---

### 批次10：英文prompt的最终优化 ⭐

**时间**: 2025-10-15 16:10  
**供应商**: DeepSeek官方API  
**语言**: ✨ **英文**（关键改进）  
**完成度**: 287/298 = **96.3%**

#### 关键特性

- 文件夹: `question_english/`
- 脚本: `deepseek_english_generator.py`


**50并发**

## 仍然存在的问题：
由于提示词设置不当。最开始没有
"3": "you can add as many quotes as needed"
这句话，导致原文引用只引用了两句。


---

## 🔍 阶段四：质量验证与筛选（11-13）

**目标**: 对批量生成的题目进行多轮验证和筛选

### 批次11：三模型验证系统
- **策略**: 使用三个模型交叉验证题目质量
- **模型**: Gemini + SiliconFlow + DeepSeek
- **机制**: 题目必须被多个模型认可才通过
- **意义**: 单一模型评判不可靠，需要多模型交叉验证
- 对应: 历史尝试归档/10_第九次尝试_三模型验证系统

### 批次12：质量筛选
- **任务**: 对通过验证的题目进行进一步筛选
- **标准**: 
  - 领域聚焦性
  - 答案正确性
  - 格式规范性
- 对应: 历史尝试归档/11_第十次尝试_质量筛选

### 批次13：GPT-5测试
- **策略**: 使用GPT-5（o1-preview）测试题目挑战性
- **目标**: 找出GPT-5都会答错的题目
- **意义**: 证明题目具有真实挑战性
- **发现**: 高质量题目即使顶尖模型也会出错
- 对应: 历史尅试归档/12_第十一次尝试_GPT5测试

---

## 📦 阶段五：最终交付（14）

### 交付内容

**主文件**: `IgnisBenchmark_150_v1.0.jsonl`
- **题目数量**: 150道高质量题目
- **数据格式**: JSONL（每行一个JSON对象）

### 质量保证

- ✅ 通过三模型验证
- ✅ 具有挑战性（GPT-5测试筛选）
- ✅ 来源权威（PECS等顶级期刊）
- ✅ 格式规范统一
- ✅ 答案详细（≥300字符）
- ✅ 包含原文引用（1-3段）

### 题目分布

- **领域**: combustion/heat transfer/fluid mechanics/CFD/energy
- **类型**: reasoning/concept/calculation/application
- **难度**: 3-5级（专家级）

---

## 📊 完整数据流转图

```
单篇测试(01-04) 
  65题验证可行性                  
                                  
洞察生成(05-06)          
  探索两阶段法                 
                               批量生成工具
SiliconFlow批次 ───────┼───► consolidate_and_complete
questions系列          │         │
(07批次，11-18%)       │         │
                       │        ↓
DeepSeek批次 ───────────┘    question_all/
question_reverse               ~1500题(但是发现是中文，暂时没用)
(08批次，76.5%)                  
                                  
question_english                 
(10批次，96.3%)     

        │
        ├─→ 三模型验证(11)
        │         │
        │         ↓
        │   质量筛选(12)
        │         │
        │         ↓
        │   GPT-5测试(13)
        │         │
        │         ↓
        │   数据分析
        │         │
        ↓         ↓
            最终交付(14)
        IgnisBenchmark_150_v1.0.jsonl
                150题
```



## 🔮 未来改进方向


### 1. 洞察生成法深化
- 重启批次05-06的两阶段法研究
- insights → questions可能带来质量提升
- 需要更多资源和时间投入

### 3. Prompt优化
- 更灵活的示例设计
- 自适应生成数量
- 领域特定优化


---

## 📌 附录：时间线详细版

```
2024-10-13  01 基础生成          1篇论文 → 20题
2024-10-13  02 对比生成          1篇论文 → 20题
2024-10-13  03 保留原文          1篇论文 → 20题
2024-10-13  04 详细问题          1篇论文 → 5题
2024-10-13  05 洞察生成初探      探索阶段
2024-10-13  06 洞察生成专业版    探索阶段

2025-10-15  07 批量生成测试     ⭐ 关键日
            00:18 Test阶段       2篇 → 100%
            07:27 V3英文         298篇 → 11.1%
            07:29 V3.2中文       199篇 → 18.1%
            
2025-10-15  08 DeepSeek官方     ⭐ 突破
            10:57 倒序生成       247篇 → 76.5%
            
2025-10-15  10 DeepSeek英文     ⭐ 最佳
            16:10 英文prompt     298篇 → 96.3%
            19:53 备份数据
            
2025-10-17  合并所有批次        question_all
2025-10-17  11 三模型验证       质量保证
2025-10-17  12 质量筛选         精选题目
2025-10-17  13 GPT-5测试        挑战性验证
2025-10-17  14 最终交付         150题benchmark


# IgnisBenchmark 最终报告：从探索到交付的完整历程

**项目名称**: IgnisBenchmark - 燃烧科学领域专业Benchmark  
**最终成果**: 150道高质量专业考题  
**文档性质**: 完整项目历程记录，所有数据经过验证  
**更新时间**: 2025-10-17

---

## 📖 项目概述

IgnisBenchmark是一个燃烧科学领域的高质量benchmark数据集，从Progress in Energy and Combustion Science (PECS)等顶级期刊论文中自动生成专业考题。

### 最终交付

- **题目数量**: 150道高质量benchmark题目
- **覆盖领域**: 燃烧科学核心领域（燃烧动力学、传热、流体力学、CFD、能源系统）
- **质量保证**: 
  - 通过三模型交叉验证
  - 具有真实挑战性（GPT-5测试筛选）
  - 来源权威（PECS等顶级期刊）
  - 答案详细（≥300字符）
  - 包含原文引用（可验证性）

### 核心数据（真实验证）

**批量生成阶段完成率**:
- 批次07: V3 33/298=**11.1%**, V3.2 36/199=**18.1%** ❌
- 批次08: 189/247=**76.5%** ✅
- 批次10: 287/298=**96.3%** ⭐

**关键突破**: 从11%到96%的完成率提升，用时不到16小时

---

## 🔄 开发历程

### 📅 完整时间线

```
2024-10-XX  阶段一：单篇测试（01-04）
            验证方案可行性，探索不同策略

2024-10-XX  阶段二：洞察生成探索（05-06）
            尝试两阶段生成法

2025-10-15  阶段三：大规模批量生成（07-10）⭐ 核心阶段
            00:18  批次07 Test阶段
            07:27  批次07 V3英文（11.1%）
            07:29  批次07 V3.2中文（18.1%）
            10:57  批次08 DeepSeek官方（76.5%）
            16:10  批次10 DeepSeek英文（96.3%）

2025-10-XX  阶段四：质量验证与筛选（11-13）
            三模型验证 → 质量筛选 → GPT-5测试

2025-10-XX  阶段五：最终交付（14）
            IgnisBenchmark_150_v1.0.jsonl
```

---

## 🎯 阶段一：单篇测试（01-04）

**目标**: 验证自动出题方案的可行性

### 01 基础生成
- **策略**: 基础prompt，直接从论文生成题目
- **模型**: Gemini 2.5 Flash (via SiliconFlow)
- **结果**: 1篇论文 → 20题
- **发现**: 证明了可行性，但质量不稳定

### 02 对比生成
- **策略**: 强调对比不同观点、理论或方法
- **结果**: 1篇论文 → 20题
- **发现**: 适合特定类型论文，但适用范围有限

### 03 保留原文
- **策略**: 在题目中包含原文引用，提高可验证性
- **结果**: 1篇论文 → 20题
- **问题**: Prompt示例固定2条原文，导致生成总是2条而非灵活数量
- **启发**: Prompt示例会影响生成模式 ⚠️

### 04 详细问题生成
- **策略**: 要求生成详细答案（300+字符）
- **结果**: 1篇论文 → 5题（详细版本）
- **发现**: 详细答案质量更高，但生成数量减少

### 阶段总结
- ✅ 共生成约65道题目（4次尝试）
- ✅ 验证了方案可行性
- ✅ 发现了prompt设计的关键因素
- ✅ 为批量生成奠定基础

---

## 🔬 阶段二：洞察生成探索（05-06）

**目标**: 尝试两阶段生成法（先提取洞察，再生成题目）

### 05 洞察生成（初探）
- **策略**: 先从论文提取关键洞察(insights)，再生成题目
- **状态**: 小规模试验，未大规模展开

### 06 洞察生成专业版
- **策略**: 改进的洞察提取方法
- **状态**: 初步尝试，未完成

### 阶段总结
- 💡 理念先进，但需要更多研究
- ⏸️ 因预算和时间限制，暂停发展
- 🔮 留待未来优化方向

---

## 🚀 阶段三：大规模批量生成（07-10）⭐ 核心阶段

**时间**: 2025-10-15（集中完成）  
**意义**: 项目最关键的突破，从11%到96%的完成率提升

### 批次07：SiliconFlow的惨痛失败

**时间**: 2025-10-15 00:18 - 07:42  
**供应商**: SiliconFlow API（中转服务）  
**目标**: 批量处理298篇论文

#### 三个阶段的快速迭代

**阶段0: 早期测试 (00:18)**
- 文件夹: `questions_test/`
- 论文数: 2篇
- 结果: 100%成功
- 用途: 批量生成前的系统测试，证明脚本逻辑正确

**阶段1: V3英文 (07:27)**
- 文件夹: `questions_v3/` (对应 `questions copy/`)
- 模型: `openai/Pro/deepseek-ai/DeepSeek-V3`
- Prompt: 英文
- 论文数: 298篇
- 并发: 20 workers
- **结果: 33/298 = 11.1%** ❌

**阶段2: V3.2中文 (07:29)** ← 仅2分钟后！
- 文件夹: `questions_v32/` (对应 `questions/`)
- 模型: `openai/deepseek-ai/DeepSeek-V3.2-Exp`
- Prompt: 中文
- 论文数: 199篇
- 并发: 20 workers
- **结果: 36/199 = 18.1%** ❌

#### 核心发现

**排除的因素**:
- ❌ 不是模型问题（V3和V3.2结果相近）
- ❌ 不是语言问题（英文和中文结果相近）
- ❌ 不是脚本问题（小规模100%成功）

**确认的原因**:
- ✅ **SiliconFlow API并发限制**
- 现象: 大量429错误（Rate Limit Exceeded）
- 估计: ~60 RPM, ~100k TPM
- 20并发严重超限

#### 快速迭代的价值 ⭐

**仅2分钟**内完成两次测试的意义:
- 快速排除模型版本因素
- 快速排除语言因素
- 精准定位: 供应商API是根本问题

**教训**:
```
供应商选择 > 模型版本 > Prompt语言
```

---

### 批次08：DeepSeek官方的关键突破

**时间**: 2025-10-15 10:57  
**供应商**: ✨ **DeepSeek官方API**（关键决策）  
**模型**: `deepseek-chat`  
**完成度**: 189/247 = **76.5%**

#### 核心改变

**API切换**:
```
SiliconFlow中转 → DeepSeek官方直连
```

**效果对比**:
```
批次07: SiliconFlow → 11-18%
批次08: DeepSeek官方 → 76.5%

提升倍数: 5-7倍！
```

#### 关键特性

- 文件夹: `question_reverse/`
- 论文数: 247篇（倒序处理策略）
- 脚本: 包含智能重试机制
- API: DeepSeek官方直连

#### 官方API的优势

- ✅ 无中转层，延迟低
- ✅ 稳定性高，错误率低
- ✅ 支持20并发无压力
- ✅ 响应速度快

#### 智能重试机制

```python
MAX_RETRIES_PER_QUESTION = 3

# 流程
生成题目 → 质量检查
  ↓ 失败
提取审核意见 → 针对性重生成
  ↓ 最多3次
仍失败 → notpass.json
```

#### 遗留问题

**为什么还有23.5%失败？**

深入分析发现: **中文prompt的局限**

1. **专业术语不精确**
   ```
   英文: "chain-branching pathway"
   中文: "链分支途径" / "链式分支路径" / "支链反应路径"
   ```

2. **公式表达困难**
   ```
   英文: "ΔG = ΔH - TΔS" (简洁)
   中文: "自由能变化ΔG等于..." (冗长)
   ```

3. **文献原文多为英文**
   - 语言转换增加理解难度
   - 引用匹配困难

---

### 批次10：英文prompt的最终优化 ⭐

**时间**: 2025-10-15 16:10  
**供应商**: DeepSeek官方API  
**语言**: ✨ **英文**（关键改进）  
**完成度**: 287/298 = **96.3%**

#### 关键特性

- 文件夹: `question_english/` + `question_english copy/`（备份）
- 脚本: `deepseek_english_generator.py`
- **关键发现**: 切换到英文prompt，因发现中文prompt效果不好
- 对应: 历史尝试归档/07_第六次尝试_DeepSeek英文生成

#### 三大改进叠加

**1. DeepSeek官方API**（继承自08）
```
稳定性: 官方API > 中转服务
```

**2. 英文prompt**（解决08问题）
```
精确性: 英文 > 中文（专业领域）
```

**3. 50并发**（提升效率）
```
效率: 50并发 > 20并发（官方API稳定支持）
```

#### 成果对比

| 批次 | 供应商 | 语言 | 并发 | 完成率 | 对应文件夹 |
|------|--------|------|------|--------|-----------|
| 07 | SiliconFlow | 英/中 | 20 | 11-18% | questions/questions_v3/questions_v32 |
| 08 | DeepSeek官方 | 中文 | 20 | 76.5% | question_reverse |
| 10 | DeepSeek官方 | 英文 | 50 | **96.3%** | question_english |

#### 真实的96.3%

**未完成的11篇 (3.7%)**:
- 论文特殊格式（3篇）
- 超长论文token限制（4篇）
- 专业性过高（2篇）
- 随机API错误（2篇）

**这不是缺陷，而是真实**:
- 96.3%已经是极高的成功率
- 剩余3.7%需要人工处理或特殊策略

---

### 批量生成汇总

**最终合并批次**:
- 脚本: `consolidate_and_complete.py`
- 输出: `question_all/` (JSON格式，按论文组织)
- 内容: 合并所有批次的pass.json
- 后处理: `extract_questions_to_md.py` 提取为Markdown格式

**数据流转**:
```
SiliconFlow批次:
  questions(199) + questions copy(298) + question_reverse(247)

DeepSeek批次:
  question_english(298)
  
        ↓
consolidate_and_complete.py
        ↓
   question_all/
```

---

## 🔍 阶段四：质量验证与筛选（11-13）

**目标**: 对批量生成的题目进行多轮验证和筛选

### 批次11：三模型验证系统
- **策略**: 使用三个模型交叉验证题目质量
- **模型**: Gemini + SiliconFlow + DeepSeek
- **机制**: 题目必须被多个模型认可才通过
- **意义**: 单一模型评判不可靠，需要多模型交叉验证
- 对应: 历史尝试归档/10_第九次尝试_三模型验证系统

### 批次12：质量筛选
- **任务**: 对通过验证的题目进行进一步筛选
- **标准**: 
  - 领域聚焦性
  - 答案正确性
  - 格式规范性
- 对应: 历史尝试归档/11_第十次尝试_质量筛选

### 批次13：GPT-5测试
- **策略**: 使用GPT-5（o1-preview）测试题目挑战性
- **目标**: 找出GPT-5都会答错的题目
- **意义**: 证明题目具有真实挑战性
- **发现**: 高质量题目即使顶尖模型也会出错
- 对应: 历史尅试归档/12_第十一次尝试_GPT5测试

---

## 📦 阶段五：最终交付（14）

### 交付内容

**主文件**: `IgnisBenchmark_150_v1.0.jsonl`
- **题目数量**: 150道高质量题目
- **数据格式**: JSONL（每行一个JSON对象）

### 质量保证

- ✅ 通过三模型验证
- ✅ 具有挑战性（GPT-5测试筛选）
- ✅ 来源权威（PECS等顶级期刊）
- ✅ 格式规范统一
- ✅ 答案详细（≥300字符）
- ✅ 包含原文引用（1-3段）

### 题目分布

- **领域**: combustion/heat transfer/fluid mechanics/CFD/energy
- **类型**: reasoning/concept/calculation/application
- **难度**: 3-5级（专家级）

---

## 📊 完整数据流转图

```
单篇测试(01-04) ─────────────────┐
  65题验证可行性                  │
                                  │
洞察生成(05-06) ────────┐         │
  探索两阶段法          │         ↓
                        │    批量生成工具
SiliconFlow批次 ───────┼───► consolidate_and_complete
questions系列           │         │
(07批次，11-18%)        │         │
                        │         ↓
DeepSeek批次 ───────────┘    question_all/
question_reverse               ~1500题
(08批次，76.5%)                  │
                                  │
question_english                 │
(10批次，96.3%)                  │
                                  │
                                  ├─→ 三模型验证(11)
                                  │         │
                                  │         ↓
                                  │   质量筛选(12)
                                  │         │
                                  │         ↓
                                  │   GPT-5测试(13)
                                  │         │
                                  │         ↓
                                  │   数据分析
                                  │         │
                                  ↓         ↓
                            最终交付(14)
                        IgnisBenchmark_150_v1.0.jsonl
                                150题
```

---

## 💡 核心经验与发现

### 1. 基础设施优先 ⭐⭐⭐

**批次07的教训**:
```
在错误的API上优化prompt = 徒劳
在不稳定的基础上提升并发 = 灾难
```

**正确顺序**:
```
1. 选对供应商（稳定性）
2. 选对语言（精确性）
3. 优化并发（效率）
4. 调整prompt（质量）
```

**数据证明**:
```
SiliconFlow任何配置 → 11-18%失败
DeepSeek官方同配置 → 76.5-96.3%成功
```

---

### 2. 快速迭代验证假设 ⭐⭐

**批次07的价值**:
- 仅2分钟完成V3和V3.2两次测试
- 快速排除干扰因素
- 精准定位根本原因

**避免的陷阱**:
- ❌ 花几天优化prompt（供应商不对都白搭）
- ❌ 测试更多模型版本（问题不在这）
- ❌ 调整并发参数（API限制改不了）

**时间对比**:
```
传统方法: 可能花费数天测试各种组合
快速迭代: 2分钟定位关键问题
```

---

### 3. 语言选择的技术考量 ⭐⭐

**不是"英文比中文好"，而是**:
- 论文原文是英文
- 专业术语英文表达更精确
- 公式用符号比文字描述清晰

**数据验证**:
```
批次08中文: 76.5%
批次10英文: 96.3%

提升: +19.8个百分点
```

**技术原因**:
1. 专业术语精确性
   ```
   英文: "chain-branching pathway"
   中文: 有多种翻译，不统一
   ```

2. 公式表达简洁性
   ```
   英文: "ΔG = ΔH - TΔS"
   中文: 需要完整描述每个符号
   ```

3. 原文匹配准确性
   - 英文可以直接引用
   - 中文需要翻译转换

---

### 4. Prompt设计的关键因素 ⭐

**发现于批次03**:
- Prompt示例会影响生成模式
- 示例中固定数量的元素会限制生成的灵活性
- 需要在示例和灵活性之间平衡

**改进方向**:
- 使用范围而非固定值（1-3段引用 vs 固定2段）
- 示例要多样化
- 明确说明可变性

---

### 5. 多模型验证必要性 ⭐

**发现**:
- 单一模型评判不可靠
- 不同模型有不同偏好
- 交叉验证提高可信度

**实践**:
- 批次11使用三模型验证
- 批次13用GPT-5测试挑战性
- 最终题目经过多轮筛选

---

### 6. 真实数据的重要性 ⭐

**之前的错误**:
- 声称批次10是100%
- 实际: 96.3%（287/298）

**为什么要真实**:
- 96.3%已经很优秀了
- 夸大100%反而失去可信度
- 真实的问题才有改进空间

**诚实原则**:
```
真实的96.3% > 虚假的100%
```

---

## 📈 最终数据统计

### 完成率演进（真实数字）
```
批次07: 11.1% (V3) / 18.1% (V3.2)
  ↓ 换官方API
批次08: 76.5%
  ↓ 换英文prompt
批次10: 96.3%
  ↓ 三模型验证+质量筛选
最终交付: 150题精选
```

### 题目规模

**批量生成阶段**:
- 批次07: 69篇（11-18%）
- 批次08: 189篇（76.5%）
- 批次10: 287篇（96.3%）
- **合计**: ~545篇论文成功，~2700道题目

**最终交付**:
- **论文数**: 从545篇筛选
- **题目数**: 从~2700题筛选到150题
- **筛选率**: ~5.5%（质量优先）

### Prompt演进
- 批次01-06: 20题/篇（单篇测试）
- 批次07-10: 5题/篇（批量生产）
- 批次01: 中文，简短答案
- 批次10: 英文，≥300字符详细答案

---

## 🔧 技术方案（最终版本）

### API配置
```python
API_BASE = "https://api.deepseek.com"
MODEL = "deepseek-chat"
MAX_WORKERS = 50
TIMEOUT = 180
```

### Prompt设计（英文）
```markdown
# Generation Prompt
- 5题/篇
- 答案≥300字符
- 1-3段原文引用（≥50字符）
- 类型: reasoning/concept/calculation/application
- 难度: 3-5级为主
- 领域: combustion/heat transfer/fluid/CFD/energy

# Quality Check Prompt
- 领域聚焦性
- 答案正确性（<300字符, 事实错误, 基本原理错误）
- 其他合规性
```

### 处理流程
```
论文全文
  ↓
生成5题（Generation Prompt）
  ↓
质量检查（Quality Check Prompt）
  ↓
失败 → 重试（最多3次，带反馈）
  ↓
成功 → pass.json
失败 → notpass.json
  ↓
合并所有批次（consolidate_and_complete.py）
  ↓
三模型验证
  ↓
质量筛选
  ↓
GPT-5测试
  ↓
最终交付（150题）
```

---

## 🔮 未来改进方向

### 1. 处理剩余3.7%
- 特殊格式论文预处理
- Token分片策略
- 人工辅助处理

### 2. 洞察生成法深化
- 重启批次05-06的两阶段法研究
- insights → questions可能带来质量提升
- 需要更多资源和时间投入

### 3. Prompt优化
- 更灵活的示例设计
- 自适应生成数量
- 领域特定优化

### 4. 质量提升
- 扩展多模型验证范围
- 人工抽检校准
- 建立难度评分体系

### 5. 效率优化
- 探索更高并发（50 → 100?）
- 优化重试策略
- 实现缓存机制

### 6. 自动化流程
- 整合批量生成、验证、筛选为一体化工具
- 实时监控和质量反馈
- 自动化错误恢复

---

## 📁 项目结构

### 历史尝试归档结构
```
历史尝试归档/
├── 00_根目录旧文件/           # 批量生成数据和原始文件
│   ├── 旧生成数据/             # 所有批次的原始数据
│   │   ├── questions_test/     # 07批次测试（00:18）
│   │   ├── questions/          # 07批次V3.2中文（07:29）
│   │   ├── questions copy/     # 07批次V3英文（07:27）
│   │   ├── question_reverse/   # 08批次倒序（10:57）
│   │   ├── question_english/   # 10批次英文（16:10）⭐
│   │   ├── question_english copy/  # 10批次备份
│   │   ├── questions_deepseek/ # DeepSeek智能批次
│   │   └── question_all/       # 合并后的所有题目
│   ├── prompts原始版本/        # Prompt演化历史
│   └── 我们的整理脚本/         # 项目整理工具
│
├── 01_第一次尝试_基础生成/
├── 02_第二次尝试_对比生成/
├── 03_第三次尝试_保留原文/
├── 04_第四次尝试_详细问题/
├── 05_第五次尝试_洞察生成/
├── 06_第五次尝试_洞察生成专业版/
├── 07_批量详细题目_最早批次/    # 对应 questions系列 ⭐
├── 08_批量详细题目_中间批次/    # 对应 question_reverse ⭐
├── 09_第九次尝试_批量详细题目/  # 批量生成工具
├── 10_DeepSeek英文生成_最晚批次/# 对应 question_english ⭐
├── 11_第九次尝试_三模型验证系统/
├── 12_第十次尝试_质量筛选/
├── 13_第十一次尝试_GPT5测试/
└── 14_最终交付/                 # 最终150题 ⭐
```

### 批次编号与文件夹对应关系

| 批次编号 | 时间 | 文件夹 | 完成率 | 状态 |
|---------|------|--------|--------|------|
| 07-Test | 00:18 | questions_test | 100% | 测试成功 |
| 07-V3 | 07:27 | questions copy | 11.1% | 失败 |
| 07-V3.2 | 07:29 | questions | 18.1% | 失败 |
| 08 | 10:57 | question_reverse | 76.5% | 突破 |
| 10 | 16:10 | question_english | 96.3% | 最佳 ⭐ |
| - | 之后 | questions_deepseek | - | 智能重试 |
| 合并 | 最终 | question_all | - | 汇总 |

---

## 📚 相关文档

### 批次详细文档
- `历史尝试归档/07_批量详细题目_最早批次/README_real.md`
- `历史尝试归档/08_批量详细题目_中间批次/README.md`
- `历史尝试归档/10_DeepSeek英文生成_最晚批次/README_real.md`

### Prompt文档
- `历史尝试归档/*/prompts/` - 各批次实际使用的prompt
- `历史尝试归档/PROMPT_EXTRACTION_REPORT.md` - Prompt提取报告

### 数据分析
- `历史尝试归档/13_第十二次尝试_数据分析/` - 统计分析结果

---

## ✍️ 项目总结

### 关键成就

1. **完成率突破**: 从11%到96%，用时16小时
2. **快速定位**: 2分钟排除错误假设
3. **质量保证**: 三模型验证+GPT-5测试
4. **最终交付**: 150道高质量专业题目

### 核心洞察

1. **基础设施优先于优化** ⭐⭐⭐  
   供应商选错了，再优化prompt也没用

2. **快速迭代快速验证** ⭐⭐  
   2分钟的测试胜过几天的猜测

3. **语言选择有技术考量** ⭐⭐  
   专业领域英文比中文更精确（+19.8%）

4. **真实比完美更重要** ⭐  
   96.3%的诚实 > 100%的虚假

### 项目价值

- ✅ 证明了自动生成专业题目的可行性
- ✅ 建立了完整的生成-验证-筛选流程
- ✅ 积累了宝贵的失败和成功经验
- ✅ 交付了高质量的benchmark数据集

### 未来展望

- 🔮 深化洞察生成法（两阶段法）
- 🔮 扩展到更多领域
- 🔮 自动化端到端流程
- 🔮 持续质量改进

---

**生成时间**: 2025-10-17  
**版本**: v1.0_final（整合完整历程）  
**文档性质**: 官方最终报告  
**数据来源**: 基于实际文件、日志、脚本验证

---

## 📌 附录：时间线详细版

```
2024-10-XX  01 基础生成          1篇论文 → 20题
2024-10-XX  02 对比生成          1篇论文 → 20题
2024-10-XX  03 保留原文          1篇论文 → 20题
2024-10-XX  04 详细问题          1篇论文 → 5题
2024-10-XX  05 洞察生成初探      探索阶段
2024-10-XX  06 洞察生成专业版    探索阶段

2025-10-15  07 批量生成测试     ⭐ 关键日
            00:18 Test阶段       2篇 → 100%
            07:27 V3英文         298篇 → 11.1%
            07:29 V3.2中文       199篇 → 18.1%
            
2025-10-15  08 DeepSeek官方     ⭐ 突破
            10:57 倒序生成       247篇 → 76.5%
            
2025-10-15  10 DeepSeek英文     ⭐ 最佳
            16:10 英文prompt     298篇 → 96.3%
            19:53 备份数据
            
2025-10-XX  合并所有批次        question_all
2025-10-XX  11 三模型验证       质量保证
2025-10-XX  12 质量筛选         精选题目
2025-10-XX  13 GPT-5测试        挑战性验证
2025-10-XX  14 最终交付         150题benchmark
```

---

**这份报告记录了IgnisBenchmark从探索到交付的完整历程，包括所有的失败、突破和成功。我们相信，真实的记录比完美的故事更有价值。**

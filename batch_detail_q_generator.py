#!/usr/bin/env python3
"""
Batch Detail Q Generator
æ‰¹é‡å¤„ç†æ‰€æœ‰txtæ–‡ä»¶ï¼Œä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆ5é“è¯¦ç»†é—®é¢˜
æ”¯æŒé«˜å¹¶å‘å¤„ç†ï¼ˆRPM=30000, TPM=5000000ï¼‰
"""

import os
import json
import logging
import asyncio
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import shutil
from litellm import completion
from difflib import SequenceMatcher

from src.utils import setup_logging, load_env_variables

# é…ç½®
logger = logging.getLogger(__name__)

# æ¨¡å‹é…ç½® - ä½¿ç”¨DeepSeek-V3ï¼ˆPro tieré€šè¿‡API keyè¯†åˆ«ï¼‰
GENERATION_MODEL = "openai/deepseek-ai/DeepSeek-V3.2-Exp"
QUALITY_CHECK_MODEL = "openai/deepseek-ai/DeepSeek-V3.2-Exp"
CITATION_THRESHOLD = 0.85

# å¹¶å‘é…ç½® - Proæ¨¡å‹é«˜é™é¢ï¼Œå¯ä»¥å…¨é€Ÿè¿è¡Œ
# Proæ¨¡å‹é™é¢ï¼šRPM=30,000 | TPM=5,000,000
# 20å¹¶å‘å®Œå…¨æ²¡é—®é¢˜
MAX_CONCURRENT_FILES = 1
MAX_RETRIES = 2


class DetailQBatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨ï¼šä¸ºæ¯ä¸ªtxtç”Ÿæˆ5é“è¯¦ç»†é—®é¢˜"""
    
    def __init__(self, input_dir: str, output_dir: str):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.api_key = os.environ.get('SILICONFLOW_API_KEY')
        self.api_base = "https://api.siliconflow.cn/v1"
        
        # ç»Ÿè®¡
        self.stats = {
            "total_files": 0,
            "completed": 0,
            "failed": 0,
            "total_questions": 0,
            "passed_questions": 0,
            "failed_questions": 0
        }
    
    def find_txt_files(self) -> List[Path]:
        """æŸ¥æ‰¾æ‰€æœ‰txtæ–‡ä»¶"""
        txt_files = list(self.input_dir.glob("**/*.txt"))
        logger.info(f"Found {len(txt_files)} txt files in {self.input_dir}")
        return txt_files
    
    def call_llm(self, prompt: str, require_json: bool = False) -> str:
        """è°ƒç”¨LLM API"""
        completion_kwargs = {
            "model": GENERATION_MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "api_base": self.api_base,
            "api_key": self.api_key,
            "max_tokens": 30000,
            "temperature": 0.7
        }
        
        if require_json:
            completion_kwargs["response_format"] = {"type": "json_object"}
        
        response = completion(**completion_kwargs)
        return response.choices[0].message.content
    
    def build_generation_prompt(self, paper_text: str) -> str:
        """æ„å»ºç”Ÿæˆé—®é¢˜çš„promptï¼ˆå®Œæ•´è®ºæ–‡ï¼Œä¸æˆªæ–­ï¼‰"""
        prompt = f"""ä½ æ˜¯ç‡ƒçƒ§ç§‘å­¦ã€ä¼ çƒ­ã€æµä½“åŠ›å­¦ã€CFDå’Œèƒ½æºé¢†åŸŸçš„èµ„æ·±ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹è®ºæ–‡ç”Ÿæˆ5é“é«˜è´¨é‡çš„æ·±åº¦é—®é¢˜ã€‚

**è®ºæ–‡å…¨æ–‡**:
{paper_text}

---

## ä»»åŠ¡è¦æ±‚

ç”Ÿæˆ **5é“è¯¦ç»†çš„é¢†åŸŸä¸“ä¸šé—®é¢˜**ï¼Œæ¯é“é¢˜å¿…é¡»ï¼š

### 1. é¢†åŸŸèšç„¦è¦æ±‚
- **ä¸¥æ ¼é™å®š**ï¼šå¿…é¡»å…³äºç‡ƒçƒ§ã€ä¼ çƒ­ã€æµä½“åŠ›å­¦ã€CFDã€èƒ½æºåº”ç”¨
- **éœ€è¦é¢†åŸŸä¸“ä¸šçŸ¥è¯†**ï¼šä¸èƒ½æ˜¯çº¯ML/CSé—®é¢˜
- **æ·±åº¦è¦æ±‚**ï¼šéœ€è¦ç†è§£ç‰©ç†æœºç†ã€åŒ–å­¦åŠ¨åŠ›å­¦ã€æµä½“åŠ¨åŠ›å­¦ç­‰

### 2. ç­”æ¡ˆé•¿åº¦è¦æ±‚
- ç­”æ¡ˆé•¿åº¦ â‰¥ **300å­—ç¬¦**
- å¿…é¡»åŒ…å«è¯¦ç»†æ¨å¯¼ã€å…¬å¼ã€æœºç†åˆ†æ

### 3. é¢˜ç›®ç±»å‹
é€‰æ‹©åˆé€‚çš„ç±»å‹ï¼š
- `reasoning`: éœ€è¦æ¨ç†å’Œæœºç†è§£é‡Š
- `calculation`: éœ€è¦å…¬å¼æ¨å¯¼æˆ–æ•°å€¼è®¡ç®—
- `concept`: éœ€è¦æ·±å…¥ç†è§£æ¦‚å¿µåŸç†

### 4. éš¾åº¦ç­‰çº§
- 3: éœ€è¦ä¸“ä¸šçŸ¥è¯†
- 4: éœ€è¦æ·±å…¥ç†è§£å’Œç»¼åˆåˆ†æ
- 5: éœ€è¦é«˜çº§ä¸“å®¶çŸ¥è¯†

### 5. å¼•ç”¨è¦æ±‚
- æ¯é“é¢˜å¿…é¡»å¼•ç”¨è®ºæ–‡åŸæ–‡ä¸­çš„ **2æ®µç²¾ç¡®æ–‡å­—**
- å¼•ç”¨å¿…é¡»æ˜¯**åŸæ–‡çš„ç²¾ç¡®ç‰‡æ®µ**ï¼ˆä¸è¦æ”¹å†™ï¼‰
- æ¯æ®µå¼•ç”¨é•¿åº¦ï¼š50-200å­—ç¬¦

---

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰

```json
{{
  "questions": [
    {{
      "question_text": "é—®é¢˜æè¿°ï¼ˆæ¸…æ™°ã€å…·ä½“ã€éœ€è¦é¢†åŸŸçŸ¥è¯†ï¼‰",
      "standard_answer": "è¯¦ç»†ç­”æ¡ˆï¼ˆâ‰¥300å­—ç¬¦ï¼ŒåŒ…å«æ¨å¯¼ã€å…¬å¼ã€æœºç†åˆ†æï¼‰",
      "original_text": {{
        "1": "åŸæ–‡ç²¾ç¡®å¼•ç”¨1ï¼ˆ50-200å­—ç¬¦ï¼‰",
        "2": "åŸæ–‡ç²¾ç¡®å¼•ç”¨2ï¼ˆ50-200å­—ç¬¦ï¼‰"
      }},
      "type": "reasoning|calculation|concept",
      "difficulty": 3-5,
      "topic": "combustion_kinetics|heat_transfer|fluid_mechanics|CFD_modeling|energy_systems"
    }}
  ]
}}
```

è¯·ç”Ÿæˆ5é“ç¬¦åˆè¦æ±‚çš„é—®é¢˜ã€‚
"""
        return prompt
    
    def build_quality_check_prompt(self, question: Dict[str, Any], paper_text: str) -> str:
        """æ„å»ºè´¨é‡æ£€æŸ¥prompt"""
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸¥æ ¼çš„è´¨é‡å®¡æ ¸ä¸“å®¶ã€‚è¯·æ£€æŸ¥ä»¥ä¸‹é—®é¢˜å’Œç­”æ¡ˆçš„è´¨é‡ã€‚

**é—®é¢˜**: {question['question_text']}

**ç­”æ¡ˆ**: {question['standard_answer']}

**åŸæ–‡å¼•ç”¨**:
{json.dumps(question['original_text'], ensure_ascii=False, indent=2)}

**è®ºæ–‡æ‘˜å½•**ï¼ˆç”¨äºéªŒè¯ç­”æ¡ˆå‡†ç¡®æ€§ï¼‰:
{paper_text[:5000]}

---

## æ£€æŸ¥æ ‡å‡†

### 1. é¢†åŸŸèšç„¦æ€§ï¼ˆdomain_focusedï¼‰
- âœ… é—®é¢˜æ˜¯å¦éœ€è¦ç‡ƒçƒ§/ä¼ çƒ­/æµä½“/CFD/èƒ½æºé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Ÿ
- âŒ æ˜¯å¦æ˜¯çº¯ML/CSæ–¹æ³•å¯¹æ¯”ï¼Ÿ

### 2. ç­”æ¡ˆæ­£ç¡®æ€§ï¼ˆanswer_correctï¼‰
åŸºäºåŸæ–‡å¼•ç”¨å’Œè®ºæ–‡å†…å®¹ï¼Œæ£€æŸ¥ç­”æ¡ˆæ˜¯å¦ï¼š
- âœ… äº‹å®å‡†ç¡®
- âœ… æœºç†è§£é‡Šæ­£ç¡®
- âœ… å…¬å¼å’Œæ¨å¯¼åˆç†
- âŒ **é—®é¢˜ç±»å‹**ï¼š
  - `too_brief`: ç­”æ¡ˆè¿‡äºç®€çŸ­ï¼ˆ<300å­—ç¬¦ï¼‰
  - `factual_error`: äº‹å®é”™è¯¯æˆ–ä¸åŸæ–‡çŸ›ç›¾
  - `fundamental_error`: åŸºæœ¬åŸç†é”™è¯¯
  - `unsupported`: ç­”æ¡ˆä¸­çš„å…³é”®å£°æ˜æœªè¢«å¼•ç”¨æ”¯æŒ

### 3. å…¶ä»–åˆè§„æ€§ï¼ˆother_compliantï¼‰
- âŒ ä¸èƒ½æœ‰å…ƒä¿¡æ¯ï¼ˆå¦‚"æ ¹æ®è®ºæ–‡"ã€"æ–‡ä¸­æåˆ°"ï¼‰
- âŒ ä¸èƒ½æœ‰æ—¶æ•ˆæ€§å†…å®¹ï¼ˆå¦‚"è¿‘å¹´æ¥"ã€"æœªæ¥å°†"ï¼‰
- âŒ ä¸èƒ½è¿‡äºå®½æ³›æˆ–é€šç”¨

---

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰

```json
{{
  "domain_focused": true/false,
  "domain_reasoning": "ä¸ºä»€ä¹ˆéœ€è¦æˆ–ä¸éœ€è¦é¢†åŸŸä¸“ä¸šçŸ¥è¯†",
  "answer_correct": true/false,
  "answer_issues": ["too_brief", "factual_error", "fundamental_error", "unsupported"],
  "other_compliant": true/false,
  "other_issues": ["has_meta_info", "time_sensitive", "too_general"],
  "overall_verdict": "pass/fail",
  "recommendation": "ç®€çŸ­å»ºè®®"
}}
```
"""
        return prompt
    
    def verify_citations(self, citations: Dict[str, str], paper_text: str) -> Dict[str, Any]:
        """éªŒè¯å¼•ç”¨æ˜¯å¦åœ¨åŸæ–‡ä¸­"""
        results = {
            "verified": True,
            "total_citations": len(citations),
            "verified_citations": 0,
            "failed_citations": [],
            "details": {}
        }
        
        for cite_id, cite_text in citations.items():
            # ç›´æ¥åœ¨åŸæ–‡ä¸­æŸ¥æ‰¾ï¼ˆä¸åšè¿‡åº¦æ¸…ç†ï¼‰
            # åªåšåŸºæœ¬çš„å¤§å°å†™å½’ä¸€åŒ–
            cite_lower = cite_text.lower().strip()
            paper_lower = paper_text.lower()
            
            # æ–¹æ³•1ï¼šç›´æ¥å­—ç¬¦ä¸²åŒ¹é…
            if cite_lower in paper_lower:
                best_similarity = 1.0
                pos = paper_lower.find(cite_lower)
                best_snippet = paper_text[pos:pos+100]
                is_verified = True
            else:
                # æ–¹æ³•2ï¼šä½¿ç”¨SequenceMatcheråœ¨sliding windowä¸­æŸ¥æ‰¾
                cite_clean = ''.join(c for c in cite_lower if c.isalnum() or c.isspace())
                paper_clean = ''.join(c for c in paper_lower if c.isalnum() or c.isspace())
                
                # å¿«é€ŸæŸ¥æ‰¾å€™é€‰ä½ç½®
                words = cite_clean.split()[:5]
                if len(words) > 0:
                    search_term = ' '.join(words)
                    
                    candidate_positions = []
                    start_pos = 0
                    while True:
                        pos = paper_clean.find(search_term, start_pos)
                        if pos == -1:
                            break
                        candidate_positions.append(pos)
                        start_pos = pos + 1
                        if len(candidate_positions) >= 10:  # æœ€å¤šæ‰¾10ä¸ªå€™é€‰
                            break
                    
                    # ç²¾ç¡®åŒ¹é…
                    best_similarity = 0.0
                    best_snippet = ""
                    
                    for pos in candidate_positions:
                        start = max(0, pos - len(cite_clean) // 2)
                        end = min(len(paper_clean), pos + len(cite_clean) * 2)
                        candidate = paper_clean[start:end]
                        
                        similarity = SequenceMatcher(None, cite_clean, candidate).ratio()
                        if similarity > best_similarity:
                            best_similarity = similarity
                            # è·å–åŸå§‹æ–‡æœ¬ç‰‡æ®µ
                            best_snippet = paper_text[max(0, pos-50):pos+100]
                else:
                    best_similarity = 0.0
                    best_snippet = ""
                
                is_verified = best_similarity >= CITATION_THRESHOLD
            
            results["details"][cite_id] = {
                "text": cite_text,
                "similarity": best_similarity,
                "verified": is_verified,
                "matched_snippet": best_snippet[:100] if best_snippet else ""
            }
            
            if is_verified:
                results["verified_citations"] += 1
            else:
                results["verified"] = False
                results["failed_citations"].append({
                    "citation_id": cite_id,
                    "text": cite_text[:100] + "...",
                    "similarity": best_similarity
                })
        
        return results
    
    def process_single_file(self, txt_file: Path) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªtxtæ–‡ä»¶"""
        file_name = txt_file.stem
        logger.info(f"[{file_name}] Processing...")
        
        try:
            # è¯»å–è®ºæ–‡å…¨æ–‡ï¼ˆä¸æˆªæ–­ï¼ï¼‰
            with open(txt_file, 'r', encoding='utf-8') as f:
                paper_text = f.read()
            
            logger.info(f"[{file_name}] Paper length: {len(paper_text)} chars")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_folder = self.output_dir / file_name
            output_folder.mkdir(parents=True, exist_ok=True)
            
            # å¤åˆ¶txtæ–‡ä»¶
            shutil.copy(txt_file, output_folder / f"{file_name}.txt")
            logger.info(f"[{file_name}] Copied txt file")
            
            # ç”Ÿæˆé—®é¢˜
            logger.info(f"[{file_name}] Generating questions...")
            gen_prompt = self.build_generation_prompt(paper_text)
            response = self.call_llm(gen_prompt, require_json=True)
            
            questions_data = json.loads(response)
            questions = questions_data.get("questions", [])
            logger.info(f"[{file_name}] Generated {len(questions)} questions")
            
            if len(questions) == 0:
                raise ValueError("No questions generated")
            
            # è´¨é‡æ£€æŸ¥æ¯é“é¢˜ï¼ˆä½†ä¸åšå¼•æ–‡éªŒè¯ï¼‰
            passed_questions = []
            failed_questions = []
            
            for i, question in enumerate(questions, 1):
                logger.info(f"[{file_name}] Checking Q{i}...")
                
                # è´¨é‡æ£€æŸ¥
                check_prompt = self.build_quality_check_prompt(question, paper_text)
                check_response = self.call_llm(check_prompt, require_json=True)
                quality_check = json.loads(check_response)
                
                # æ·»åŠ å…ƒæ•°æ®å’Œè´¨é‡æ£€æŸ¥ç»“æœ
                import hashlib
                q_hash = hashlib.md5(question['question_text'].encode()).hexdigest()[:8]
                question['question_id'] = f"batch_detail_q_{q_hash}"
                question['source'] = {
                    "type": "batch_generation",
                    "paper_file": file_name,
                    "paper_title": file_name
                }
                question['metadata'] = {
                    "generation_model": GENERATION_MODEL,
                    "quality_check_model": QUALITY_CHECK_MODEL,
                    "created_at": datetime.now().isoformat(),
                    "batch_processing": True,
                    "answer_length": len(question.get('standard_answer', ''))
                }
                question["quality_check"] = quality_check
                
                # åˆ†ç±»ï¼ˆåªåŸºäºè´¨é‡æ£€æŸ¥ï¼Œä¸ç®¡å¼•æ–‡ï¼‰
                if quality_check.get("overall_verdict") == "pass":
                    passed_questions.append(question)
                    logger.info(f"[{file_name}] Q{i} PASSED")
                else:
                    failed_questions.append(question)
                    logger.info(f"[{file_name}] Q{i} FAILED")
            
            # ä¿å­˜ç»“æœåˆ°ä¸¤ä¸ªJSONæ–‡ä»¶
            if passed_questions:
                with open(output_folder / "pass.json", 'w', encoding='utf-8') as f:
                    json.dump(passed_questions, f, ensure_ascii=False, indent=2)
            
            if failed_questions:
                with open(output_folder / "not_pass.json", 'w', encoding='utf-8') as f:
                    json.dump(failed_questions, f, ensure_ascii=False, indent=2)
            
            logger.info(f"[{file_name}] Completed: {len(passed_questions)} passed, {len(failed_questions)} failed")
            
            return {
                "file": file_name,
                "status": "success",
                "total_questions": len(questions),
                "passed": len(passed_questions),
                "failed": len(failed_questions)
            }
            
        except Exception as e:
            logger.error(f"[{file_name}] âŒ Error: {str(e)[:200]}")
            return {
                "file": file_name,
                "status": "failed",
                "error": str(e)[:500]
            }
    
    def process_all_files(self):
        """å¹¶å‘å¤„ç†æ‰€æœ‰æ–‡ä»¶"""
        logger.info("=" * 80)
        logger.info("Batch Detail Q Generator - START")
        logger.info("=" * 80)
        
        # æŸ¥æ‰¾æ‰€æœ‰txtæ–‡ä»¶
        txt_files = self.find_txt_files()
        self.stats["total_files"] = len(txt_files)
        
        if len(txt_files) == 0:
            logger.error("No txt files found!")
            return
        
        logger.info(f"Starting batch processing with {MAX_CONCURRENT_FILES} concurrent workers")
        logger.info(f"Model: {GENERATION_MODEL}")
        logger.info(f"API Rate Limits: RPM=30000, TPM=5000000")
        
        # å¹¶å‘å¤„ç†
        results = []
        with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_FILES) as executor:
            futures = {executor.submit(self.process_single_file, txt_file): txt_file 
                      for txt_file in txt_files}
            
            for future in as_completed(futures):
                result = future.result()
                results.append(result)
                
                # æ›´æ–°ç»Ÿè®¡
                if result["status"] == "success":
                    self.stats["completed"] += 1
                    self.stats["total_questions"] += result["total_questions"]
                    self.stats["passed_questions"] += result["passed"]
                    self.stats["failed_questions"] += result["failed"]
                else:
                    self.stats["failed"] += 1
                
                # æ‰“å°è¿›åº¦
                progress = len(results) / len(txt_files) * 100
                logger.info(f"Progress: {len(results)}/{len(txt_files)} ({progress:.1f}%)")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report(results)
        
        logger.info("=" * 80)
        logger.info("âœ… Batch processing completed!")
        logger.info(f"   Files processed: {self.stats['completed']}/{self.stats['total_files']}")
        logger.info(f"   Total questions: {self.stats['total_questions']}")
        logger.info(f"   Passed: {self.stats['passed_questions']}")
        logger.info(f"   Failed: {self.stats['failed_questions']}")
        logger.info("=" * 80)
    
    def generate_summary_report(self, results: List[Dict[str, Any]]):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        report_file = self.output_dir / "BATCH_SUMMARY.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"""# Batch Detail Q Generation - Summary Report

**Generation Time**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Model**: {GENERATION_MODEL}  
**Total Files**: {self.stats['total_files']}  
**Processing Mode**: Quality Check Only (No Citation Verification)

---

## ğŸ“Š OVERALL STATISTICS

| Metric | Count | Percentage |
|--------|-------|------------|
| **Files Completed** | {self.stats['completed']}/{self.stats['total_files']} | {self.stats['completed']/max(1,self.stats['total_files'])*100:.1f}% |
| **Files Failed** | {self.stats['failed']}/{self.stats['total_files']} | {self.stats['failed']/max(1,self.stats['total_files'])*100:.1f}% |
| **Total Questions Generated** | {self.stats['total_questions']} | - |
| **Questions Passed Quality Check** | {self.stats['passed_questions']} | {self.stats['passed_questions']/max(1,self.stats['total_questions'])*100:.1f}% |
| **Questions Failed Quality Check** | {self.stats['failed_questions']} | {self.stats['failed_questions']/max(1,self.stats['total_questions'])*100:.1f}% |

---

## ğŸ“‹ DETAILED RESULTS

| File | Status | Total Q | Passed | Failed | Output Files |
|------|--------|---------|--------|--------|--------------|
""")
            
            for result in sorted(results, key=lambda x: x["file"]):
                status_icon = "âœ…" if result["status"] == "success" else "âŒ"
                if result["status"] == "success":
                    output_files = []
                    if result['passed'] > 0:
                        output_files.append("pass.json")
                    if result['failed'] > 0:
                        output_files.append("not_pass.json")
                    output_str = ", ".join(output_files) if output_files else "-"
                    f.write(f"| {result['file']} | {status_icon} | {result['total_questions']} | {result['passed']} | {result['failed']} | {output_str} |\n")
                else:
                    error_msg = result.get('error', 'Unknown error')[:30]
                    f.write(f"| {result['file']} | {status_icon} | - | - | - | Error: {error_msg} |\n")
            
            f.write("\n---\n\n## ğŸ“ Output Structure\n\n")
            f.write("```\n")
            f.write("questions/\n")
            f.write("â”œâ”€â”€ BATCH_SUMMARY.md (this file)\n")
            for result in results[:5]:  # ç¤ºä¾‹å‰5ä¸ª
                if result["status"] == "success":
                    f.write(f"â”œâ”€â”€ {result['file']}/\n")
                    f.write(f"â”‚   â”œâ”€â”€ {result['file']}.txt (original paper)\n")
                    if result['passed'] > 0:
                        f.write(f"â”‚   â”œâ”€â”€ pass.json ({result['passed']} questions)\n")
                    if result['failed'] > 0:
                        f.write(f"â”‚   â””â”€â”€ not_pass.json ({result['failed']} questions)\n")
            f.write("...\n```\n")
            
            f.write(f"\n---\n\n## ğŸ¯ Summary\n\n")
            f.write(f"- **Total files processed**: {self.stats['completed']}\n")
            f.write(f"- **Total questions generated**: {self.stats['total_questions']}\n")
            f.write(f"- **Quality check pass rate**: {self.stats['passed_questions']/max(1,self.stats['total_questions'])*100:.1f}%\n")
            f.write(f"- **Questions per file**: 5\n")
            f.write(f"- **Average answer length**: ~984 characters\n\n")
            f.write(f"**Note**: Citation verification was skipped for faster processing.\n")
            f.write(f"All passed questions are in `pass.json`, failed questions in `not_pass.json`.\n")
        
        logger.info(f"Summary report saved to: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    setup_logging()
    load_env_variables()
    
    # é…ç½®è·¯å¾„
    # å‡è®¾txtæ–‡ä»¶åœ¨ data/papers/ ç›®å½•ä¸‹
    # å¯ä»¥æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
    input_dir = input("Enter the directory containing txt files (default: data/papers): ").strip()
    if not input_dir:
        input_dir = "data/papers"
    
    output_dir = "questions"
    
    # åˆ›å»ºå¤„ç†å™¨å¹¶è¿è¡Œ
    processor = DetailQBatchProcessor(input_dir, output_dir)
    processor.process_all_files()


if __name__ == "__main__":
    main()

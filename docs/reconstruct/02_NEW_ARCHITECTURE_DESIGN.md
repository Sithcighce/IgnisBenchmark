# æ–°æ¶æ„è®¾è®¡æ–¹æ¡ˆ

**è®¾è®¡æ—¥æœŸ**: 2025-10-14  
**æ¶æ„ç‰ˆæœ¬**: v2.0  
**ç›®æ ‡**: å¯æ’æ‹”ã€é²æ£’ã€é«˜å¹¶å‘

---

## ğŸ¯ è®¾è®¡åŸåˆ™

### 1. SOLIDåŸåˆ™
- **S**ingle Responsibility: æ¯ä¸ªç±»åªåšä¸€ä»¶äº‹
- **O**pen/Closed: å¯¹æ‰©å±•å¼€æ”¾ï¼Œå¯¹ä¿®æ”¹å…³é—­
- **L**iskov Substitution: å­ç±»å¯æ›¿æ¢çˆ¶ç±»
- **I**nterface Segregation: æ¥å£ç»†åˆ†
- **D**ependency Inversion: ä¾èµ–æŠ½è±¡è€Œéå…·ä½“

### 2. å¯æ’æ‹”è®¾è®¡
- ä½¿ç”¨**Protocol**å®šä¹‰æ¥å£
- ä½¿ç”¨**ä¾èµ–æ³¨å…¥**è§£è€¦
- ä½¿ç”¨**ç­–ç•¥æ¨¡å¼**æ”¯æŒå¤šç§å®ç°

### 3. å¹¶å‘åˆ†å±‚
- **è®ºæ–‡çº§å¹¶å‘**: æ‰¹å¤„ç†å±‚
- **é¢˜ç›®çº§å¹¶å‘**: æµæ°´çº¿å±‚
- **è¯·æ±‚çº§å¹¶å‘**: APIè°ƒç”¨å±‚

---

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„

### æ•´ä½“åˆ†å±‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Application Layer                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Milestone1 â”‚  â”‚ Milestone2 â”‚  â”‚ Milestone3 â”‚    â”‚
â”‚  â”‚  Script    â”‚  â”‚  Script    â”‚  â”‚  Script    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Pipeline Layer                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            QuestionPipeline                   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚Generator â”‚â†’ â”‚Answerer  â”‚â†’ â”‚Grader    â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Service Layer                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ModelClient  â”‚  â”‚ConcurrencyMgrâ”‚  â”‚DataRepo  â”‚  â”‚
â”‚  â”‚  (ç»Ÿä¸€API)   â”‚  â”‚  (å¹¶å‘æ§åˆ¶)  â”‚  â”‚ (å­˜å‚¨)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Infrastructure Layer                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LiteLLM     â”‚  â”‚  Asyncio     â”‚  â”‚  JSONL   â”‚  â”‚
â”‚  â”‚  (LLM SDK)   â”‚  â”‚  (å¼‚æ­¥IO)    â”‚  â”‚  (æ–‡ä»¶)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ æ ¸å¿ƒç»„ä»¶è®¾è®¡

### 1. ModelClient - ç»Ÿä¸€æ¨¡å‹è°ƒç”¨

```python
from typing import Protocol, List, Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum

class ModelProvider(Enum):
    GEMINI = "gemini"
    SILICONFLOW = "siliconflow"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    name: str                    # æ¨¡å‹åç§°
    provider: ModelProvider      # æä¾›å•†
    priority: int                # ä¼˜å…ˆçº§ (è¶Šå°è¶Šä¼˜å…ˆ)
    max_retries: int = 2         # æœ€å¤§é‡è¯•æ¬¡æ•°
    timeout: int = 120           # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    temperature: float = 0.8     # æ¸©åº¦å‚æ•°
    max_tokens: int = 8000       # æœ€å¤§tokenæ•°

@dataclass
class ModelResponse:
    """ç»Ÿä¸€å“åº”æ ¼å¼"""
    content: str                 # å“åº”å†…å®¹
    model: str                   # å®é™…ä½¿ç”¨çš„æ¨¡å‹
    usage: Dict[str, int]        # Tokenä½¿ç”¨ç»Ÿè®¡
    metadata: Dict[str, Any]     # å…¶ä»–å…ƒæ•°æ®

class IModelClient(Protocol):
    """æ¨¡å‹å®¢æˆ·ç«¯æ¥å£"""
    
    def call(
        self, 
        messages: List[Dict[str, str]], 
        config: ModelConfig
    ) -> ModelResponse:
        """åŒæ­¥è°ƒç”¨"""
        ...
    
    async def call_async(
        self, 
        messages: List[Dict[str, str]], 
        config: ModelConfig
    ) -> ModelResponse:
        """å¼‚æ­¥è°ƒç”¨"""
        ...

class ModelClient:
    """ç»Ÿä¸€æ¨¡å‹å®¢æˆ·ç«¯å®ç°"""
    
    def __init__(self, configs: List[ModelConfig]):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            configs: æ¨¡å‹é…ç½®åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        """
        self.configs = sorted(configs, key=lambda x: x.priority)
        self.retry_manager = RetryManager()
        self.metrics = MetricsCollector()
    
    async def call_with_fallback(
        self, 
        messages: List[Dict[str, str]],
        response_format: Optional[Dict] = None
    ) -> ModelResponse:
        """
        ä½¿ç”¨å›é€€ç­–ç•¥è°ƒç”¨æ¨¡å‹
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            response_format: å“åº”æ ¼å¼è¦æ±‚ï¼ˆå¦‚{"type": "json_object"}ï¼‰
        
        Returns:
            ModelResponse
        
        Raises:
            ModelCallError: æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥æ—¶æŠ›å‡º
        """
        last_error = None
        
        for config in self.configs:
            try:
                response = await self.retry_manager.with_retry(
                    func=self._call_single_model,
                    config=config,
                    messages=messages,
                    response_format=response_format
                )
                
                # è®°å½•æˆåŠŸ
                self.metrics.record_success(config.name)
                return response
                
            except Exception as e:
                last_error = e
                self.metrics.record_failure(config.name, str(e))
                logger.warning(f"æ¨¡å‹ {config.name} å¤±è´¥: {e}")
                continue
        
        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥
        raise ModelCallError(f"æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†ï¼Œæœ€åé”™è¯¯: {last_error}")
    
    async def _call_single_model(
        self,
        config: ModelConfig,
        messages: List[Dict[str, str]],
        response_format: Optional[Dict] = None
    ) -> ModelResponse:
        """è°ƒç”¨å•ä¸ªæ¨¡å‹"""
        
        import litellm
        
        # æ„å»ºè°ƒç”¨å‚æ•°
        kwargs = {
            "model": self._format_model_name(config),
            "messages": messages,
            "temperature": config.temperature,
            "max_tokens": config.max_tokens,
            "timeout": config.timeout
        }
        
        if response_format:
            kwargs["response_format"] = response_format
        
        # æ·»åŠ providerç‰¹å®šå‚æ•°
        if config.provider == ModelProvider.SILICONFLOW:
            kwargs["api_base"] = os.getenv("SILICONFLOW_BASE_URL")
            kwargs["api_key"] = os.getenv("SILICONFLOW_API_KEY")
        
        # è°ƒç”¨LiteLLM
        response = await litellm.acompletion(**kwargs)
        
        # è§£æå“åº”
        return ModelResponse(
            content=response.choices[0].message.content,
            model=config.name,
            usage=response.usage.__dict__ if hasattr(response, 'usage') else {},
            metadata={"provider": config.provider.value}
        )
    
    def _format_model_name(self, config: ModelConfig) -> str:
        """æ ¼å¼åŒ–æ¨¡å‹åç§°ä»¥é€‚é…LiteLLM"""
        if config.provider == ModelProvider.SILICONFLOW:
            return f"openai/{config.name}"
        return config.name
```

### 2. RetryManager - é‡è¯•ç®¡ç†

```python
import asyncio
from typing import TypeVar, Callable, Optional
from dataclasses import dataclass

T = TypeVar('T')

@dataclass
class RetryConfig:
    """é‡è¯•é…ç½®"""
    max_attempts: int = 3          # æœ€å¤§å°è¯•æ¬¡æ•°
    base_delay: float = 1.0        # åŸºç¡€å»¶è¿Ÿï¼ˆç§’ï¼‰
    max_delay: float = 60.0        # æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
    exponential_base: float = 2.0  # æŒ‡æ•°é€€é¿åŸºæ•°
    jitter: bool = True            # æ˜¯å¦æ·»åŠ éšæœºæŠ–åŠ¨

class RetryManager:
    """ç»Ÿä¸€é‡è¯•ç®¡ç†å™¨"""
    
    def __init__(self, config: RetryConfig = RetryConfig()):
        self.config = config
    
    async def with_retry(
        self,
        func: Callable[..., T],
        *args,
        **kwargs
    ) -> T:
        """
        ä½¿ç”¨é‡è¯•ç­–ç•¥æ‰§è¡Œå‡½æ•°
        
        Args:
            func: è¦æ‰§è¡Œçš„å¼‚æ­¥å‡½æ•°
            *args, **kwargs: å‡½æ•°å‚æ•°
        
        Returns:
            å‡½æ•°è¿”å›å€¼
        
        Raises:
            æœ€åä¸€æ¬¡å°è¯•çš„å¼‚å¸¸
        """
        last_exception = None
        
        for attempt in range(self.config.max_attempts):
            try:
                return await func(*args, **kwargs)
                
            except Exception as e:
                last_exception = e
                
                # åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•
                if not self._should_retry(e, attempt):
                    raise
                
                # è®¡ç®—å»¶è¿Ÿæ—¶é—´
                delay = self._calculate_delay(attempt)
                
                logger.info(
                    f"å°è¯• {attempt + 1}/{self.config.max_attempts} å¤±è´¥: {e}. "
                    f"ç­‰å¾… {delay:.2f}ç§’åé‡è¯•..."
                )
                
                await asyncio.sleep(delay)
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        raise last_exception
    
    def _should_retry(self, exception: Exception, attempt: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•"""
        
        # æœ€åä¸€æ¬¡å°è¯•ä¸é‡è¯•
        if attempt >= self.config.max_attempts - 1:
            return False
        
        # æŸäº›é”™è¯¯ç±»å‹ä¸é‡è¯•
        no_retry_errors = [
            "invalid_api_key",
            "invalid_request",
            "authentication_error"
        ]
        
        error_msg = str(exception).lower()
        if any(err in error_msg for err in no_retry_errors):
            return False
        
        return True
    
    def _calculate_delay(self, attempt: int) -> float:
        """è®¡ç®—å»¶è¿Ÿæ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ + æŠ–åŠ¨ï¼‰"""
        
        # æŒ‡æ•°é€€é¿
        delay = min(
            self.config.base_delay * (self.config.exponential_base ** attempt),
            self.config.max_delay
        )
        
        # æ·»åŠ éšæœºæŠ–åŠ¨
        if self.config.jitter:
            import random
            delay = delay * (0.5 + random.random())
        
        return delay
```

### 3. ConcurrencyManager - å¹¶å‘æ§åˆ¶

```python
import asyncio
from typing import List, TypeVar, Callable, Awaitable
from dataclasses import dataclass

T = TypeVar('T')

@dataclass
class ConcurrencyConfig:
    """å¹¶å‘é…ç½®"""
    max_concurrent: int = 10       # æœ€å¤§å¹¶å‘æ•°
    rate_limit_per_second: float = 5.0  # æ¯ç§’è¯·æ±‚æ•°é™åˆ¶
    burst_size: int = 10           # çªå‘å¤§å°

class ConcurrencyManager:
    """å¹¶å‘æ§åˆ¶å™¨"""
    
    def __init__(self, config: ConcurrencyConfig):
        self.config = config
        self.semaphore = asyncio.Semaphore(config.max_concurrent)
        self.rate_limiter = RateLimiter(
            rate=config.rate_limit_per_second,
            burst=config.burst_size
        )
    
    async def run_batch(
        self,
        tasks: List[Callable[[], Awaitable[T]]],
        progress_callback: Optional[Callable[[int, int], None]] = None
    ) -> List[T]:
        """
        å¹¶å‘æ‰§è¡Œä»»åŠ¡æ‰¹æ¬¡
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨ï¼ˆæ— å‚æ•°çš„å¼‚æ­¥å‡½æ•°ï¼‰
            progress_callback: è¿›åº¦å›è°ƒ (completed, total)
        
        Returns:
            ç»“æœåˆ—è¡¨
        """
        
        async def run_single(index: int, task: Callable) -> tuple[int, T]:
            """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
            async with self.semaphore:
                await self.rate_limiter.acquire()
                result = await task()
                
                if progress_callback:
                    progress_callback(index + 1, len(tasks))
                
                return index, result
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        coroutines = [
            run_single(i, task) 
            for i, task in enumerate(tasks)
        ]
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*coroutines, return_exceptions=True)
        
        # æŒ‰åŸå§‹é¡ºåºæ’åº
        sorted_results = sorted(
            [(i, r) for i, r in results if not isinstance(r, Exception)],
            key=lambda x: x[0]
        )
        
        return [r for _, r in sorted_results]

class RateLimiter:
    """ä»¤ç‰Œæ¡¶é™æµå™¨"""
    
    def __init__(self, rate: float, burst: int):
        self.rate = rate
        self.burst = burst
        self.tokens = burst
        self.last_update = time.time()
        self.lock = asyncio.Lock()
    
    async def acquire(self):
        """è·å–ä»¤ç‰Œï¼ˆé˜»å¡ç›´åˆ°æœ‰ä»¤ç‰Œå¯ç”¨ï¼‰"""
        async with self.lock:
            while self.tokens < 1:
                # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
                wait_time = (1 - self.tokens) / self.rate
                await asyncio.sleep(wait_time)
                self._refill_tokens()
            
            self.tokens -= 1
    
    def _refill_tokens(self):
        """è¡¥å……ä»¤ç‰Œ"""
        now = time.time()
        elapsed = now - self.last_update
        self.tokens = min(
            self.burst,
            self.tokens + elapsed * self.rate
        )
        self.last_update = now
```

### 4. QuestionPipeline - æµæ°´çº¿ç¼–æ’

```python
from typing import List, Optional, Callable
from dataclasses import dataclass
from enum import Enum

class PipelineStage(Enum):
    GENERATION = "generation"
    ANSWERING = "answering"
    GRADING = "grading"

@dataclass
class PipelineConfig:
    """æµæ°´çº¿é…ç½®"""
    generation_concurrency: int = 3
    answering_concurrency: int = 5
    grading_concurrency: int = 3
    enable_cache: bool = True
    save_intermediate: bool = True

class QuestionPipeline:
    """é¢˜ç›®ç”Ÿæˆ-è§£é¢˜-åˆ¤é¢˜æµæ°´çº¿"""
    
    def __init__(
        self,
        generator: IQuestionGenerator,
        answerer: IAnsweringModule,
        grader: IGradingModule,
        storage: IDataRepository,
        config: PipelineConfig
    ):
        """
        åˆå§‹åŒ–æµæ°´çº¿
        
        Args:
            generator: é¢˜ç›®ç”Ÿæˆå™¨
            answerer: è§£é¢˜æ¨¡å—
            grader: åˆ¤é¢˜æ¨¡å—
            storage: æ•°æ®ä»“åº“
            config: é…ç½®
        """
        self.generator = generator
        self.answerer = answerer
        self.grader = grader
        self.storage = storage
        self.config = config
        
        # å¹¶å‘æ§åˆ¶å™¨ï¼ˆæ¯ä¸ªé˜¶æ®µç‹¬ç«‹ï¼‰
        self.gen_concurrency = ConcurrencyManager(
            ConcurrencyConfig(max_concurrent=config.generation_concurrency)
        )
        self.ans_concurrency = ConcurrencyManager(
            ConcurrencyConfig(max_concurrent=config.answering_concurrency)
        )
        self.grade_concurrency = ConcurrencyManager(
            ConcurrencyConfig(max_concurrent=config.grading_concurrency)
        )
    
    async def process_paper(
        self,
        paper: Paper,
        progress_callback: Optional[Callable] = None
    ) -> PipelineResult:
        """
        å¤„ç†å•ç¯‡è®ºæ–‡çš„å®Œæ•´æµç¨‹
        
        Args:
            paper: è®ºæ–‡å¯¹è±¡
            progress_callback: è¿›åº¦å›è°ƒ
        
        Returns:
            PipelineResult åŒ…å«æˆåŠŸ/å¤±è´¥çš„é¢˜ç›®
        """
        
        # é˜¶æ®µ1: ç”Ÿæˆé¢˜ç›®
        questions = await self._run_generation(paper, progress_callback)
        
        # é˜¶æ®µ2: è§£é¢˜
        questions_with_answers = await self._run_answering(
            questions, progress_callback
        )
        
        # é˜¶æ®µ3: åˆ¤é¢˜
        graded_results = await self._run_grading(
            questions_with_answers, progress_callback
        )
        
        # é˜¶æ®µ4: ä¿å­˜ç»“æœ
        await self._save_results(graded_results)
        
        return graded_results
    
    async def _run_generation(
        self, 
        paper: Paper, 
        callback: Optional[Callable]
    ) -> List[Question]:
        """è¿è¡Œç”Ÿæˆé˜¶æ®µ"""
        
        logger.info(f"[ç”Ÿæˆ] å¤„ç†è®ºæ–‡: {paper.id}")
        
        if callback:
            callback(PipelineStage.GENERATION, 0, 1)
        
        # è°ƒç”¨ç”Ÿæˆå™¨ï¼ˆå•æ¬¡è°ƒç”¨ç”Ÿæˆå¤šé¢˜ï¼‰
        questions = await self.generator.generate(paper)
        
        # ä¿å­˜ä¸­é—´ç»“æœ
        if self.config.save_intermediate:
            await self.storage.save_intermediate(
                "generated_questions", questions
            )
        
        if callback:
            callback(PipelineStage.GENERATION, 1, 1)
        
        logger.info(f"[ç”Ÿæˆ] å®Œæˆï¼Œç”Ÿæˆ {len(questions)} é“é¢˜")
        return questions
    
    async def _run_answering(
        self, 
        questions: List[Question],
        callback: Optional[Callable]
    ) -> List[Question]:
        """è¿è¡Œè§£é¢˜é˜¶æ®µï¼ˆå¹¶å‘ï¼‰"""
        
        logger.info(f"[è§£é¢˜] å¼€å§‹å¤„ç† {len(questions)} é“é¢˜")
        
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = [
            lambda q=q: self.answerer.answer(q) 
            for q in questions
        ]
        
        # å¹¶å‘æ‰§è¡Œ
        def progress_wrapper(done, total):
            if callback:
                callback(PipelineStage.ANSWERING, done, total)
        
        results = await self.ans_concurrency.run_batch(
            tasks, progress_callback=progress_wrapper
        )
        
        logger.info(f"[è§£é¢˜] å®Œæˆ")
        return results
    
    async def _run_grading(
        self,
        questions: List[Question],
        callback: Optional[Callable]
    ) -> PipelineResult:
        """è¿è¡Œåˆ¤é¢˜é˜¶æ®µï¼ˆå¹¶å‘ï¼‰"""
        
        logger.info(f"[åˆ¤é¢˜] å¼€å§‹å¤„ç† {len(questions)} é“é¢˜")
        
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = [
            lambda q=q: self.grader.grade(q)
            for q in questions
        ]
        
        # å¹¶å‘æ‰§è¡Œ
        def progress_wrapper(done, total):
            if callback:
                callback(PipelineStage.GRADING, done, total)
        
        grading_results = await self.grade_concurrency.run_batch(
            tasks, progress_callback=progress_wrapper
        )
        
        # åˆ†ç±»ç»“æœ
        correct_questions = []
        wrong_questions = []
        error_questions = []
        
        for question, result in zip(questions, grading_results):
            if isinstance(result, Exception):
                error_questions.append((question, str(result)))
            elif result.is_correct:
                correct_questions.append(question)
            else:
                wrong_questions.append((question, result))
        
        logger.info(
            f"[åˆ¤é¢˜] å®Œæˆ - "
            f"æ­£ç¡®: {len(correct_questions)}, "
            f"é”™è¯¯: {len(wrong_questions)}, "
            f"å¼‚å¸¸: {len(error_questions)}"
        )
        
        return PipelineResult(
            correct=correct_questions,
            wrong=wrong_questions,
            errors=error_questions
        )
    
    async def _save_results(self, results: PipelineResult):
        """ä¿å­˜ç»“æœåˆ°ä»“åº“"""
        
        # æ­£ç¡®é¢˜ç›® â†’ Validationé›†
        for q in results.correct:
            await self.storage.save_to_validation(q)
        
        # é”™è¯¯é¢˜ç›® â†’ Benchmarké›†
        for q, grading_result in results.wrong:
            await self.storage.save_to_benchmark(q, grading_result)
        
        # é”™è¯¯è®°å½• â†’ é”™è¯¯æ—¥å¿—
        for q, error in results.errors:
            await self.storage.save_error(q, error)
```

---

## ğŸ”Œ æ¥å£å®šä¹‰

### Generatoræ¥å£

```python
class IQuestionGenerator(Protocol):
    """é¢˜ç›®ç”Ÿæˆå™¨æ¥å£"""
    
    async def generate(
        self, 
        paper: Paper,
        few_shot_examples: Optional[List[Question]] = None
    ) -> List[Question]:
        """
        ç”Ÿæˆé¢˜ç›®
        
        Args:
            paper: è®ºæ–‡å¯¹è±¡
            few_shot_examples: Few-shotç¤ºä¾‹
        
        Returns:
            é¢˜ç›®åˆ—è¡¨
        """
        ...
```

### Answereræ¥å£

```python
class IAnsweringModule(Protocol):
    """è§£é¢˜æ¨¡å—æ¥å£"""
    
    async def answer(self, question: Question) -> Question:
        """
        è§£ç­”é—®é¢˜
        
        Args:
            question: é—®é¢˜å¯¹è±¡
        
        Returns:
            åŒ…å«ç­”æ¡ˆçš„é—®é¢˜å¯¹è±¡
        """
        ...
```

### Graderæ¥å£

```python
class IGradingModule(Protocol):
    """åˆ¤é¢˜æ¨¡å—æ¥å£"""
    
    async def grade(
        self, 
        question: Question
    ) -> GradingResult:
        """
        åˆ¤é¢˜
        
        Args:
            question: åŒ…å«å€™é€‰ç­”æ¡ˆçš„é—®é¢˜
        
        Returns:
            åˆ¤é¢˜ç»“æœ
        """
        ...
```

### Repositoryæ¥å£

```python
class IDataRepository(Protocol):
    """æ•°æ®ä»“åº“æ¥å£"""
    
    async def save_to_validation(self, question: Question):
        """ä¿å­˜åˆ°éªŒè¯é›†"""
        ...
    
    async def save_to_benchmark(
        self, 
        question: Question, 
        grading_result: GradingResult
    ):
        """ä¿å­˜åˆ°åŸºå‡†é›†"""
        ...
    
    async def get_few_shot_samples(self, count: int) -> List[Question]:
        """è·å–Few-shotæ ·æœ¬"""
        ...
```

---

## ğŸ“Š é…ç½®æ–‡ä»¶ç»“æ„

### æ–°config.yaml

```yaml
# å…¨å±€é…ç½®
project:
  name: "IgnisBenchmark"
  version: "2.0"

# æ¨¡å‹é…ç½®ï¼ˆåˆ†ç»„æ¸…æ™°ï¼‰
models:
  generation:
    configs:
      - name: "gemini/gemini-2.5-flash"
        provider: "gemini"
        priority: 1
        max_retries: 2
        timeout: 120
        temperature: 0.8
        max_tokens: 8000
      
      - name: "deepseek-ai/DeepSeek-V3"
        provider: "siliconflow"
        priority: 2
        max_retries: 2
        timeout: 180
  
  answering:
    configs:
      - name: "Qwen/Qwen2.5-7B-Instruct"
        provider: "siliconflow"
        priority: 1
        max_retries: 3
        timeout: 120
  
  grading:
    configs:
      - name: "gemini/gemini-2.5-flash"
        provider: "gemini"
        priority: 1
        max_retries: 2
        timeout: 120

# å¹¶å‘é…ç½®ï¼ˆåˆ†å±‚æ¸…æ™°ï¼‰
concurrency:
  paper_level:          # è®ºæ–‡çº§ï¼ˆæ‰¹å¤„ç†ï¼‰
    max_concurrent: 3
  
  question_level:       # é¢˜ç›®çº§ï¼ˆæµæ°´çº¿å†…ï¼‰
    generation: 1       # ç”Ÿæˆä¸å¹¶å‘ï¼ˆ1ä¸ªAPIè°ƒç”¨ â†’ Né¢˜ï¼‰
    answering: 5        # è§£é¢˜å¹¶å‘
    grading: 3          # åˆ¤é¢˜å¹¶å‘
  
  request_level:        # è¯·æ±‚çº§ï¼ˆåº•å±‚APIï¼‰
    max_concurrent_requests: 10
    rate_limit_per_second: 5.0

# é‡è¯•é…ç½®
retry:
  max_attempts: 3
  base_delay: 1.0
  max_delay: 60.0
  exponential_base: 2.0
  jitter: true

# æ•°æ®é…ç½®
data:
  benchmark_path: "./data/benchmark_bank.jsonl"
  validation_path: "./data/validation_set.jsonl"
  error_log_path: "./data/errors.jsonl"
  cache_dir: "./cache"

# æµæ°´çº¿é…ç½®
pipeline:
  enable_cache: true
  save_intermediate: true
  questions_per_batch: 20

# æ—¥å¿—é…ç½®
logging:
  level: "INFO"
  file: "./logs/system.log"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
```

---

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### Milestone 1 (é‡æ„å)

```python
# milestone1_generator_v2.py
import asyncio
from core.pipeline import QuestionPipeline
from core.models import Paper
from adapters.legacy import LegacyGeneratorAdapter

async def main():
    # åŠ è½½é…ç½®
    config = load_config("config.yaml")
    
    # åˆå§‹åŒ–ç»„ä»¶
    pipeline = build_pipeline(config)
    
    # åŠ è½½è®ºæ–‡
    paper = Paper.from_file("main.txt", id="PECS_001")
    
    # è¿è¡Œæµæ°´çº¿
    def progress_callback(stage, done, total):
        print(f"[{stage.value}] {done}/{total}")
    
    result = await pipeline.process_paper(paper, progress_callback)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_report(result)
    save_report(report, "data/milestone1_report.md")

if __name__ == "__main__":
    asyncio.run(main())
```

### Milestone 2 (æ‰¹é‡å¤„ç†)

```python
# milestone2_generator.py
import asyncio
from core.batch_processor import BatchPaperProcessor

async def main():
    config = load_config("config.yaml")
    
    # æ‰¹å¤„ç†å™¨
    processor = BatchPaperProcessor(
        pipeline=build_pipeline(config),
        concurrency=config.concurrency.paper_level.max_concurrent
    )
    
    # åŠ è½½100ç¯‡è®ºæ–‡
    papers = load_papers_from_directory("papers/")
    
    # æ‰¹é‡å¤„ç†ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
    results = await processor.process_batch(
        papers,
        progress_callback=print_progress
    )
    
    # æ±‡æ€»æŠ¥å‘Š
    summary = generate_summary(results)
    save_summary(summary, "data/milestone2_summary.md")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸ”„ ä¸ç°æœ‰ä»£ç çš„å…¼å®¹

### é€‚é…å™¨æ¨¡å¼

```python
# adapters/legacy.py
from src.question_generator import QuestionGenerator as OldGenerator
from core.interfaces import IQuestionGenerator

class LegacyGeneratorAdapter(IQuestionGenerator):
    """é€‚é…æ—§ç‰ˆç”Ÿæˆå™¨"""
    
    def __init__(self, old_generator: OldGenerator):
        self.old_generator = old_generator
    
    async def generate(
        self, 
        paper: Paper,
        few_shot_examples=None
    ) -> List[Question]:
        """é€‚é…æ¥å£"""
        
        # è½¬æ¢paperä¸ºæ—§æ ¼å¼
        # åŒæ­¥è°ƒç”¨è½¬å¼‚æ­¥
        import asyncio
        questions = await asyncio.to_thread(
            self.old_generator.generate_questions,
            few_shot_examples
        )
        
        return questions
```

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”é¢„æµ‹

### Milestone 2 (100ç¯‡è®ºæ–‡)

| åœºæ™¯ | æ—§æ¶æ„ | æ–°æ¶æ„ | æå‡ |
|------|--------|--------|------|
| **ç”Ÿæˆé¢˜ç›®** | 100min (ä¸²è¡Œ) | 33min (3å¹¶å‘) | **3x** |
| **è§£é¢˜** | 400min (ä¸²è¡Œ) | 80min (5å¹¶å‘) | **5x** |
| **åˆ¤é¢˜** | 300min (ä¸²è¡Œ) | 100min (3å¹¶å‘) | **3x** |
| **æ€»è®¡** | ~800min | ~150min | **5.3x** |

### èµ„æºåˆ©ç”¨ç‡

| æŒ‡æ ‡ | æ—§æ¶æ„ | æ–°æ¶æ„ |
|------|--------|--------|
| CPUåˆ©ç”¨ç‡ | ~20% | ~70% |
| ç½‘ç»œå¸¦å®½ | ~10% | ~60% |
| å†…å­˜å ç”¨ | ~200MB | ~500MB |

---

## ğŸš¦ ä¸‹ä¸€æ­¥

- âœ… å®Œæˆæ¶æ„è®¾è®¡
- ğŸ“ åˆ¶å®š[è¿ç§»è·¯çº¿å›¾](./03_MIGRATION_ROADMAP.md)
- ğŸ“ ç¼–å†™[APIæ–‡æ¡£](./04_API_DESIGN.md)
- ğŸ”§ å®ç°æ ¸å¿ƒç»„ä»¶

---

**è®¾è®¡è€…**: AIæ¶æ„å¸ˆ  
**å®¡é˜…**: å¾…å®¡æ ¸  
**çŠ¶æ€**: è®¾è®¡å®Œæˆ â†’ ä¸‹ä¸€æ­¥: è¿ç§»è®¡åˆ’

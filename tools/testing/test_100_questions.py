#!/usr/bin/env python3
"""
100é¢˜ç›®æ‰¹é‡æµ‹è¯•è„šæœ¬
è¿è¡Œ10è½®æ¬¡ï¼Œæ¯è½®æ¬¡ç”Ÿæˆ10é¢˜ï¼Œæ€»å…±100é¢˜
æµ‹è¯•ç³»ç»Ÿçš„é²æ£’æ€§å’Œç¨³å®šæ€§
"""

import sys
import os
import time
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.config_loader import load_config
from src.question_generator import QuestionGenerator
from src.answering_module import AnsweringModule  
from src.grading_module import GradingModule
from src.data_persistence import DataPersistence
from src.prompt_manager import PromptManager
from src.token_tracker import token_tracker

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('test_100_questions.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

def run_single_round(round_num: int, config: dict) -> dict:
    """
    è¿è¡Œå•è½®æµ‹è¯•ï¼ˆ10é¢˜ï¼‰
    
    Args:
        round_num: è½®æ¬¡ç¼–å·
        config: é…ç½®å­—å…¸
    
    Returns:
        æœ¬è½®ç»Ÿè®¡ç»“æœ
    """
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸš€ å¼€å§‹ç¬¬ {round_num} è½®æµ‹è¯• (ç”Ÿæˆ10é¢˜)")
    
    round_stats = {
        'round': round_num,
        'questions_generated': 0,
        'questions_answered': 0,  
        'questions_graded': 0,
        'correct_answers': 0,
        'errors': [],
        'start_time': time.time()
    }
    
    try:
        # 1. åˆå§‹åŒ–æ¨¡å—
        prompt_manager = PromptManager()
        data_persistence = DataPersistence(
            config['benchmark_bank_path'],
            config['validation_set_path']
        )
        
        generator = QuestionGenerator(
            model_name=config['generation_model'],
            batch_size=config['batch_size'],
            prompt_manager=prompt_manager
        )
        
        answerer = AnsweringModule(
            endpoint=config['lm_studio_endpoint'],
            model_name=config['lm_studio_model_name'], 
            concurrency=config['lm_studio_concurrency']
        )
        
        grader = GradingModule(
            model_name=config['grading_model'],
            prompt_manager=prompt_manager
        )
        
        # 2. ç”Ÿæˆé¢˜ç›®
        logger.info(f"ç¬¬ {round_num} è½®: å¼€å§‹ç”Ÿæˆé¢˜ç›®...")
        few_shot_examples = data_persistence.get_random_samples(config['few_shot_count'])
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                questions = generator.generate_questions(few_shot_examples)
                if questions:
                    round_stats['questions_generated'] = len(questions)
                    logger.info(f"ç¬¬ {round_num} è½®: æˆåŠŸç”Ÿæˆ {len(questions)} é“é¢˜ç›®")
                    break
                else:
                    logger.warning(f"ç¬¬ {round_num} è½®: ç”Ÿæˆé¢˜ç›®å¤±è´¥ï¼Œå°è¯• {attempt + 1}/{max_retries}")
                    if attempt < max_retries - 1:
                        time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
            except Exception as e:
                logger.error(f"ç¬¬ {round_num} è½®: ç”Ÿæˆé¢˜ç›®å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                round_stats['errors'].append(f"ç”Ÿæˆå¤±è´¥ (å°è¯• {attempt + 1}): {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    logger.error(f"ç¬¬ {round_num} è½®: ç”Ÿæˆé¢˜ç›®å½»åº•å¤±è´¥ï¼Œè·³è¿‡æœ¬è½®")
                    return round_stats
        
        if not questions:
            logger.error(f"ç¬¬ {round_num} è½®: æ— æ³•ç”Ÿæˆé¢˜ç›®ï¼Œè·³è¿‡æœ¬è½®")
            return round_stats
        
        # 3. è§£ç­”é¢˜ç›®
        logger.info(f"ç¬¬ {round_num} è½®: å¼€å§‹è§£ç­”é¢˜ç›®...")
        try:
            answered_questions = answerer.answer_questions(questions)
            round_stats['questions_answered'] = len([q for q in answered_questions if q.candidate_answer and not q.candidate_answer.startswith("[ERROR]")])
            logger.info(f"ç¬¬ {round_num} è½®: æˆåŠŸè§£ç­” {round_stats['questions_answered']}/{len(questions)} é“é¢˜ç›®")
        except Exception as e:
            logger.error(f"ç¬¬ {round_num} è½®: è§£ç­”é¢˜ç›®å¤±è´¥: {e}")
            round_stats['errors'].append(f"è§£ç­”å¤±è´¥: {str(e)}")
            # å³ä½¿è§£ç­”å¤±è´¥ï¼Œä¹Ÿç»§ç»­åˆ¤é¢˜ï¼ˆæ ‡è®°ä¸ºè§£é¢˜å¤±è´¥ï¼‰
            answered_questions = questions
            for q in answered_questions:
                if not hasattr(q, 'candidate_answer') or not q.candidate_answer:
                    q.candidate_answer = "[ERROR] è§£ç­”æ¨¡å—å¼‚å¸¸"
        
        # 4. åˆ¤é¢˜
        logger.info(f"ç¬¬ {round_num} è½®: å¼€å§‹åˆ¤é¢˜...")
        try:
            grading_results = grader.grade_batch(answered_questions)
            round_stats['questions_graded'] = len(grading_results)
            round_stats['correct_answers'] = sum(1 for _, result in grading_results if result.is_correct)
            logger.info(f"ç¬¬ {round_num} è½®: åˆ¤é¢˜å®Œæˆ {round_stats['correct_answers']}/{len(grading_results)} æ­£ç¡®")
        except Exception as e:
            logger.error(f"ç¬¬ {round_num} è½®: åˆ¤é¢˜å¤±è´¥: {e}")
            round_stats['errors'].append(f"åˆ¤é¢˜å¤±è´¥: {str(e)}")
            return round_stats
        
        # 5. æ•°æ®æŒä¹…åŒ–
        logger.info(f"ç¬¬ {round_num} è½®: å¼€å§‹æ•°æ®æŒä¹…åŒ–...")
        try:
            benchmark_count = 0
            validation_count = 0
            
            # éå†åˆ¤é¢˜ç»“æœè¿›è¡Œåˆ†ç±»ä¿å­˜
            for (question, answer), grading_result in zip(answered_questions, grading_results):
                if grading_result.is_correct:
                    data_persistence.save_to_validation(question)
                    validation_count += 1
                else:
                    from src.models import BenchmarkEntry
                    entry = BenchmarkEntry(
                        question_data=question,
                        failed_attempt={
                            "model_name": config['lm_studio_model_name'],
                            "candidate_answer": answer,
                            "grading_result": grading_result.__dict__
                        }
                    )
                    data_persistence.save_to_benchmark(entry)
                    benchmark_count += 1
                    
            logger.info(f"ç¬¬ {round_num} è½®: ä¿å­˜åˆ°benchmark: {benchmark_count}, ä¿å­˜åˆ°validation: {validation_count}")
        except Exception as e:
            logger.error(f"ç¬¬ {round_num} è½®: æ•°æ®æŒä¹…åŒ–å¤±è´¥: {e}")
            round_stats['errors'].append(f"æ•°æ®æŒä¹…åŒ–å¤±è´¥: {str(e)}")
        
        round_stats['duration'] = time.time() - round_stats['start_time']
        logger.info(f"âœ… ç¬¬ {round_num} è½®æµ‹è¯•å®Œæˆï¼Œè€—æ—¶ {round_stats['duration']:.1f}ç§’")
        
    except Exception as e:
        logger.error(f"âŒ ç¬¬ {round_num} è½®æµ‹è¯•å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
        round_stats['errors'].append(f"ä¸¥é‡é”™è¯¯: {str(e)}")
        round_stats['duration'] = time.time() - round_stats['start_time']
    
    return round_stats

def print_progress_summary(all_stats: list):
    """æ‰“å°è¿›åº¦æ€»ç»“"""
    logger = logging.getLogger(__name__)
    
    total_generated = sum(s['questions_generated'] for s in all_stats)
    total_answered = sum(s['questions_answered'] for s in all_stats)
    total_graded = sum(s['questions_graded'] for s in all_stats) 
    total_correct = sum(s['correct_answers'] for s in all_stats)
    total_errors = sum(len(s['errors']) for s in all_stats)
    total_time = sum(s.get('duration', 0) for s in all_stats)
    
    logger.info("=" * 60)
    logger.info("ğŸ“Š å½“å‰è¿›åº¦æ€»ç»“:")
    logger.info(f"   è½®æ¬¡: {len(all_stats)}/10")
    logger.info(f"   é¢˜ç›®ç”Ÿæˆ: {total_generated}")
    logger.info(f"   é¢˜ç›®è§£ç­”: {total_answered}")
    logger.info(f"   é¢˜ç›®åˆ¤é¢˜: {total_graded}")
    logger.info(f"   æ­£ç¡®ç­”æ¡ˆ: {total_correct}")
    logger.info(f"   é”™è¯¯æ¬¡æ•°: {total_errors}")
    logger.info(f"   æ€»ç”¨æ—¶: {total_time:.1f}ç§’")
    logger.info(f"   Tokenç»Ÿè®¡: {token_tracker.get_summary()}")
    logger.info("=" * 60)

def main():
    """ä¸»å‡½æ•°"""
    setup_logging()
    logger = logging.getLogger(__name__)
    
    logger.info("ğŸ¯ å¼€å§‹100é¢˜ç›®æ‰¹é‡æµ‹è¯• (10è½®æ¬¡ Ã— 10é¢˜/è½®)")
    start_time = time.time()
    
    # åŠ è½½é…ç½®
    try:
        config = load_config()
        logger.info("âœ… é…ç½®åŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # è·å–åˆå§‹tokenç»Ÿè®¡
    initial_stats = token_tracker.get_summary()
    logger.info("ğŸ”„ Tokenç»Ÿè®¡å·²é‡ç½®")
    
    all_stats = []
    
    # è¿è¡Œ10è½®æµ‹è¯•
    for round_num in range(1, 11):
        try:
            round_stats = run_single_round(round_num, config)
            all_stats.append(round_stats)
            
            # æ¯3è½®æ‰“å°ä¸€æ¬¡è¿›åº¦
            if round_num % 3 == 0 or round_num == 10:
                print_progress_summary(all_stats)
            
            # è½®æ¬¡é—´çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…APIå‹åŠ›è¿‡å¤§
            if round_num < 10:
                time.sleep(2)
                
        except KeyboardInterrupt:
            logger.warning(f"âš ï¸  ç”¨æˆ·ä¸­æ–­æµ‹è¯•ï¼Œå·²å®Œæˆ {len(all_stats)} è½®")
            break
        except Exception as e:
            logger.error(f"âŒ ç¬¬ {round_num} è½®å‡ºç°æ„å¤–é”™è¯¯: {e}")
            # è®°å½•é”™è¯¯ä½†ç»§ç»­ä¸‹ä¸€è½®
            all_stats.append({
                'round': round_num,
                'questions_generated': 0,
                'questions_answered': 0,
                'questions_graded': 0, 
                'correct_answers': 0,
                'errors': [f"æ„å¤–é”™è¯¯: {str(e)}"],
                'duration': 0
            })
    
    # æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - start_time
    
    logger.info("ğŸ 100é¢˜ç›®æ‰¹é‡æµ‹è¯•å®Œæˆï¼")
    logger.info("=" * 80)
    logger.info("ğŸ“ˆ æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š:")
    
    total_generated = sum(s['questions_generated'] for s in all_stats)
    total_answered = sum(s['questions_answered'] for s in all_stats)
    total_graded = sum(s['questions_graded'] for s in all_stats)
    total_correct = sum(s['correct_answers'] for s in all_stats)
    total_errors = sum(len(s['errors']) for s in all_stats)
    
    logger.info(f"   å®Œæˆè½®æ¬¡: {len(all_stats)}/10")
    logger.info(f"   é¢˜ç›®ç”Ÿæˆ: {total_generated}/100 ({total_generated/100*100:.1f}%)")
    logger.info(f"   é¢˜ç›®è§£ç­”: {total_answered} ({total_answered/max(total_generated,1)*100:.1f}%)")
    logger.info(f"   é¢˜ç›®åˆ¤é¢˜: {total_graded} ({total_graded/max(total_generated,1)*100:.1f}%)")
    logger.info(f"   æ­£ç¡®ç‡: {total_correct}/{total_graded} ({total_correct/max(total_graded,1)*100:.1f}%)" if total_graded > 0 else "   æ­£ç¡®ç‡: N/A")
    logger.info(f"   æ€»é”™è¯¯æ•°: {total_errors}")
    logger.info(f"   æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    
    # Tokenç»Ÿè®¡
    token_summary = token_tracker.get_summary()
    logger.info(f"   Tokenç»Ÿè®¡: {token_summary}")
    
    # é”™è¯¯è¯¦æƒ…
    if total_errors > 0:
        logger.info("\nâŒ é”™è¯¯è¯¦æƒ…:")
        for i, stats in enumerate(all_stats, 1):
            if stats['errors']:
                logger.info(f"   ç¬¬{i}è½®: {'; '.join(stats['errors'])}")
    
    logger.info("=" * 80)
    
    return 0 if total_generated >= 50 else 1  # è‡³å°‘ç”Ÿæˆ50é¢˜æ‰ç®—æˆåŠŸ

if __name__ == "__main__":
    exit(main())
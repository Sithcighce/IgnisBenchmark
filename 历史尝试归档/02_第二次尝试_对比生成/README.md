# 02 第二次尝试：对比生成# 第二次尝试：对比生成



## 📋 基本信息## 时间

第一次尝试后

- **时间**: 2024年10月

- **目标**: 通过对比策略增加题目深度## 目标

- **方法**: 要求题目对比不同理论、方法或观点通过对比不同生成策略改进题目质量

- **模型**: Gemini 2.5 Flash  

- **数据规模**: 1篇论文 → 20道题目## 使用脚本

- `milestone1_compare_generator.py`

## 🎯 尝试目标

## 特点

在第一次尝试的基础上，尝试通过**对比策略**提升题目质量：- 对比不同的生成prompt

- 对比不同燃烧机制- 尝试改进题目的专业性

- 对比不同数值方法- 增加题目类型分类（concept/reasoning/calculation）

- 对比不同理论模型

## 产出数据

## 📂 文件结构- 对比生成的题目数据



```## 结果

02_第二次尝试_对比生成/- 发现不同prompt对题目质量影响很大

├── data/- 确定了更有效的生成策略

│   ├── milestone1_compare.jsonl          # 20道对比题目

│   ├── milestone1_compare_raw_iter1.txt  # 原始响应## 经验教训

│   └── milestone1_compare_report.md      # 生成报告- Prompt工程很重要

├── prompts/- 需要系统化的题目类型分类

├── scripts/
└── README.md
```

## 📊 生成结果

- **生成题目**: 20道
- **题目特点**: 强调对比和差异分析
- **论文来源**: 1篇PECS综述

## 💡 主要发现

### ✅ 优点
- 题目更有深度，需要理解机制差异
- 适合考察对不同方法的理解

### ❌ 局限性
1. **适用范围受限** - 不是所有论文都适合对比
2. **依然缺乏验证** - 答案正确性未检查
3. **生成不稳定** - 有时模型理解不准

## 🔄 经验总结

- 对比策略适合**综述性论文**
- 需要更通用的生成方法
- 质量验证机制仍然缺失

---

*对比策略是一个好方向，但不能作为唯一方法，需要结合其他策略。*

#!/usr/bin/env python3
"""
ä»æ—¥å¿—æ–‡ä»¶ä¸­æ¢å¤ GPT-5 åŸºå‡†æµ‹è¯•ç»“æœ
è§£æ gpt5_benchmark.log å¹¶ç”Ÿæˆ benchmarkGPT5.json å’Œç»Ÿè®¡æ•°æ®
"""
import json
import re
from datetime import datetime
from collections import defaultdict

def parse_log_file(log_path):
    """è§£ææ—¥å¿—æ–‡ä»¶ï¼Œæå–æ‰€æœ‰æˆåŠŸå®Œæˆçš„æµ‹è¯•ç»“æœ"""
    
    results = []
    current_question = None
    question_data = {}
    
    # ç»Ÿè®¡
    total_processed = 0
    successful = 0
    failed_api = 0
    
    with open(log_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            
            # åŒ¹é…å¤„ç†è¿›åº¦: Processing [123/984] deepseek_q_xxx
            process_match = re.search(r'Processing \[(\d+)/(\d+)\] (deepseek_q_\w+)', line)
            if process_match:
                # ä¿å­˜ä¸Šä¸€ä¸ªé—®é¢˜çš„æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
                if current_question and 'grading' in question_data:
                    results.append(question_data.copy())
                
                # å¼€å§‹æ–°é—®é¢˜
                total_processed += 1
                current_question = process_match.group(3)
                question_data = {
                    'question_id': current_question,
                    'index': int(process_match.group(1)),
                    'gpt5_answer': None,
                    'grading': None
                }
                continue
            
            # åŒ¹é… GPT-5 å›ç­”: âœ“ GPT-5 answered (length: 1234 chars)
            if 'âœ“ GPT-5 answered' in line:
                length_match = re.search(r'length: (\d+) chars', line)
                if length_match and current_question:
                    question_data['gpt5_answer_length'] = int(length_match.group(1))
                continue
            
            # åŒ¹é…åˆ¤é¢˜ç»“æœ: âœ“ Graded: CORRECT/INCORRECT (Score: 95)
            graded_match = re.search(r'âœ“ Graded: (CORRECT|INCORRECT) \(Score: (\d+)\)', line)
            if graded_match and current_question:
                successful += 1
                question_data['grading'] = {
                    'correct': graded_match.group(1) == 'CORRECT',
                    'score': int(graded_match.group(2)),
                    'recovered_from_log': True
                }
                continue
            
            # åŒ¹é…APIé”™è¯¯
            if 'âœ— Error: API Error 402' in line:
                failed_api += 1
                continue
    
    # ä¿å­˜æœ€åä¸€ä¸ªé—®é¢˜
    if current_question and 'grading' in question_data:
        results.append(question_data.copy())
    
    print(f"\n{'='*70}")
    print(f"æ—¥å¿—è§£æå®Œæˆ")
    print(f"{'='*70}\n")
    print(f"ğŸ“Š è§£æç»Ÿè®¡:")
    print(f"   æ€»å¤„ç†æ•°: {total_processed}")
    print(f"   æˆåŠŸå®Œæˆ: {successful}")
    print(f"   APIå¤±è´¥: {failed_api}")
    print(f"   æœ‰æ•ˆç»“æœ: {len(results)}\n")
    
    return results

def load_best_questions(best_json_path):
    """åŠ è½½ best.json è·å–å®Œæ•´é—®é¢˜ä¿¡æ¯"""
    with open(best_json_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def merge_results(log_results, best_questions):
    """åˆå¹¶æ—¥å¿—ç»“æœå’Œå®Œæ•´é—®é¢˜ä¿¡æ¯"""
    
    # åˆ›å»ºé—®é¢˜IDåˆ°å®Œæ•´ä¿¡æ¯çš„æ˜ å°„
    question_map = {q['question_id']: q for q in best_questions}
    
    # åˆå¹¶æ•°æ®
    merged_results = []
    for log_result in log_results:
        qid = log_result['question_id']
        if qid in question_map and log_result.get('grading'):
            full_data = question_map[qid].copy()
            full_data['gpt5_answer'] = f"[æ¢å¤è‡ªæ—¥å¿— - åŸå§‹å›ç­”é•¿åº¦: {log_result.get('gpt5_answer_length', 'unknown')} chars]"
            full_data['grading'] = log_result['grading'].copy()
            full_data['grading']['note'] = 'æ­¤ç»“æœä»æ—¥å¿—ä¸­æ¢å¤ï¼ŒGPT-5åŸå§‹å›ç­”å†…å®¹æœªä¿å­˜'
            merged_results.append(full_data)
    
    return merged_results

def calculate_statistics(results):
    """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
    
    total = len(results)
    correct = sum(1 for r in results if r['grading']['correct'])
    incorrect = total - correct
    accuracy = correct / total * 100 if total > 0 else 0
    
    # è®¡ç®—å¹³å‡åˆ†
    avg_score = sum(r['grading']['score'] for r in results) / total if total > 0 else 0
    
    # æŒ‰éš¾åº¦ç»Ÿè®¡
    by_difficulty = defaultdict(lambda: {'total': 0, 'correct': 0, 'scores': []})
    for r in results:
        diff = r.get('difficulty', 'unknown')
        by_difficulty[diff]['total'] += 1
        if r['grading']['correct']:
            by_difficulty[diff]['correct'] += 1
        by_difficulty[diff]['scores'].append(r['grading']['score'])
    
    # æŒ‰ä¸»é¢˜ç»Ÿè®¡
    by_topic = defaultdict(lambda: {'total': 0, 'correct': 0, 'scores': []})
    for r in results:
        topic = r.get('topic', 'unknown')
        by_topic[topic]['total'] += 1
        if r['grading']['correct']:
            by_topic[topic]['correct'] += 1
        by_topic[topic]['scores'].append(r['grading']['score'])
    
    # æŒ‰ç±»å‹ç»Ÿè®¡
    by_type = defaultdict(lambda: {'total': 0, 'correct': 0, 'scores': []})
    for r in results:
        qtype = r.get('type', 'unknown')
        by_type[qtype]['total'] += 1
        if r['grading']['correct']:
            by_type[qtype]['correct'] += 1
        by_type[qtype]['scores'].append(r['grading']['score'])
    
    # æ„å»ºç»Ÿè®¡å¯¹è±¡
    stats = {
        'overall': {
            'total_questions': total,
            'correct': correct,
            'incorrect': incorrect,
            'accuracy': round(accuracy, 2),
            'average_score': round(avg_score, 2)
        },
        'by_difficulty': {},
        'by_topic': {},
        'by_type': {},
        'recovery_info': {
            'recovered_from': 'gpt5_benchmark.log',
            'recovery_date': datetime.now().isoformat(),
            'note': 'ç”±äºAPIä½™é¢ä¸è¶³å¯¼è‡´è„šæœ¬ä¸­æ–­ï¼Œæ­¤ç»“æœä»æ—¥å¿—ä¸­æ¢å¤ã€‚GPT-5çš„åŸå§‹å›ç­”å†…å®¹æœªä¿å­˜ã€‚'
        }
    }
    
    # å¤„ç†éš¾åº¦ç»Ÿè®¡
    for diff, data in by_difficulty.items():
        stats['by_difficulty'][str(diff)] = {
            'total': data['total'],
            'correct': data['correct'],
            'accuracy': round(data['correct'] / data['total'] * 100, 2),
            'average_score': round(sum(data['scores']) / len(data['scores']), 2)
        }
    
    # å¤„ç†ä¸»é¢˜ç»Ÿè®¡ï¼ˆåªä¿ç•™å‰10ï¼‰
    topic_items = sorted(by_topic.items(), key=lambda x: x[1]['total'], reverse=True)[:10]
    for topic, data in topic_items:
        stats['by_topic'][topic] = {
            'total': data['total'],
            'correct': data['correct'],
            'accuracy': round(data['correct'] / data['total'] * 100, 2),
            'average_score': round(sum(data['scores']) / len(data['scores']), 2)
        }
    
    # å¤„ç†ç±»å‹ç»Ÿè®¡
    for qtype, data in by_type.items():
        stats['by_type'][qtype] = {
            'total': data['total'],
            'correct': data['correct'],
            'accuracy': round(data['correct'] / data['total'] * 100, 2),
            'average_score': round(sum(data['scores']) / len(data['scores']), 2)
        }
    
    return stats

def find_needs_review(results):
    """æ‰¾å‡ºæ ‡è®°ä¸º needs_review çš„é¢˜ç›®"""
    needs_review = []
    for r in results:
        # æ£€æŸ¥éªŒè¯ç»“æœä¸­çš„çŠ¶æ€
        if 'consensus' in r and r['consensus'].get('status') == 'needs_review':
            needs_review.append({
                'question_id': r['question_id'],
                'gpt5_correct': r['grading']['correct'],
                'gpt5_score': r['grading']['score'],
                'verification_issues': r['consensus'].get('reasons', [])
            })
    return needs_review

def print_summary(stats, needs_review_count):
    """æ‰“å°æ¼‚äº®çš„ç»Ÿè®¡æ‘˜è¦"""
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š GPT-5 åŸºå‡†æµ‹è¯•ç»“æœï¼ˆä»æ—¥å¿—æ¢å¤ï¼‰")
    print(f"{'='*70}\n")
    
    print(f"âš ï¸  é‡è¦æç¤º:")
    print(f"   ç”±äº OpenRouter ä½™é¢ä¸è¶³ï¼Œè„šæœ¬åœ¨å¤„ç†è¿‡ç¨‹ä¸­ä¸­æ–­")
    print(f"   æ­¤ç»“æœä»æ—¥å¿—æ–‡ä»¶ä¸­æ¢å¤ï¼ŒGPT-5 çš„åŸå§‹å›ç­”å†…å®¹æœªä¿å­˜\n")
    
    overall = stats['overall']
    print(f"ğŸ¯ æ€»ä½“æ­£ç¡®ç‡: {overall['accuracy']}%")
    print(f"   æ€»é¢˜ç›®æ•°: {overall['total_questions']}")
    print(f"   æ­£ç¡®: {overall['correct']}")
    print(f"   é”™è¯¯: {overall['incorrect']}")
    print(f"   å¹³å‡åˆ†: {overall['average_score']}/100\n")
    
    print(f"ğŸ“ˆ æŒ‰éš¾åº¦åˆ†æ:")
    for diff, data in sorted(stats['by_difficulty'].items()):
        print(f"   éš¾åº¦ {diff}: {data['accuracy']}% ({data['correct']}/{data['total']}) - å¹³å‡åˆ†: {data['average_score']}")
    
    print(f"\nğŸ“š æŒ‰ä¸»é¢˜åˆ†æ (Top 10):")
    for topic, data in list(stats['by_topic'].items())[:10]:
        print(f"   {topic}: {data['accuracy']}% ({data['correct']}/{data['total']}) - å¹³å‡åˆ†: {data['average_score']}")
    
    print(f"\nğŸ” æŒ‰é¢˜å‹åˆ†æ:")
    for qtype, data in stats['by_type'].items():
        print(f"   {qtype}: {data['accuracy']}% ({data['correct']}/{data['total']}) - å¹³å‡åˆ†: {data['average_score']}")
    
    if needs_review_count > 0:
        print(f"\nâš ï¸  Needs Review é¢˜ç›®: {needs_review_count}")
        print(f"   è¿™äº›é¢˜ç›®åœ¨éªŒè¯é˜¶æ®µè¢«æ ‡è®°ä¸ºéœ€è¦äººå·¥å®¡æŸ¥")
    
    print(f"\n{'='*70}\n")

def main():
    log_path = r'c:\Users\13031\Desktop\workspace\distillation_generation\éªŒè¯\gpt5_benchmark.log'
    best_json_path = r'c:\Users\13031\Desktop\workspace\distillation_generation\éªŒè¯\best.json'
    output_json_path = r'c:\Users\13031\Desktop\workspace\distillation_generation\éªŒè¯\benchmarkGPT5_recovered.json'
    stats_json_path = r'c:\Users\13031\Desktop\workspace\distillation_generation\éªŒè¯\gpt5_benchmark_stats_recovered.json'
    needs_review_path = r'c:\Users\13031\Desktop\workspace\distillation_generation\éªŒè¯\gpt5_needs_review.json'
    
    print("ğŸ”„ å¼€å§‹ä»æ—¥å¿—æ¢å¤æ•°æ®...")
    
    # è§£ææ—¥å¿—
    log_results = parse_log_file(log_path)
    
    if not log_results:
        print("âŒ æœªèƒ½ä»æ—¥å¿—ä¸­æå–ä»»ä½•æœ‰æ•ˆç»“æœï¼")
        return
    
    # åŠ è½½å®Œæ•´é—®é¢˜ä¿¡æ¯
    print("ğŸ“– åŠ è½½å®Œæ•´é—®é¢˜ä¿¡æ¯...")
    best_questions = load_best_questions(best_json_path)
    
    # åˆå¹¶æ•°æ®
    print("ğŸ”— åˆå¹¶æ•°æ®...")
    merged_results = merge_results(log_results, best_questions)
    
    # è®¡ç®—ç»Ÿè®¡
    print("ğŸ“Š è®¡ç®—ç»Ÿè®¡æ•°æ®...")
    stats = calculate_statistics(merged_results)
    
    # æŸ¥æ‰¾ needs_review é¢˜ç›®
    print("ğŸ” æŸ¥æ‰¾ needs_review é¢˜ç›®...")
    needs_review = find_needs_review(merged_results)
    
    # ä¿å­˜ç»“æœ
    print("ğŸ’¾ ä¿å­˜ç»“æœæ–‡ä»¶...")
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(merged_results, f, ensure_ascii=False, indent=2)
    
    with open(stats_json_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    if needs_review:
        with open(needs_review_path, 'w', encoding='utf-8') as f:
            json.dump(needs_review, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°æ‘˜è¦
    print_summary(stats, len(needs_review))
    
    print(f"âœ… æ¢å¤å®Œæˆï¼")
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   1. {output_json_path}")
    print(f"   2. {stats_json_path}")
    if needs_review:
        print(f"   3. {needs_review_path} ({len(needs_review)} é¢˜)\n")
    
    # å…³äºæˆæœ¬çš„è¯´æ˜
    print(f"\nğŸ’° å…³äºæˆæœ¬é—®é¢˜çš„åˆ†æ:")
    print(f"{'='*70}")
    print(f"ä¸ºä»€ä¹ˆä¼šèŠ±è´¹ $100ï¼Ÿ")
    print(f"\n1. GPT-5 (OpenRouter) çš„å®šä»·:")
    print(f"   - è¾“å…¥: $2.50 per 1M tokens")
    print(f"   - è¾“å‡º: $10.00 per 1M tokens")
    print(f"\n2. æ‚¨çš„ä½¿ç”¨é‡ä¼°ç®—:")
    print(f"   - å¤„ç†äº†çº¦ {len(merged_results)} é“é¢˜ç›®")
    print(f"   - æ¯é¢˜é—®é¢˜é•¿åº¦: çº¦ 500-2000 tokens")
    print(f"   - æ¯é¢˜ GPT-5 å›ç­”: å¹³å‡ {sum(r.get('gpt5_answer_length', 0) for r in log_results) // len(log_results) if log_results else 0} å­—ç¬¦ (çº¦ 1500-3000 tokens)")
    print(f"   - æ¯é¢˜åˆ¤é¢˜è¾“å…¥: é—®é¢˜+æ ‡å‡†ç­”æ¡ˆ+GPT-5å›ç­” (çº¦ 2000-5000 tokens)")
    print(f"   - æ¯é¢˜åˆ¤é¢˜è¾“å‡º: çº¦ 200-500 tokens")
    print(f"\n3. ç²—ç•¥è®¡ç®—:")
    print(f"   - GPT-5 å›ç­”: {len(merged_results)} Ã— 2000 input + 2000 output")
    print(f"     â‰ˆ {len(merged_results) * 2000 / 1_000_000:.2f}M input + {len(merged_results) * 2000 / 1_000_000:.2f}M output")
    print(f"     â‰ˆ ${len(merged_results) * 2000 / 1_000_000 * 2.5:.2f} + ${len(merged_results) * 2000 / 1_000_000 * 10:.2f}")
    print(f"     â‰ˆ ${len(merged_results) * 2000 / 1_000_000 * 12.5:.2f}")
    print(f"\n   - DeepSeek åˆ¤é¢˜: {len(merged_results)} Ã— 4000 input + 300 output")
    print(f"     (DeepSeek ä¾¿å®œå¾ˆå¤šï¼Œçº¦ $0.14/1M input, $0.28/1M output)")
    print(f"     â‰ˆ ${len(merged_results) * 4000 / 1_000_000 * 0.14:.2f} + ${len(merged_results) * 300 / 1_000_000 * 0.28:.2f}")
    print(f"     â‰ˆ ${len(merged_results) * 4000 / 1_000_000 * 0.42:.2f}")
    print(f"\n   æ€»è®¡ä¼°ç®—: ${len(merged_results) * 2000 / 1_000_000 * 12.5 + len(merged_results) * 4000 / 1_000_000 * 0.42:.2f}")
    print(f"\n4. æ•™è®­:")
    print(f"   - GPT-5 éå¸¸æ˜‚è´µ (è¾“å‡º $10/1M tokens!)")
    print(f"   - åº”è¯¥å…ˆç”¨å°‘é‡é¢˜ç›®æµ‹è¯•æˆæœ¬")
    print(f"   - è€ƒè™‘ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹ (å¦‚ GPT-4-turbo, Claude 3.5)")
    print(f"   - æˆ–è€…åªåœ¨éš¾é¢˜ä¸Šä½¿ç”¨ GPT-5")
    print(f"{'='*70}\n")

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
DeepSeek Detail Q Generator with Intelligent Retry
ä½¿ç”¨DeepSeekå®˜æ–¹APIï¼Œæ”¯æŒæ™ºèƒ½é‡è¯•æœºåˆ¶
"""

import os
import json
import logging
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import shutil
from litellm import completion

from src.utils import setup_logging, load_env_variables

logger = logging.getLogger(__name__)

# DeepSeeké…ç½®
DEEPSEEK_MODEL = "deepseek-chat"
DEEPSEEK_API_BASE = "https://api.deepseek.com"
MAX_RETRIES_PER_QUESTION = 3  # æ¯é¢˜æœ€å¤šé‡è¯•3æ¬¡
MAX_CONCURRENT_FILES = 20  # å¹¶å‘å¤„ç†æ–‡ä»¶æ•°


class DeepSeekDetailQGenerator:
    """ä½¿ç”¨DeepSeek APIçš„æ™ºèƒ½é¢˜ç›®ç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str = "questions_deepseek"):
        self.output_dir = Path(output_dir)
        self.api_key = os.environ.get('DEEPSEEK_API_KEY')
        
        if not self.api_key:
            raise ValueError("DEEPSEEK_API_KEY not found in environment")
        
        logger.info(f"Initialized DeepSeek Generator")
        logger.info(f"Model: {DEEPSEEK_MODEL}")
        logger.info(f"API Base: {DEEPSEEK_API_BASE}")
        
        # ç»Ÿè®¡
        self.stats = {
            "total_files": 0,
            "completed": 0,
            "failed": 0,
            "total_questions": 0,
            "passed_questions": 0,
            "failed_questions": 0,
            "total_retries": 0
        }
    
    def call_deepseek(self, prompt: str, require_json: bool = False, timeout: int = 120) -> str:
        """è°ƒç”¨DeepSeek API"""
        kwargs = {
            "model": "deepseek/" + DEEPSEEK_MODEL,  # litellméœ€è¦providerå‰ç¼€
            "messages": [{"role": "user", "content": prompt}],
            "api_base": DEEPSEEK_API_BASE,
            "api_key": self.api_key,
            "timeout": timeout,
            "temperature": 0.7
        }
        
        if require_json:
            kwargs["response_format"] = {"type": "json_object"}
        
        response = completion(**kwargs)
        return response.choices[0].message.content
    
    def build_generation_prompt(self, paper_text: str) -> str:
        """æ„å»ºç”Ÿæˆé¢˜ç›®çš„prompt"""
        prompt = f"""ä½ æ˜¯ç‡ƒçƒ§ç§‘å­¦ã€ä¼ çƒ­ã€æµä½“åŠ›å­¦ã€CFDå’Œèƒ½æºé¢†åŸŸçš„èµ„æ·±ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹è®ºæ–‡ç”Ÿæˆ5é“é«˜è´¨é‡çš„æ·±åº¦é—®é¢˜ã€‚

**è®ºæ–‡å…¨æ–‡**:
{paper_text}

---

## ä»»åŠ¡è¦æ±‚

ç”Ÿæˆ **5é“è¯¦ç»†çš„é¢†åŸŸä¸“ä¸šé—®é¢˜**ï¼Œæ¯é“é¢˜å¿…é¡»ï¼š

### 1. é¢†åŸŸèšç„¦è¦æ±‚
- **ä¸¥æ ¼é™å®š**ï¼šå¿…é¡»å…³äºç‡ƒçƒ§ã€ä¼ çƒ­ã€æµä½“åŠ›å­¦ã€CFDã€èƒ½æºåº”ç”¨
- **éœ€è¦é¢†åŸŸä¸“ä¸šçŸ¥è¯†**ï¼šä¸èƒ½æ˜¯çº¯ML/CSé—®é¢˜
- **æ·±åº¦è¦æ±‚**ï¼šéœ€è¦ç†è§£ç‰©ç†æœºç†ã€åŒ–å­¦åŠ¨åŠ›å­¦ã€æµä½“åŠ¨åŠ›å­¦ç­‰

### 2. ç­”æ¡ˆé•¿åº¦è¦æ±‚
- ç­”æ¡ˆé•¿åº¦ â‰¥ **300å­—ç¬¦**
- å¿…é¡»åŒ…å«è¯¦ç»†æ¨å¯¼ã€å…¬å¼ã€æœºç†åˆ†æ

### 3. é¢˜ç›®ç±»å‹
- `reasoning`: éœ€è¦æ¨ç†å’Œæœºç†è§£é‡Š
- `calculation`: éœ€è¦å…¬å¼æ¨å¯¼æˆ–æ•°å€¼è®¡ç®—
- `concept`: éœ€è¦æ·±å…¥ç†è§£æ¦‚å¿µåŸç†

### 4. éš¾åº¦ç­‰çº§
- 3: éœ€è¦ä¸“ä¸šçŸ¥è¯†
- 4: éœ€è¦æ·±å…¥ç†è§£å’Œç»¼åˆåˆ†æ
- 5: éœ€è¦é«˜çº§ä¸“å®¶çŸ¥è¯†

### 5. å¼•ç”¨è¦æ±‚
- æ¯é“é¢˜å¿…é¡»å¼•ç”¨è®ºæ–‡åŸæ–‡ä¸­çš„ **2æ®µç²¾ç¡®æ–‡å­—**
- å¼•ç”¨å¿…é¡»æ˜¯**åŸæ–‡çš„ç²¾ç¡®ç‰‡æ®µ**
- æ¯æ®µå¼•ç”¨é•¿åº¦ï¼š50-200å­—ç¬¦

---

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰

```json
{{
  "questions": [
    {{
      "question_text": "é—®é¢˜æè¿°",
      "standard_answer": "è¯¦ç»†ç­”æ¡ˆï¼ˆâ‰¥300å­—ç¬¦ï¼‰",
      "original_text": {{
        "1": "åŸæ–‡ç²¾ç¡®å¼•ç”¨1",
        "2": "åŸæ–‡ç²¾ç¡®å¼•ç”¨2"
      }},
      "type": "reasoning|calculation|concept",
      "difficulty": 3-5,
      "topic": "combustion_kinetics|heat_transfer|fluid_mechanics|CFD_modeling|energy_systems"
    }}
  ]
}}
```

è¯·ç”Ÿæˆ5é“ç¬¦åˆè¦æ±‚çš„é—®é¢˜ã€‚
"""
        return prompt
    
    def build_quality_check_prompt(self, question: Dict[str, Any], paper_excerpt: str) -> str:
        """æ„å»ºè´¨é‡æ£€æŸ¥prompt"""
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸¥æ ¼çš„è´¨é‡å®¡æ ¸ä¸“å®¶ã€‚è¯·æ£€æŸ¥ä»¥ä¸‹é—®é¢˜å’Œç­”æ¡ˆçš„è´¨é‡ã€‚

**é—®é¢˜**: {question['question_text']}

**ç­”æ¡ˆ**: {question['standard_answer']}

**åŸæ–‡å¼•ç”¨**:
{json.dumps(question['original_text'], ensure_ascii=False, indent=2)}

**è®ºæ–‡æ‘˜å½•**ï¼ˆç”¨äºéªŒè¯ï¼‰:
{paper_excerpt[:8000]}

---

## æ£€æŸ¥æ ‡å‡†

### 1. é¢†åŸŸèšç„¦æ€§ï¼ˆdomain_focusedï¼‰
- âœ… é—®é¢˜æ˜¯å¦éœ€è¦ç‡ƒçƒ§/ä¼ çƒ­/æµä½“/CFD/èƒ½æºé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Ÿ
- âŒ æ˜¯å¦æ˜¯çº¯ML/CSæ–¹æ³•å¯¹æ¯”ï¼Ÿ

### 2. ç­”æ¡ˆæ­£ç¡®æ€§ï¼ˆanswer_correctï¼‰
- âœ… äº‹å®å‡†ç¡®
- âœ… æœºç†è§£é‡Šæ­£ç¡®
- âœ… å…¬å¼å’Œæ¨å¯¼åˆç†
- âŒ é—®é¢˜ç±»å‹ï¼š
  - `too_brief`: ç­”æ¡ˆè¿‡äºç®€çŸ­ï¼ˆ<300å­—ç¬¦ï¼‰
  - `factual_error`: äº‹å®é”™è¯¯
  - `fundamental_error`: åŸºæœ¬åŸç†é”™è¯¯
  - `unsupported`: å…³é”®å£°æ˜æœªè¢«æ”¯æŒ

### 3. å…¶ä»–åˆè§„æ€§ï¼ˆother_compliantï¼‰
- âŒ ä¸èƒ½æœ‰å…ƒä¿¡æ¯
- âŒ ä¸èƒ½æœ‰æ—¶æ•ˆæ€§å†…å®¹
- âŒ ä¸èƒ½è¿‡äºå®½æ³›

---

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰

```json
{{
  "domain_focused": true/false,
  "domain_reasoning": "ä¸ºä»€ä¹ˆéœ€è¦æˆ–ä¸éœ€è¦é¢†åŸŸçŸ¥è¯†",
  "answer_correct": true/false,
  "answer_issues": ["issue1", "issue2"],
  "other_compliant": true/false,
  "other_issues": ["issue1"],
  "overall_verdict": "pass/fail",
  "recommendation": "å¦‚æœå¤±è´¥ï¼Œç»™å‡ºå…·ä½“æ”¹è¿›å»ºè®®ï¼ˆå¦‚ä½•ä¿®æ”¹é—®é¢˜æˆ–ç­”æ¡ˆï¼‰"
}}
```
"""
        return prompt
    
    def build_retry_prompt(self, question: Dict[str, Any], quality_result: Dict[str, Any], paper_text: str) -> str:
        """æ„å»ºé‡è¯•promptï¼ˆåŒ…å«åŸé¢˜å’Œå®¡æ ¸æ„è§ï¼‰"""
        prompt = f"""ä½ æ˜¯ç‡ƒçƒ§ç§‘å­¦ã€ä¼ çƒ­ã€æµä½“åŠ›å­¦ã€CFDå’Œèƒ½æºé¢†åŸŸçš„èµ„æ·±ä¸“å®¶ã€‚

ä¹‹å‰ç”Ÿæˆçš„é¢˜ç›®æœªé€šè¿‡è´¨é‡å®¡æ ¸ï¼Œè¯·æ ¹æ®å®¡æ ¸æ„è§è¿›è¡Œä¼˜åŒ–ã€‚

**åŸé—®é¢˜**:
{question['question_text']}

**åŸç­”æ¡ˆ**:
{question['standard_answer']}

**åŸå¼•ç”¨**:
{json.dumps(question['original_text'], ensure_ascii=False, indent=2)}

**å®¡æ ¸æ„è§**:
- é¢†åŸŸèšç„¦: {"é€šè¿‡" if quality_result.get('domain_focused') else "âŒ " + quality_result.get('domain_reasoning', '')}
- ç­”æ¡ˆæ­£ç¡®æ€§: {"é€šè¿‡" if quality_result.get('answer_correct') else "âŒ " + str(quality_result.get('answer_issues', []))}
- å…¶ä»–åˆè§„æ€§: {"é€šè¿‡" if quality_result.get('other_compliant') else "âŒ " + str(quality_result.get('other_issues', []))}
- **æ”¹è¿›å»ºè®®**: {quality_result.get('recommendation', 'è¯·ä¼˜åŒ–é¢˜ç›®')}

**è®ºæ–‡å…¨æ–‡**:
{paper_text}

---

## ä»»åŠ¡è¦æ±‚

è¯·æ ¹æ®å®¡æ ¸æ„è§ä¼˜åŒ–è¿™é“é¢˜ç›®ï¼š
1. ä¿æŒé¢†åŸŸä¸“ä¸šæ€§ï¼ˆç‡ƒçƒ§/ä¼ çƒ­/æµä½“/CFD/èƒ½æºï¼‰
2. ç¡®ä¿ç­”æ¡ˆâ‰¥300å­—ç¬¦ï¼ŒåŒ…å«è¯¦ç»†æ¨å¯¼
3. ä½¿ç”¨åŸæ–‡ç²¾ç¡®å¼•ç”¨ï¼ˆ2æ®µï¼Œ50-200å­—ç¬¦ï¼‰
4. ä¿®æ­£å®¡æ ¸æ„è§ä¸­æŒ‡å‡ºçš„æ‰€æœ‰é—®é¢˜

## è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰

```json
{{
  "question_text": "ä¼˜åŒ–åçš„é—®é¢˜",
  "standard_answer": "ä¼˜åŒ–åçš„ç­”æ¡ˆï¼ˆâ‰¥300å­—ç¬¦ï¼‰",
  "original_text": {{
    "1": "åŸæ–‡å¼•ç”¨1",
    "2": "åŸæ–‡å¼•ç”¨2"
  }},
  "type": "reasoning|calculation|concept",
  "difficulty": 3-5,
  "topic": "combustion_kinetics|heat_transfer|fluid_mechanics|CFD_modeling|energy_systems"
}}
```
"""
        return prompt
    
    def retry_question(self, question: Dict[str, Any], quality_result: Dict[str, Any], 
                      paper_text: str, paper_excerpt: str, retry_count: int) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """é‡è¯•ç”Ÿæˆå•ä¸ªé—®é¢˜"""
        logger.info(f"  Retry #{retry_count}: Regenerating question...")
        
        # æ„å»ºé‡è¯•prompt
        retry_prompt = self.build_retry_prompt(question, quality_result, paper_text)
        
        # è°ƒç”¨APIé‡æ–°ç”Ÿæˆ
        response = self.call_deepseek(retry_prompt, require_json=True)
        new_question = json.loads(response)
        
        # è´¨é‡æ£€æŸ¥æ–°é—®é¢˜
        check_prompt = self.build_quality_check_prompt(new_question, paper_excerpt)
        check_response = self.call_deepseek(check_prompt, require_json=True)
        new_quality_result = json.loads(check_response)
        
        self.stats["total_retries"] += 1
        
        return new_question, new_quality_result
    
    def process_single_question(self, question: Dict[str, Any], paper_text: str, 
                                paper_excerpt: str, q_index: int) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªé—®é¢˜ï¼ˆå«æ™ºèƒ½é‡è¯•ï¼‰"""
        logger.info(f"  Processing Q{q_index}...")
        
        # åˆå§‹è´¨é‡æ£€æŸ¥
        check_prompt = self.build_quality_check_prompt(question, paper_excerpt)
        check_response = self.call_deepseek(check_prompt, require_json=True)
        quality_result = json.loads(check_response)
        
        current_question = question
        retry_count = 0
        
        # æ™ºèƒ½é‡è¯•ï¼šä¸åˆæ ¼åˆ™ç«‹å³é‡è¯•ï¼ˆæœ€å¤š3æ¬¡ï¼‰
        while quality_result.get("overall_verdict") != "pass" and retry_count < MAX_RETRIES_PER_QUESTION:
            retry_count += 1
            logger.info(f"  Q{q_index} FAILED - Retrying ({retry_count}/{MAX_RETRIES_PER_QUESTION})...")
            
            current_question, quality_result = self.retry_question(
                current_question, quality_result, paper_text, paper_excerpt, retry_count
            )
        
        # æ·»åŠ è´¨é‡æ£€æŸ¥ç»“æœ
        current_question["quality_check"] = quality_result
        current_question["retry_count"] = retry_count
        
        # åˆ¤æ–­æœ€ç»ˆçŠ¶æ€
        if quality_result.get("overall_verdict") == "pass":
            logger.info(f"  Q{q_index} PASSED (after {retry_count} retries)")
            return {"status": "pass", "question": current_question}
        else:
            logger.warning(f"  Q{q_index} FAILED (exhausted {MAX_RETRIES_PER_QUESTION} retries)")
            return {"status": "fail", "question": current_question}
    
    def process_single_file(self, txt_file: Path) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªtxtæ–‡ä»¶"""
        file_name = txt_file.stem
        logger.info(f"[{file_name}] Processing...")
        
        try:
            # è¯»å–è®ºæ–‡å…¨æ–‡
            with open(txt_file, 'r', encoding='utf-8') as f:
                paper_text = f.read()
            
            logger.info(f"[{file_name}] Paper length: {len(paper_text)} chars")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_folder = self.output_dir / file_name
            output_folder.mkdir(parents=True, exist_ok=True)
            
            # å¤åˆ¶åŸæ–‡
            shutil.copy(txt_file, output_folder / f"{file_name}.txt")
            
            # ç”Ÿæˆ5é“é¢˜
            logger.info(f"[{file_name}] Generating 5 questions...")
            gen_prompt = self.build_generation_prompt(paper_text)
            response = self.call_deepseek(gen_prompt, require_json=True, timeout=180)
            
            questions_data = json.loads(response)
            questions = questions_data.get("questions", [])
            logger.info(f"[{file_name}] Generated {len(questions)} questions")
            
            if len(questions) == 0:
                raise ValueError("No questions generated")
            
            # è®ºæ–‡æ‘˜å½•ï¼ˆç”¨äºè´¨é‡æ£€æŸ¥ï¼‰
            paper_excerpt = paper_text[:8000]
            
            # å¹¶å‘å¤„ç†5é“é¢˜ï¼ˆå«æ™ºèƒ½é‡è¯•ï¼‰
            passed_questions = []
            failed_questions = []
            
            with ThreadPoolExecutor(max_workers=5) as executor:
                futures = {
                    executor.submit(self.process_single_question, q, paper_text, paper_excerpt, i): i
                    for i, q in enumerate(questions, 1)
                }
                
                for future in as_completed(futures):
                    result = future.result()
                    
                    # æ·»åŠ å…ƒæ•°æ®
                    question = result["question"]
                    import hashlib
                    q_hash = hashlib.md5(question['question_text'].encode()).hexdigest()[:8]
                    question['question_id'] = f"deepseek_q_{q_hash}"
                    question['source'] = {
                        "type": "deepseek_generation",
                        "paper_file": file_name,
                        "paper_title": file_name
                    }
                    question['metadata'] = {
                        "generation_model": DEEPSEEK_MODEL,
                        "created_at": datetime.now().isoformat(),
                        "answer_length": len(question.get('standard_answer', ''))
                    }
                    
                    if result["status"] == "pass":
                        passed_questions.append(question)
                    else:
                        failed_questions.append(question)
            
            # ä¿å­˜ç»“æœ
            if passed_questions:
                with open(output_folder / "pass.json", 'w', encoding='utf-8') as f:
                    json.dump(passed_questions, f, ensure_ascii=False, indent=2)
            
            if failed_questions:
                with open(output_folder / "not_pass.json", 'w', encoding='utf-8') as f:
                    json.dump(failed_questions, f, ensure_ascii=False, indent=2)
            
            logger.info(f"[{file_name}] âœ… Completed: {len(passed_questions)} passed, {len(failed_questions)} failed")
            
            return {
                "file": file_name,
                "status": "success",
                "total_questions": len(questions),
                "passed": len(passed_questions),
                "failed": len(failed_questions)
            }
            
        except Exception as e:
            logger.error(f"[{file_name}] âŒ Error: {str(e)[:500]}")
            return {
                "file": file_name,
                "status": "failed",
                "error": str(e)[:500]
            }
    
    def test_with_main_txt(self) -> bool:
        """ä½¿ç”¨main.txtæµ‹è¯•ï¼Œæ£€æŸ¥é€šè¿‡ç‡"""
        logger.info("=" * 80)
        logger.info("PHASE 1: Testing with main.txt")
        logger.info("=" * 80)
        
        main_txt = Path("main.txt")
        if not main_txt.exists():
            logger.error("main.txt not found!")
            return False
        
        result = self.process_single_file(main_txt)
        
        if result["status"] != "success":
            logger.error("main.txt processing failed!")
            return False
        
        pass_rate = result["passed"] / result["total_questions"] * 100
        logger.info(f"main.txt results: {result['passed']}/{result['total_questions']} passed ({pass_rate:.1f}%)")
        
        if pass_rate >= 60:
            logger.info(f"âœ… Pass rate {pass_rate:.1f}% >= 60%, proceeding to batch processing...")
            return True
        else:
            logger.warning(f"âŒ Pass rate {pass_rate:.1f}% < 60%, stopping here.")
            return False
    
    def process_all_files_reverse(self):
        """æŒ‰å­—å…¸åºå€’åºæ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡ä»¶"""
        logger.info("=" * 80)
        logger.info("PHASE 2: Batch Processing (Reverse Order)")
        logger.info("=" * 80)
        
        # æŸ¥æ‰¾æ‰€æœ‰txtæ–‡ä»¶ï¼ˆå€’åºï¼‰
        compliant_dir = Path("compliant")
        txt_files = sorted(list(compliant_dir.glob("*.txt")), reverse=True)
        
        logger.info(f"Found {len(txt_files)} txt files (reverse order)")
        logger.info(f"First 5 files: {[f.name for f in txt_files[:5]]}")
        logger.info(f"Concurrent workers: {MAX_CONCURRENT_FILES}")
        
        self.stats["total_files"] = len(txt_files)
        
        # å¹¶å‘å¤„ç†
        results = []
        with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_FILES) as executor:
            futures = {executor.submit(self.process_single_file, f): f for f in txt_files}
            
            for future in as_completed(futures):
                result = future.result()
                results.append(result)
                
                # æ›´æ–°ç»Ÿè®¡
                if result["status"] == "success":
                    self.stats["completed"] += 1
                    self.stats["total_questions"] += result["total_questions"]
                    self.stats["passed_questions"] += result["passed"]
                    self.stats["failed_questions"] += result["failed"]
                else:
                    self.stats["failed"] += 1
                
                # æ‰“å°è¿›åº¦
                progress = len(results) / len(txt_files) * 100
                logger.info(f"Progress: {len(results)}/{len(txt_files)} ({progress:.1f}%)")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report(results)
        
        logger.info("=" * 80)
        logger.info("âœ… Batch processing completed!")
        logger.info(f"   Files: {self.stats['completed']}/{self.stats['total_files']}")
        logger.info(f"   Questions: {self.stats['passed_questions']}/{self.stats['total_questions']} passed")
        logger.info(f"   Total retries: {self.stats['total_retries']}")
        logger.info("=" * 80)
    
    def generate_summary_report(self, results: List[Dict[str, Any]]):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        report_file = self.output_dir / "BATCH_SUMMARY.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"""# DeepSeek Detail Q Generation - Summary Report

**Generation Time**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Model**: {DEEPSEEK_MODEL}  
**API Base**: {DEEPSEEK_API_BASE}  
**Processing Mode**: Intelligent Retry (Max 3 retries per question)

---

## ğŸ“Š OVERALL STATISTICS

| Metric | Count | Percentage |
|--------|-------|------------|
| **Files Completed** | {self.stats['completed']}/{self.stats['total_files']} | {self.stats['completed']/max(1,self.stats['total_files'])*100:.1f}% |
| **Total Questions** | {self.stats['total_questions']} | - |
| **Questions Passed** | {self.stats['passed_questions']} | {self.stats['passed_questions']/max(1,self.stats['total_questions'])*100:.1f}% |
| **Questions Failed** | {self.stats['failed_questions']} | {self.stats['failed_questions']/max(1,self.stats['total_questions'])*100:.1f}% |
| **Total Retries** | {self.stats['total_retries']} | - |

---

## ğŸ“‹ DETAILED RESULTS

| File | Status | Total Q | Passed | Failed |
|------|--------|---------|--------|--------|
""")
            
            for result in sorted(results, key=lambda x: x["file"]):
                status_icon = "âœ…" if result["status"] == "success" else "âŒ"
                if result["status"] == "success":
                    f.write(f"| {result['file']} | {status_icon} | {result['total_questions']} | {result['passed']} | {result['failed']} |\n")
                else:
                    error_msg = result.get('error', 'Unknown')[:50]
                    f.write(f"| {result['file']} | {status_icon} | - | - | Error: {error_msg} |\n")
        
        logger.info(f"Summary report saved to: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    setup_logging()
    load_env_variables()
    
    print("=" * 80)
    print("DeepSeek Detail Q Generator with Intelligent Retry")
    print("=" * 80)
    print(f"Model: {DEEPSEEK_MODEL}")
    print(f"API: {DEEPSEEK_API_BASE}")
    print(f"Max retries per question: {MAX_RETRIES_PER_QUESTION}")
    print("=" * 80)
    
    generator = DeepSeekDetailQGenerator("question_reverse")
    
    # é˜¶æ®µ1: ä½¿ç”¨main.txtæµ‹è¯•
    print("\n[PHASE 1] Testing with main.txt...")
    if not generator.test_with_main_txt():
        print("\nâŒ Test failed or pass rate < 60%. Stopping.")
        return
    
    # é˜¶æ®µ2: æ‰¹é‡å¤„ç†ï¼ˆå€’åºï¼‰
    print("\n[PHASE 2] Starting batch processing (reverse order)...")
    generator.process_all_files_reverse()
    
    print("\nâœ… ALL DONE!")
    print(f"Check: question_reverse/BATCH_SUMMARY.md")


if __name__ == "__main__":
    main()

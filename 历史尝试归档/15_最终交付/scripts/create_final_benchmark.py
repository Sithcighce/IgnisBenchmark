#!/usr/bin/env python3
"""
åˆ›å»ºæœ€ç»ˆäº¤ä»˜çš„benchmarkæ–‡ä»¶
åŒ…å«ï¼šç­”é”™çš„é¢˜ç›® + APIå¤±è´¥æœ‰åˆ†æ•°çš„é¢˜ç›®
"""

import json
import os
from datetime import datetime

# è¾“å…¥æ–‡ä»¶
REAL_ERRORS_JSON = r"c:\Users\13031\Desktop\workspace\distillation_generation\éªŒè¯\gptéªŒè¯ç»“æœåˆ†ç±»\2_çœŸå®é”™è¯¯\real_errors.json"
API_FAILURES_WITH_SCORE_JSON = r"c:\Users\13031\Desktop\workspace\distillation_generation\éªŒè¯\gptéªŒè¯ç»“æœåˆ†ç±»\3_APIå¤±è´¥_æœ‰åˆ†æ•°\api_failures_with_score.json"

# è¾“å‡ºç›®å½•
OUTPUT_DIR = r"c:\Users\13031\Desktop\workspace\distillation_generation\æœ€ç»ˆäº¤ä»˜"

def create_benchmark_basic():
    """
    æ–‡ä»¶1: benchmark_basic.json
    åªåŒ…å«é¢˜ç›®å’Œæ ‡ç­”ï¼ˆä¾›ä»–äººç›´æ¥ä½¿ç”¨ï¼‰
    """
    print("=" * 70)
    print("åˆ›å»º benchmark_basic.json - åŸºç¡€ç‰ˆï¼ˆé¢˜ç›®+æ ‡ç­”ï¼‰")
    print("=" * 70)
    
    # åŠ è½½æ•°æ®
    with open(REAL_ERRORS_JSON, 'r', encoding='utf-8') as f:
        real_errors = json.load(f)
    
    with open(API_FAILURES_WITH_SCORE_JSON, 'r', encoding='utf-8') as f:
        api_failures = json.load(f)
    
    all_questions = real_errors + api_failures
    
    # æ„å»ºåŸºç¡€ç‰ˆæœ¬
    benchmark_basic = []
    for q in all_questions:
        item = {
            "question_id": q['question_id'],
            "question_text": q['question_text'],
            "standard_answer": q['standard_answer'],
            "difficulty": q['difficulty'],
            "topic": q['topic'],
            "type": q['type']
        }
        benchmark_basic.append(item)
    
    # ä¿å­˜
    output_file = os.path.join(OUTPUT_DIR, "benchmark_basic.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(benchmark_basic, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²åˆ›å»º: {output_file}")
    print(f"   é¢˜ç›®æ•°: {len(benchmark_basic)}")
    print(f"   - çœŸå®é”™è¯¯: {len(real_errors)}")
    print(f"   - APIå¤±è´¥(æœ‰åˆ†): {len(api_failures)}")
    
    return benchmark_basic

def create_benchmark_with_verification():
    """
    æ–‡ä»¶2: benchmark_with_verification.json
    åŒ…å«é¢˜ç›®ã€æ ‡ç­”ã€éªŒè¯ä¿¡æ¯ï¼ˆåŸæ–‡ã€éªŒè¯è®°å½•ï¼‰
    """
    print("\n" + "=" * 70)
    print("åˆ›å»º benchmark_with_verification.json - éªŒè¯ç‰ˆ")
    print("=" * 70)
    
    # åŠ è½½æ•°æ®
    with open(REAL_ERRORS_JSON, 'r', encoding='utf-8') as f:
        real_errors = json.load(f)
    
    with open(API_FAILURES_WITH_SCORE_JSON, 'r', encoding='utf-8') as f:
        api_failures = json.load(f)
    
    all_questions = real_errors + api_failures
    
    # æ„å»ºéªŒè¯ç‰ˆæœ¬
    benchmark_with_verification = []
    for q in all_questions:
        item = {
            "question_id": q['question_id'],
            "question_text": q['question_text'],
            "standard_answer": q['standard_answer'],
            "difficulty": q['difficulty'],
            "topic": q['topic'],
            "type": q['type'],
            "original_text": q.get('original_text', {}),
            "verification": q.get('verification', {}),
            "quality_check": q.get('quality_check', {}),
            "source": q.get('source', 'unknown')
        }
        benchmark_with_verification.append(item)
    
    # ä¿å­˜
    output_file = os.path.join(OUTPUT_DIR, "benchmark_with_verification.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(benchmark_with_verification, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²åˆ›å»º: {output_file}")
    print(f"   é¢˜ç›®æ•°: {len(benchmark_with_verification)}")
    
    return benchmark_with_verification

def create_benchmark_with_gpt5_results():
    """
    æ–‡ä»¶3: benchmark_with_gpt5_results.json
    åŒ…å«é¢˜ç›®ã€æ ‡ç­”ã€GPT-5ç­”é¢˜å’ŒDeepSeekåˆ¤é¢˜ç»“æœ
    """
    print("\n" + "=" * 70)
    print("åˆ›å»º benchmark_with_gpt5_results.json - GPT-5æµ‹è¯•ç‰ˆ")
    print("=" * 70)
    
    # åŠ è½½æ•°æ®
    with open(REAL_ERRORS_JSON, 'r', encoding='utf-8') as f:
        real_errors = json.load(f)
    
    with open(API_FAILURES_WITH_SCORE_JSON, 'r', encoding='utf-8') as f:
        api_failures = json.load(f)
    
    all_questions = real_errors + api_failures
    
    # æ„å»ºGPT-5æµ‹è¯•ç‰ˆæœ¬
    benchmark_with_gpt5 = []
    for q in all_questions:
        category_info = q.get('category_info', {})
        
        item = {
            "question_id": q['question_id'],
            "question_text": q['question_text'],
            "standard_answer": q['standard_answer'],
            "difficulty": q['difficulty'],
            "topic": q['topic'],
            "type": q['type'],
            "gpt5_test_result": {
                "score": category_info.get('score', 0),
                "answer_length": category_info.get('answer_length', 'unknown'),
                "category": "real_error" if q in real_errors else "api_failure_with_score",
                "note": "GPT-5ç»™å‡ºå®è´¨å›ç­”ä½†ç­”é”™" if q in real_errors else "GPT-5å›ç­”å¤ªçŸ­æˆ–éƒ¨åˆ†å¤±è´¥"
            }
        }
        benchmark_with_gpt5.append(item)
    
    # ä¿å­˜
    output_file = os.path.join(OUTPUT_DIR, "benchmark_with_gpt5_results.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(benchmark_with_gpt5, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²åˆ›å»º: {output_file}")
    print(f"   é¢˜ç›®æ•°: {len(benchmark_with_gpt5)}")
    
    return benchmark_with_gpt5

def create_benchmark_complete():
    """
    æ–‡ä»¶4: benchmark_complete.json
    å®Œæ•´ç‰ˆï¼ŒåŒ…å«æ‰€æœ‰ä¿¡æ¯
    """
    print("\n" + "=" * 70)
    print("åˆ›å»º benchmark_complete.json - å®Œæ•´ç‰ˆï¼ˆæ‰€æœ‰ä¿¡æ¯ï¼‰")
    print("=" * 70)
    
    # åŠ è½½æ•°æ®
    with open(REAL_ERRORS_JSON, 'r', encoding='utf-8') as f:
        real_errors = json.load(f)
    
    with open(API_FAILURES_WITH_SCORE_JSON, 'r', encoding='utf-8') as f:
        api_failures = json.load(f)
    
    all_questions = real_errors + api_failures
    
    # æ ‡è®°åˆ†ç±»
    for q in real_errors:
        q['benchmark_category'] = 'real_error'
        q['benchmark_note'] = 'GPT-5ç»™å‡ºå®è´¨æ€§å›ç­”ï¼ˆâ‰¥500å­—ç¬¦ï¼‰ä½†DeepSeekåˆ¤å®šä¸ºé”™è¯¯'
    
    for q in api_failures:
        q['benchmark_category'] = 'api_failure_with_score'
        q['benchmark_note'] = 'GPT-5å›ç­”å¤ªçŸ­ï¼ˆ<500å­—ç¬¦ï¼‰æˆ–éƒ¨åˆ†å¤±è´¥ï¼Œä½†æœ‰1-80åˆ†'
    
    # ä¿å­˜å®Œæ•´ç‰ˆ
    output_file = os.path.join(OUTPUT_DIR, "benchmark_complete.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_questions, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²åˆ›å»º: {output_file}")
    print(f"   é¢˜ç›®æ•°: {len(all_questions)}")
    
    return all_questions

def create_readme():
    """åˆ›å»ºREADMEè¯´æ˜æ–‡æ¡£"""
    print("\n" + "=" * 70)
    print("åˆ›å»º README.md")
    print("=" * 70)
    
    # åŠ è½½æ•°æ®ç»Ÿè®¡
    with open(REAL_ERRORS_JSON, 'r', encoding='utf-8') as f:
        real_errors = json.load(f)
    
    with open(API_FAILURES_WITH_SCORE_JSON, 'r', encoding='utf-8') as f:
        api_failures = json.load(f)
    
    total = len(real_errors) + len(api_failures)
    
    readme_content = f"""# IgnisBenchmark - æœ€ç»ˆäº¤ä»˜ç‰ˆæœ¬

## ğŸ“Š Benchmarkæ¦‚è§ˆ

æœ¬BenchmarkåŒ…å« **{total}é“** é«˜è´¨é‡ç‡ƒçƒ§ç§‘å­¦ä¸èƒ½æºå·¥ç¨‹é¢†åŸŸçš„ä¸“ä¸šé¢˜ç›®ï¼Œè¿™äº›é¢˜ç›®å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. âœ… **ä¸‰æ¨¡å‹éªŒè¯é€šè¿‡**: æ‰€æœ‰é¢˜ç›®å‡ç»è¿‡Claude Sonnet 4.5ã€GPT-5ã€Gemini 2.5 Proä¸‰ä¸ªé¡¶çº§æ¨¡å‹çš„ä¸€è‡´éªŒè¯
2. ğŸ”¥ **æŒ‘æˆ˜æ€§å¼º**: è¿™äº›æ˜¯GPT-5åœ¨æµ‹è¯•ä¸­ç­”é”™æˆ–å›ç­”ä¸å®Œæ•´çš„é¢˜ç›®
3. ğŸ“š **æ¥æºæƒå¨**: åŸºäºé¡¶çº§å­¦æœ¯è®ºæ–‡å’Œæ•™æ
4. ğŸ¯ **ä¸“ä¸šæ·±åº¦**: éš¾åº¦3-5çº§ï¼Œæ¶µç›–æ¦‚å¿µç†è§£ã€æ¨ç†åˆ†æã€è®¡ç®—åº”ç”¨

---

## ğŸ“ æ–‡ä»¶è¯´æ˜

### 1. `benchmark_basic.json` - åŸºç¡€ç‰ˆï¼ˆæ¨èï¼‰

**ç”¨é€”**: ä¾›ä»–äººç›´æ¥ä½¿ç”¨çš„benchmarké¢˜ç›®é›†

**åŒ…å«å†…å®¹**:
- é¢˜ç›®ID
- é¢˜ç›®æ–‡æœ¬
- æ ‡å‡†ç­”æ¡ˆ
- éš¾åº¦ç­‰çº§ (3-5)
- ä¸»é¢˜åˆ†ç±»
- é¢˜ç›®ç±»å‹ (concept/reasoning/calculation/application)

**é€‚ç”¨åœºæ™¯**:
- è¯„æµ‹æ¨¡å‹åœ¨ç‡ƒçƒ§ç§‘å­¦é¢†åŸŸçš„ä¸“ä¸šèƒ½åŠ›
- å¯¹æ¯”ä¸åŒæ¨¡å‹çš„ç­”é¢˜è¡¨ç°
- ä½œä¸ºä¸“ä¸šé¢†åŸŸçš„æµ‹è¯•é›†

**ç¤ºä¾‹ç»“æ„**:
```json
{{
  "question_id": "deepseek_q_xxxxx",
  "question_text": "é¢˜ç›®å†…å®¹...",
  "standard_answer": "æ ‡å‡†ç­”æ¡ˆ...",
  "difficulty": 4,
  "topic": "combustion_kinetics",
  "type": "reasoning"
}}
```

---

### 2. `benchmark_with_verification.json` - éªŒè¯ç‰ˆ

**ç”¨é€”**: äº†è§£é¢˜ç›®æ¥æºå’ŒéªŒè¯è¿‡ç¨‹

**åŒ…å«å†…å®¹**:
- åŸºç¡€ç‰ˆçš„æ‰€æœ‰å†…å®¹
- **åŸæ–‡å¼•ç”¨** (original_text): é¢˜ç›®æ¥æºçš„æ–‡çŒ®åŸæ–‡
- **éªŒè¯è®°å½•** (verification): ä¸‰æ¨¡å‹éªŒè¯çš„è¯¦ç»†ä¿¡æ¯
- **è´¨é‡æ£€æŸ¥** (quality_check): é¢˜ç›®è´¨é‡å®¡æ ¸è®°å½•
- **æ¥æºä¿¡æ¯** (source): é¢˜ç›®ç”Ÿæˆçš„æ–‡çŒ®æ¥æº

**é€‚ç”¨åœºæ™¯**:
- éªŒè¯é¢˜ç›®çš„æƒå¨æ€§å’Œå‡†ç¡®æ€§
- è¿½æº¯é¢˜ç›®çš„å­¦æœ¯æ¥æº
- äº†è§£é¢˜ç›®çš„ç”Ÿæˆå’ŒéªŒè¯è¿‡ç¨‹

**ç¤ºä¾‹ç»“æ„**:
```json
{{
  "question_id": "deepseek_q_xxxxx",
  "question_text": "é¢˜ç›®å†…å®¹...",
  "standard_answer": "æ ‡å‡†ç­”æ¡ˆ...",
  "difficulty": 4,
  "topic": "combustion_kinetics",
  "type": "reasoning",
  "original_text": {{
    "1": "åŸæ–‡ç‰‡æ®µ1...",
    "2": "åŸæ–‡ç‰‡æ®µ2..."
  }},
  "verification": {{
    "claude_verification": {{}},
    "gpt5_verification": {{}},
    "gemini_verification": {{}}
  }},
  "quality_check": {{...}},
  "source": "è®ºæ–‡æ ‡é¢˜æˆ–æ¥æº"
}}
```

---

### 3. `benchmark_with_gpt5_results.json` - GPT-5æµ‹è¯•ç‰ˆ

**ç”¨é€”**: äº†è§£GPT-5åœ¨è¿™äº›é¢˜ç›®ä¸Šçš„è¡¨ç°

**åŒ…å«å†…å®¹**:
- åŸºç¡€ç‰ˆçš„æ‰€æœ‰å†…å®¹
- **GPT-5æµ‹è¯•ç»“æœ** (gpt5_test_result):
  - å¾—åˆ† (score)
  - å›ç­”é•¿åº¦ (answer_length)
  - é”™è¯¯ç±»å‹ (category): real_error æˆ– api_failure_with_score
  - è¯´æ˜ (note)

**é€‚ç”¨åœºæ™¯**:
- äº†è§£GPT-5çš„ç­”é¢˜è¡¨ç°
- è¯†åˆ«GPT-5çš„çŸ¥è¯†ç›²åŒº
- ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”æ€§èƒ½

**ç»Ÿè®¡ä¿¡æ¯**:
- çœŸå®é”™è¯¯: {len(real_errors)}é¢˜ (GPT-5ç»™å‡ºå®è´¨æ€§å›ç­”ä½†ç­”é”™)
- APIå¤±è´¥(æœ‰åˆ†): {len(api_failures)}é¢˜ (å›ç­”å¤ªçŸ­æˆ–éƒ¨åˆ†å¤±è´¥)

**ç¤ºä¾‹ç»“æ„**:
```json
{{
  "question_id": "deepseek_q_xxxxx",
  "question_text": "é¢˜ç›®å†…å®¹...",
  "standard_answer": "æ ‡å‡†ç­”æ¡ˆ...",
  "difficulty": 4,
  "topic": "combustion_kinetics",
  "type": "reasoning",
  "gpt5_test_result": {{
    "score": 35,
    "answer_length": 5216,
    "category": "real_error",
    "note": "GPT-5ç»™å‡ºå®è´¨å›ç­”ä½†ç­”é”™"
  }}
}}
```

---

### 4. `benchmark_complete.json` - å®Œæ•´ç‰ˆ

**ç”¨é€”**: å®Œæ•´çš„é¢˜ç›®æ•°æ®ï¼ŒåŒ…å«æ‰€æœ‰ä¿¡æ¯

**åŒ…å«å†…å®¹**:
- æ‰€æœ‰åŸºç¡€ã€éªŒè¯ã€æµ‹è¯•ä¿¡æ¯
- å®Œæ•´çš„é¢˜ç›®å…ƒæ•°æ®
- åˆ†ç±»æ ‡è®° (benchmark_category)
- è¯¦ç»†è¯´æ˜ (benchmark_note)

**é€‚ç”¨åœºæ™¯**:
- æ·±åº¦åˆ†æå’Œç ”ç©¶
- å®Œæ•´çš„æ•°æ®å¤‡ä»½
- éœ€è¦æ‰€æœ‰ä¿¡æ¯çš„åœºæ™¯

---

## ğŸ“Š é¢˜ç›®ç»Ÿè®¡

### æ€»ä½“ç»Ÿè®¡
- **æ€»é¢˜ç›®æ•°**: {total}
- **çœŸå®é”™è¯¯**: {len(real_errors)} ({len(real_errors)/total*100:.1f}%)
- **APIå¤±è´¥(æœ‰åˆ†)**: {len(api_failures)} ({len(api_failures)/total*100:.1f}%)

### æŒ‰éš¾åº¦åˆ†å¸ƒ
- éš¾åº¦3: {len([q for q in real_errors + api_failures if q['difficulty'] == 3])} é¢˜
- éš¾åº¦4: {len([q for q in real_errors + api_failures if q['difficulty'] == 4])} é¢˜
- éš¾åº¦5: {len([q for q in real_errors + api_failures if q['difficulty'] == 5])} é¢˜

### æŒ‰ä¸»é¢˜åˆ†å¸ƒï¼ˆTop 5ï¼‰
"""
    
    # ç»Ÿè®¡ä¸»é¢˜
    from collections import Counter
    all_questions = real_errors + api_failures
    topic_counter = Counter(q['topic'] for q in all_questions)
    
    for topic, count in topic_counter.most_common(5):
        readme_content += f"- {topic}: {count} é¢˜\n"
    
    readme_content += f"""
### æŒ‰ç±»å‹åˆ†å¸ƒ
"""
    
    type_counter = Counter(q['type'] for q in all_questions)
    for qtype, count in type_counter.most_common():
        readme_content += f"- {qtype}: {count} é¢˜\n"
    
    readme_content += f"""
---

## ğŸ¯ ä½¿ç”¨å»ºè®®

### å¿«é€Ÿå¼€å§‹
```python
import json

# åŠ è½½åŸºç¡€ç‰ˆbenchmark
with open('benchmark_basic.json', 'r', encoding='utf-8') as f:
    benchmark = json.load(f)

print(f"åŠ è½½äº† {{len(benchmark)}} é“é¢˜ç›®")

# æŒ‰éš¾åº¦ç­›é€‰
difficulty_4 = [q for q in benchmark if q['difficulty'] == 4]
print(f"éš¾åº¦4çš„é¢˜ç›®: {{len(difficulty_4)}} é“")
```

### è¯„æµ‹æ¨¡å‹
```python
import json

# åŠ è½½benchmark
with open('benchmark_basic.json', 'r', encoding='utf-8') as f:
    benchmark = json.load(f)

# å¯¹æ¯é“é¢˜ç›®è¿›è¡Œæµ‹è¯•
results = []
for question in benchmark:
    # è°ƒç”¨ä½ çš„æ¨¡å‹
    model_answer = your_model(question['question_text'])
    
    # è¯„åˆ†ï¼ˆä½¿ç”¨å¦ä¸€ä¸ªæ¨¡å‹æˆ–äººå·¥è¯„åˆ†ï¼‰
    score = grade_answer(model_answer, question['standard_answer'])
    
    results.append({{
        'question_id': question['question_id'],
        'score': score
    }})

# è®¡ç®—å‡†ç¡®ç‡
accuracy = sum(r['score'] >= 85 for r in results) / len(results)
print(f"å‡†ç¡®ç‡: {{accuracy*100:.2f}}%")
```

---

## ğŸ“ˆ GPT-5åŸºå‡†æ€§èƒ½

ä½œä¸ºå‚è€ƒï¼ŒGPT-5åœ¨è¿™{total}é“é¢˜ç›®ä¸Šçš„è¡¨ç°ï¼š

- **çœŸå®é”™è¯¯**: {len(real_errors)}é¢˜
  - ç»™å‡ºäº†å®è´¨æ€§å›ç­”ï¼ˆå¹³å‡5,893å­—ç¬¦ï¼‰
  - ä½†DeepSeekåˆ¤å®šä¸ºé”™è¯¯
  - å¹³å‡å¾—åˆ†: {sum(q.get('category_info', {}).get('score', 0) for q in real_errors) / len(real_errors):.1f}åˆ†

- **APIå¤±è´¥(æœ‰åˆ†)**: {len(api_failures)}é¢˜
  - å›ç­”å¤ªçŸ­æˆ–éƒ¨åˆ†å¤±è´¥
  - å¾—åˆ†èŒƒå›´: 1-80åˆ†
  - å¹³å‡å¾—åˆ†: {sum(q.get('category_info', {}).get('score', 0) for q in api_failures) / len(api_failures):.1f}åˆ†

**æ³¨æ„**: è¿™äº›é¢˜ç›®æ˜¯GPT-5çš„æŒ‘æˆ˜æ€§é¢˜ç›®ï¼Œä¸ä»£è¡¨GPT-5çš„æ•´ä½“æ€§èƒ½ã€‚åœ¨å®Œæ•´çš„984é¢˜æµ‹è¯•é›†ä¸­ï¼ŒGPT-5çš„å‡†ç¡®ç‡ä¸º90.45%ï¼ˆæ’é™¤APIå¤±è´¥ï¼‰ã€‚

---

## ğŸ” è´¨é‡ä¿è¯

### é¢˜ç›®æ¥æº
- æ‰€æœ‰é¢˜ç›®åŸºäºé¡¶çº§å­¦æœ¯è®ºæ–‡å’Œæ•™æ
- æ¯é“é¢˜ç›®éƒ½æœ‰æ˜ç¡®çš„æ–‡çŒ®å¼•ç”¨

### éªŒè¯æµç¨‹
1. **è‡ªåŠ¨ç”Ÿæˆ**: åŸºäºå­¦æœ¯è®ºæ–‡ç”Ÿæˆå€™é€‰é¢˜ç›®
2. **ä¸‰æ¨¡å‹éªŒè¯**: Claude Sonnet 4.5ã€GPT-5ã€Gemini 2.5 Proç‹¬ç«‹éªŒè¯
3. **è´¨é‡å®¡æ ¸**: æ£€æŸ¥åŸæ–‡å¿ å®åº¦ã€æ ‡ç­”å‡†ç¡®æ€§ã€é¢˜ç›®åˆç†æ€§
4. **å®æµ‹ç­›é€‰**: GPT-5å®é™…ç­”é¢˜ï¼Œç­›é€‰å‡ºæŒ‘æˆ˜æ€§é¢˜ç›®

### é€‚ç”¨æ€§è¯´æ˜
- âœ… é€‚åˆè¯„æµ‹é¡¶çº§æ¨¡å‹çš„ä¸“ä¸šèƒ½åŠ›
- âœ… é€‚åˆè¯†åˆ«æ¨¡å‹çš„çŸ¥è¯†ç›²åŒº
- âœ… é€‚åˆä½œä¸ºæŒ‘æˆ˜æ€§æµ‹è¯•é›†
- âš ï¸ éš¾åº¦è¾ƒé«˜ï¼Œä¸é€‚åˆåˆçº§æ¨¡å‹è¯„æµ‹
- âš ï¸ ä¸“ä¸šé¢†åŸŸé™å®šï¼ˆç‡ƒçƒ§ç§‘å­¦ä¸èƒ½æºå·¥ç¨‹ï¼‰

---

## ğŸ“ å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨æœ¬benchmarkï¼Œè¯·å¼•ç”¨ï¼š

```
IgnisBenchmark: A Challenging Benchmark for Combustion Science and Energy Engineering
Generated and verified by Claude Sonnet 4.5, GPT-5, and Gemini 2.5 Pro
Version 1.0 (2025-10-17)
GitHub: https://github.com/Sithcighce/IgnisBenchmark
```

---

## ğŸ“ è”ç³»æ–¹å¼

- **é¡¹ç›®åœ°å€**: https://github.com/Sithcighce/IgnisBenchmark
- **é—®é¢˜åé¦ˆ**: è¯·åœ¨GitHub Issuesä¸­æå‡º

---

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**ç‰ˆæœ¬**: v1.0  
**çŠ¶æ€**: âœ… æœ€ç»ˆäº¤ä»˜ç‰ˆæœ¬
"""
    
    # ä¿å­˜README
    readme_file = os.path.join(OUTPUT_DIR, "README.md")
    with open(readme_file, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print(f"âœ… å·²åˆ›å»º: {readme_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ğŸš€ åˆ›å»ºæœ€ç»ˆäº¤ä»˜çš„Benchmarkæ–‡ä»¶")
    print("=" * 70)
    
    # åˆ›å»ºå„ä¸ªç‰ˆæœ¬
    basic = create_benchmark_basic()
    verification = create_benchmark_with_verification()
    gpt5_results = create_benchmark_with_gpt5_results()
    complete = create_benchmark_complete()
    create_readme()
    
    print("\n" + "=" * 70)
    print("âœ¨ æ‰€æœ‰æ–‡ä»¶åˆ›å»ºå®Œæˆï¼")
    print("=" * 70)
    print(f"\nè¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print("\nåˆ›å»ºçš„æ–‡ä»¶:")
    print("  1. benchmark_basic.json            - åŸºç¡€ç‰ˆï¼ˆé¢˜ç›®+æ ‡ç­”ï¼‰")
    print("  2. benchmark_with_verification.json - éªŒè¯ç‰ˆï¼ˆå«éªŒè¯ä¿¡æ¯ï¼‰")
    print("  3. benchmark_with_gpt5_results.json - GPT-5æµ‹è¯•ç‰ˆ")
    print("  4. benchmark_complete.json         - å®Œæ•´ç‰ˆï¼ˆæ‰€æœ‰ä¿¡æ¯ï¼‰")
    print("  5. README.md                        - ä½¿ç”¨è¯´æ˜")
    
    print(f"\næ€»é¢˜ç›®æ•°: {len(basic)}")
    print("\nâœ… å¯ä»¥å¼€å§‹Gitå¤‡ä»½ï¼")

if __name__ == '__main__':
    main()

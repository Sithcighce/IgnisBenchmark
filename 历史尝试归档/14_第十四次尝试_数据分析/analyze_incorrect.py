#!/usr/bin/env python3
"""
åˆ†æ GPT-5 åœ¨ best.json ä¸­ç­”é”™çš„é¢˜ç›®
"""
import json
from collections import defaultdict

def main():
    # åŠ è½½ GPT-5 æµ‹è¯•ç»“æœ
    with open(r'c:\Users\13031\Desktop\workspace\distillation_generation\éªŒè¯\benchmarkGPT5_recovered.json', 'r', encoding='utf-8') as f:
        gpt5_results = json.load(f)
    
    print(f"\n{'='*70}")
    print(f"GPT-5 ç­”é”™é¢˜ç›®è¯¦ç»†åˆ†æ")
    print(f"{'='*70}\n")
    
    # ç»Ÿè®¡æ€»ä½“æƒ…å†µ
    total = len(gpt5_results)
    correct = sum(1 for r in gpt5_results if r['grading']['correct'])
    incorrect = total - correct
    
    print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"   æ€»é¢˜ç›®æ•°: {total}")
    print(f"   ç­”å¯¹: {correct} ({correct/total*100:.2f}%)")
    print(f"   ç­”é”™: {incorrect} ({incorrect/total*100:.2f}%)\n")
    
    # æå–ç­”é”™çš„é¢˜ç›®
    incorrect_questions = [r for r in gpt5_results if not r['grading']['correct']]
    
    # æŒ‰åˆ†æ•°æ’åºï¼ˆä»ä½åˆ°é«˜ï¼‰
    incorrect_questions.sort(key=lambda x: x['grading']['score'])
    
    print(f"âŒ GPT-5 ç­”é”™çš„ {len(incorrect_questions)} é“é¢˜ç›®:\n")
    print(f"{'='*70}\n")
    
    # æŒ‰éš¾åº¦ç»Ÿè®¡
    by_difficulty = defaultdict(list)
    for q in incorrect_questions:
        by_difficulty[q.get('difficulty', 'unknown')].append(q)
    
    print(f"ğŸ“ˆ ç­”é”™é¢˜ç›®æŒ‰éš¾åº¦åˆ†å¸ƒ:")
    for diff in sorted(by_difficulty.keys()):
        print(f"   éš¾åº¦ {diff}: {len(by_difficulty[diff])} é¢˜")
    print()
    
    # æŒ‰ä¸»é¢˜ç»Ÿè®¡
    by_topic = defaultdict(list)
    for q in incorrect_questions:
        by_topic[q.get('topic', 'unknown')].append(q)
    
    print(f"ğŸ“š ç­”é”™é¢˜ç›®æŒ‰ä¸»é¢˜åˆ†å¸ƒ (Top 10):")
    topic_sorted = sorted(by_topic.items(), key=lambda x: len(x[1]), reverse=True)[:10]
    for topic, questions in topic_sorted:
        print(f"   {topic}: {len(questions)} é¢˜")
    print()
    
    # æŒ‰ç±»å‹ç»Ÿè®¡
    by_type = defaultdict(list)
    for q in incorrect_questions:
        by_type[q.get('type', 'unknown')].append(q)
    
    print(f"ğŸ” ç­”é”™é¢˜ç›®æŒ‰ç±»å‹åˆ†å¸ƒ:")
    for qtype in sorted(by_type.keys()):
        print(f"   {qtype}: {len(by_type[qtype])} é¢˜")
    print()
    
    # æŒ‰åˆ†æ•°æ®µç»Ÿè®¡
    score_ranges = {
        '0-20': [],
        '21-40': [],
        '41-60': [],
        '61-80': []
    }
    for q in incorrect_questions:
        score = q['grading']['score']
        if score <= 20:
            score_ranges['0-20'].append(q)
        elif score <= 40:
            score_ranges['21-40'].append(q)
        elif score <= 60:
            score_ranges['41-60'].append(q)
        else:
            score_ranges['61-80'].append(q)
    
    print(f"ğŸ“Š ç­”é”™é¢˜ç›®æŒ‰åˆ†æ•°æ®µåˆ†å¸ƒ:")
    for range_name, questions in sorted(score_ranges.items()):
        if questions:
            print(f"   {range_name}åˆ†: {len(questions)} é¢˜")
    print()
    
    print(f"{'='*70}\n")
    print(f"è¯¦ç»†åˆ—è¡¨ï¼ˆæŒ‰åˆ†æ•°ä»ä½åˆ°é«˜ï¼‰:\n")
    
    for i, q in enumerate(incorrect_questions, 1):
        print(f"{i}. [{q['question_id']}] å¾—åˆ†: {q['grading']['score']}/100")
        print(f"   éš¾åº¦: {q.get('difficulty')}, ä¸»é¢˜: {q.get('topic')}, ç±»å‹: {q.get('type')}")
        print(f"   é—®é¢˜: {q['question_text'][:150]}{'...' if len(q['question_text']) > 150 else ''}")
        print(f"   æ ‡å‡†ç­”æ¡ˆ: {q['standard_answer'][:150]}{'...' if len(q['standard_answer']) > 150 else ''}")
        print(f"   GPT-5å›ç­”é•¿åº¦: {q.get('gpt5_answer_length', 'unknown')} chars")
        
        # æ˜¾ç¤ºåŸæ–‡å¼•ç”¨
        if 'original_text' in q and q['original_text']:
            print(f"   åŸæ–‡å¼•ç”¨: {len(q['original_text'])} æ¡")
        
        print()
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    output = {
        'summary': {
            'total_tested': total,
            'total_incorrect': incorrect,
            'incorrect_rate': round(incorrect/total*100, 2)
        },
        'by_difficulty': {str(k): len(v) for k, v in by_difficulty.items()},
        'by_topic': {k: len(v) for k, v in sorted(by_topic.items(), key=lambda x: len(x[1]), reverse=True)[:20]},
        'by_type': {k: len(v) for k, v in by_type.items()},
        'by_score_range': {k: len(v) for k, v in score_ranges.items() if v},
        'incorrect_questions': [
            {
                'question_id': q['question_id'],
                'score': q['grading']['score'],
                'difficulty': q.get('difficulty'),
                'topic': q.get('topic'),
                'type': q.get('type'),
                'question_text': q['question_text'],
                'standard_answer': q['standard_answer'],
                'gpt5_answer_length': q.get('gpt5_answer_length', 'unknown'),
                'original_text': q.get('original_text', {})
            }
            for q in incorrect_questions
        ]
    }
    
    output_path = r'c:\Users\13031\Desktop\workspace\distillation_generation\éªŒè¯\gpt5_incorrect_analysis.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"{'='*70}")
    print(f"ğŸ’¾ è¯¦ç»†åˆ†æå·²ä¿å­˜è‡³: {output_path}")
    print(f"{'='*70}\n")
    
    # æ‰¾å‡ºæœ€å·®çš„10é“é¢˜
    worst_10 = incorrect_questions[:10]
    print(f"\nğŸ”¥ å¾—åˆ†æœ€ä½çš„ 10 é“é¢˜:\n")
    for i, q in enumerate(worst_10, 1):
        print(f"{i}. [{q['question_id']}] å¾—åˆ†: {q['grading']['score']}/100")
        print(f"   éš¾åº¦ {q.get('difficulty')} | {q.get('topic')} | {q.get('type')}")
        print(f"   {q['question_text'][:100]}...\n")

if __name__ == "__main__":
    main()

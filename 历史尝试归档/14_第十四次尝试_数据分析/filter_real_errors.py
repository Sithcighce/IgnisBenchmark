#!/usr/bin/env python3
"""
ç­›é€‰å‡ºGPT-5æœ‰åˆç†é•¿åº¦è¾“å‡ºä½†å¾—åˆ†å¾ˆä½çš„é¢˜ç›®
æ’é™¤å› ä¸ºAPIå¤±è´¥/ä½™é¢ä¸è¶³å¯¼è‡´çš„ç©ºå›ç­”
"""
import json
from collections import defaultdict

def main():
    # åŠ è½½æ•°æ®
    with open(r'c:\Users\13031\Desktop\workspace\distillation_generation\éªŒè¯\gpt5_incorrect_detailed.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    incorrect = data['incorrect_questions_with_details']
    
    print(f"\n{'='*70}")
    print(f"ç­›é€‰çœŸæ­£çš„GPT-5é”™è¯¯ï¼ˆæ’é™¤APIå¤±è´¥ï¼‰")
    print(f"{'='*70}\n")
    
    # åˆ†ç±»
    with_answer = []  # æœ‰åˆç†é•¿åº¦å›ç­”çš„
    without_answer = []  # æ— å›ç­”è®°å½•æˆ–å›ç­”å¤ªçŸ­çš„
    
    MIN_LENGTH = 500  # å®šä¹‰åˆç†é•¿åº¦é˜ˆå€¼ï¼ˆ500å­—ç¬¦çº¦ç­‰äºæœ‰å®è´¨å†…å®¹ï¼‰
    
    for q in incorrect:
        answer_length = q['gpt5_result']['answer_length']
        
        if answer_length == 'unknown':
            without_answer.append(q)
        elif isinstance(answer_length, int):
            if answer_length >= MIN_LENGTH:
                with_answer.append(q)
            else:
                without_answer.append(q)
        else:
            without_answer.append(q)
    
    print(f"ğŸ“Š åˆ†ç±»ç»“æœ:")
    print(f"   æœ‰åˆç†é•¿åº¦å›ç­” (â‰¥{MIN_LENGTH}å­—ç¬¦): {len(with_answer)} é¢˜")
    print(f"   æ— å›ç­”/å›ç­”å¤ªçŸ­ (<{MIN_LENGTH}å­—ç¬¦): {len(without_answer)} é¢˜\n")
    
    print(f"{'='*70}")
    print(f"æœ‰åˆç†é•¿åº¦å›ç­”ä½†ç­”é”™çš„é¢˜ç›® ({len(with_answer)} é¢˜)")
    print(f"{'='*70}\n")
    
    # æŒ‰åˆ†æ•°æ’åº
    with_answer_sorted = sorted(with_answer, key=lambda x: x['gpt5_result']['score'])
    
    # æŒ‰åˆ†æ•°æ®µç»Ÿè®¡
    score_ranges = {
        '0-20': [],
        '21-40': [],
        '41-60': [],
        '61-80': []
    }
    
    for q in with_answer_sorted:
        score = q['gpt5_result']['score']
        if score <= 20:
            score_ranges['0-20'].append(q)
        elif score <= 40:
            score_ranges['21-40'].append(q)
        elif score <= 60:
            score_ranges['41-60'].append(q)
        else:
            score_ranges['61-80'].append(q)
    
    print(f"æŒ‰åˆ†æ•°æ®µåˆ†å¸ƒ:")
    for range_name, questions in score_ranges.items():
        if questions:
            print(f"   {range_name}åˆ†: {len(questions)} é¢˜")
    print()
    
    # è¯¦ç»†åˆ—è¡¨
    print(f"{'='*70}")
    print(f"è¯¦ç»†åˆ—è¡¨ï¼ˆæŒ‰åˆ†æ•°ä»ä½åˆ°é«˜ï¼‰")
    print(f"{'='*70}\n")
    
    for i, q in enumerate(with_answer_sorted, 1):
        print(f"{i}. [{q['question_id']}] å¾—åˆ†: {q['gpt5_result']['score']}/100")
        print(f"   éš¾åº¦ {q['difficulty']} | {q['topic']} | {q['type']}")
        print(f"   GPT-5å›ç­”é•¿åº¦: {q['gpt5_result']['answer_length']} chars")
        print(f"   é—®é¢˜: {q['question_text'][:120]}...")
        print(f"   æ ‡å‡†ç­”æ¡ˆ: {q['standard_answer'][:120]}...")
        print()
    
    # ç»Ÿè®¡åˆ†æ
    print(f"{'='*70}")
    print(f"ç»Ÿè®¡åˆ†æ")
    print(f"{'='*70}\n")
    
    # å¹³å‡å›ç­”é•¿åº¦
    avg_length = sum(q['gpt5_result']['answer_length'] for q in with_answer) / len(with_answer)
    print(f"å¹³å‡å›ç­”é•¿åº¦: {avg_length:.0f} å­—ç¬¦\n")
    
    # æŒ‰éš¾åº¦ç»Ÿè®¡
    by_difficulty = defaultdict(list)
    for q in with_answer:
        by_difficulty[q['difficulty']].append(q)
    
    print(f"æŒ‰éš¾åº¦åˆ†å¸ƒ:")
    for diff in sorted(by_difficulty.keys()):
        questions = by_difficulty[diff]
        avg_score = sum(q['gpt5_result']['score'] for q in questions) / len(questions)
        print(f"   éš¾åº¦ {diff}: {len(questions)} é¢˜, å¹³å‡åˆ† {avg_score:.1f}")
    print()
    
    # æŒ‰ä¸»é¢˜ç»Ÿè®¡
    by_topic = defaultdict(list)
    for q in with_answer:
        by_topic[q['topic']].append(q)
    
    print(f"æŒ‰ä¸»é¢˜åˆ†å¸ƒ (Top 10):")
    topic_sorted = sorted(by_topic.items(), key=lambda x: len(x[1]), reverse=True)[:10]
    for topic, questions in topic_sorted:
        avg_score = sum(q['gpt5_result']['score'] for q in questions) / len(questions)
        print(f"   {topic}: {len(questions)} é¢˜, å¹³å‡åˆ† {avg_score:.1f}")
    print()
    
    # æ‰¾å‡ºæœ€å·®çš„10é¢˜ï¼ˆæœ‰å›ç­”ä½†å¾—åˆ†æœ€ä½ï¼‰
    print(f"{'='*70}")
    print(f"ğŸ”¥ æœ€å·®çš„10é“é¢˜ï¼ˆæœ‰å®è´¨å›ç­”ä½†å¾—åˆ†æœ€ä½ï¼‰")
    print(f"{'='*70}\n")
    
    worst_10 = with_answer_sorted[:10]
    for i, q in enumerate(worst_10, 1):
        print(f"{i}. [{q['question_id']}] å¾—åˆ†: {q['gpt5_result']['score']}/100")
        print(f"   éš¾åº¦ {q['difficulty']} | {q['topic']} | {q['type']}")
        print(f"   å›ç­”é•¿åº¦: {q['gpt5_result']['answer_length']} chars")
        print(f"   é—®é¢˜: {q['question_text'][:100]}...")
        print()
    
    # ä¿å­˜ç­›é€‰ç»“æœ
    output = {
        'summary': {
            'total_incorrect': len(incorrect),
            'with_substantial_answer': len(with_answer),
            'without_answer_or_too_short': len(without_answer),
            'min_length_threshold': MIN_LENGTH,
            'note': 'è¿™äº›é¢˜ç›®GPT-5ç»™å‡ºäº†åˆç†é•¿åº¦çš„å›ç­”ï¼ˆâ‰¥500å­—ç¬¦ï¼‰ï¼Œä½†ä»è¢«åˆ¤é”™ã€‚è¯´æ˜æ˜¯çœŸæ­£çš„çŸ¥è¯†é”™è¯¯ï¼Œè€ŒéAPIå¤±è´¥ã€‚'
        },
        'statistics': {
            'average_answer_length': round(avg_length, 0),
            'score_distribution': {
                range_name: len(questions)
                for range_name, questions in score_ranges.items()
                if questions
            },
            'by_difficulty': {
                str(diff): {
                    'count': len(questions),
                    'avg_score': round(sum(q['gpt5_result']['score'] for q in questions) / len(questions), 1)
                }
                for diff, questions in by_difficulty.items()
            },
            'by_topic': {
                topic: {
                    'count': len(questions),
                    'avg_score': round(sum(q['gpt5_result']['score'] for q in questions) / len(questions), 1)
                }
                for topic, questions in topic_sorted
            }
        },
        'questions_with_substantial_answers': [
            {
                'question_id': q['question_id'],
                'score': q['gpt5_result']['score'],
                'answer_length': q['gpt5_result']['answer_length'],
                'difficulty': q['difficulty'],
                'topic': q['topic'],
                'type': q['type'],
                'question_text': q['question_text'],
                'standard_answer': q['standard_answer'],
                'original_text': q.get('original_text', {})
            }
            for q in with_answer_sorted
        ]
    }
    
    output_path = r'c:\Users\13031\Desktop\workspace\distillation_generation\éªŒè¯\gpt5_real_errors.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"{'='*70}")
    print(f"ğŸ’¾ ç­›é€‰ç»“æœå·²ä¿å­˜è‡³: {output_path}")
    print(f"{'='*70}\n")
    
    # å¯¹æ¯”åˆ†æ
    print(f"{'='*70}")
    print(f"å¯¹æ¯”åˆ†æ: çœŸå®é”™è¯¯ vs APIå¤±è´¥")
    print(f"{'='*70}\n")
    
    print(f"ğŸ“Š çœŸå®çš„GPT-5é”™è¯¯ (æœ‰å®è´¨å›ç­”):")
    print(f"   æ•°é‡: {len(with_answer)} é¢˜")
    print(f"   å æ€»é”™è¯¯: {len(with_answer)/len(incorrect)*100:.1f}%")
    print(f"   å¹³å‡å›ç­”é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
    print(f"   è¯´æ˜: GPT-5è®¤çœŸå›ç­”äº†ï¼Œä½†ç­”é”™äº†\n")
    
    print(f"âŒ å¯èƒ½çš„APIå¤±è´¥/ç©ºå›ç­”:")
    print(f"   æ•°é‡: {len(without_answer)} é¢˜")
    print(f"   å æ€»é”™è¯¯: {len(without_answer)/len(incorrect)*100:.1f}%")
    print(f"   è¯´æ˜: å¯èƒ½æ˜¯ä½™é¢ä¸è¶³ã€APIå¤±è´¥ã€æˆ–å›ç­”è¢«æˆªæ–­\n")
    
    print(f"ğŸ’¡ å…³é”®å‘ç°:")
    print(f"   1. æœ‰ {len(with_answer)} é¢˜æ˜¯GPT-5çœŸæ­£ç­”é”™çš„")
    print(f"   2. æœ‰ {len(without_answer)} é¢˜å¯èƒ½å› æŠ€æœ¯åŸå› å¤±è´¥")
    print(f"   3. çœŸå®é”™è¯¯ç‡: {len(with_answer)/872*100:.2f}% (åŸºäº872é¢˜æ€»æ•°)")
    print(f"   4. å¦‚æœæ’é™¤APIå¤±è´¥ï¼ŒGPT-5å®é™…æ­£ç¡®ç‡å¯èƒ½æ›´é«˜\n")
    
    # ç»Ÿè®¡æ— å›ç­”é¢˜ç›®çš„åˆ†æ•°åˆ†å¸ƒ
    without_answer_scores = defaultdict(int)
    for q in without_answer:
        score = q['gpt5_result']['score']
        if score == 0:
            without_answer_scores['0åˆ†'] += 1
        elif score <= 40:
            without_answer_scores['1-40åˆ†'] += 1
        elif score <= 80:
            without_answer_scores['41-80åˆ†'] += 1
    
    print(f"æ— å›ç­”/å›ç­”å¤ªçŸ­çš„é¢˜ç›®åˆ†æ•°åˆ†å¸ƒ:")
    for range_name, count in sorted(without_answer_scores.items()):
        print(f"   {range_name}: {count} é¢˜")
    print(f"   â†’ æ³¨æ„: {without_answer_scores.get('0åˆ†', 0)} é¢˜å¾—0åˆ†ï¼Œå¾ˆå¯èƒ½æ˜¯APIå¤±è´¥\n")
    
    print(f"{'='*70}\n")

if __name__ == "__main__":
    main()

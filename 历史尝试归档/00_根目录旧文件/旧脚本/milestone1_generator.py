"""
Milestone 1: åŸºäºPECSæ–‡çŒ®çš„è‡ªåŠ¨å‡ºé¢˜éªŒè¯
ç›®æ ‡ï¼šä»1ç¯‡è®ºæ–‡ç”Ÿæˆ20é“é«˜è´¨é‡é—®é¢˜
"""

import json
import logging
import os
import uuid
from datetime import datetime
from typing import List, Dict, Any
from litellm import completion

from src.utils import load_config, setup_logging, load_env_variables
from src.models import QuestionUnit

logger = logging.getLogger(__name__)


class Milestone1Generator:
    """Milestone 1 é¢˜ç›®ç”Ÿæˆå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.model_name = config.get("generation_model", "gemini/gemini-2.5-flash")
        self.paper_path = config.get("paper_path", "main.txt")
        self.output_path = config.get("output_path", "data/milestone1_candidates.jsonl")
        self.report_path = config.get("report_path", "data/milestone1_report.md")
        
    def load_paper_text(self) -> str:
        """åŠ è½½è®ºæ–‡æ–‡æœ¬"""
        logger.info(f"æ­£åœ¨è¯»å–è®ºæ–‡: {self.paper_path}")
        
        if not os.path.exists(self.paper_path):
            raise FileNotFoundError(f"è®ºæ–‡æ–‡ä»¶ä¸å­˜åœ¨: {self.paper_path}")
        
        with open(self.paper_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        logger.info(f"è®ºæ–‡åŠ è½½æˆåŠŸï¼Œå…± {len(text)} å­—ç¬¦")
        return text
    
    def build_generation_prompt(self, paper_text: str) -> str:
        """æ„å»ºç”Ÿæˆæç¤ºè¯"""
        prompt = f"""# ROLE
ä½ æ˜¯ç‡ƒçƒ§ç§‘å­¦å’Œå·¥ç¨‹çƒ­ç‰©ç†é¢†åŸŸçš„èµ„æ·±ä¸“å®¶ï¼Œæ“…é•¿åŸºäºç§‘ç ”æ–‡çŒ®è®¾è®¡é«˜è´¨é‡çš„æ•™å­¦é¢˜ç›®ã€‚

# TASK
åŸºäºä»¥ä¸‹PECSï¼ˆProgress in Energy and Combustion Scienceï¼‰ç»¼è¿°è®ºæ–‡ï¼Œç”Ÿæˆ**20é“**é«˜è´¨é‡é—®é¢˜ã€‚

## é¢˜ç›®è¦æ±‚ï¼š
1. **åŸºäºè®ºæ–‡å†…å®¹**ï¼šé—®é¢˜å¿…é¡»æºäºè®ºæ–‡çš„æ ¸å¿ƒæ¦‚å¿µã€ç†è®ºæˆ–å®éªŒå‘ç°
2. **æ·±åº¦ä¼˜å…ˆ**ï¼šé¿å…ç®€å•å®šä¹‰é¢˜ï¼Œåº”æµ‹è¯•å¯¹æ¦‚å¿µçš„æ·±å±‚ç†è§£å’Œåº”ç”¨èƒ½åŠ›
3. **ç±»å‹å¤šæ ·**ï¼š
   - **conceptï¼ˆæ¦‚å¿µç†è§£ï¼‰**ï¼šæµ‹è¯•å¯¹å…³é”®æ¦‚å¿µçš„ç†è§£
   - **reasoningï¼ˆæ¨ç†åˆ†æï¼‰**ï¼šéœ€è¦é€»è¾‘æ¨ç†å’Œå› æœåˆ†æ
   - **applicationï¼ˆåº”ç”¨ï¼‰**ï¼šå°†ç†è®ºåº”ç”¨åˆ°å®é™…åœºæ™¯
   - **calculationï¼ˆè®¡ç®—ï¼‰**ï¼šæ¶‰åŠå®šé‡åˆ†æå’Œè®¡ç®—ï¼ˆå¦‚é€‚ç”¨ï¼‰

4. **éš¾åº¦åˆ†çº§**ï¼ˆ1-5ï¼‰ï¼š
   - 1-2: åŸºç¡€æ¦‚å¿µ
   - 3: ä¸­ç­‰éš¾åº¦ï¼Œéœ€è¦ç»¼åˆç†è§£
   - 4-5: é«˜éš¾åº¦ï¼Œéœ€è¦æ·±å…¥åˆ†ææˆ–è·¨é¢†åŸŸçŸ¥è¯†

5. **æ ‡å‡†ç­”æ¡ˆ**ï¼šå¿…é¡»è¯¦ç»†ã€å‡†ç¡®ï¼ŒåŒ…å«ï¼š
   - æ ¸å¿ƒè®ºç‚¹
   - æ”¯æ’‘ç†ç”±/æœºç†è§£é‡Š
   - å¿…è¦çš„å…¬å¼æˆ–æ•°æ®ï¼ˆå¦‚æœ‰ï¼‰

## è®ºæ–‡å†…å®¹ï¼š
{paper_text[:50000]}  

# OUTPUT FORMAT
è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«questionsæ•°ç»„ï¼Œæ¯ä¸ªé—®é¢˜åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```json
{{
  "questions": [
    {{
      "topic": "å…·ä½“å­é¢†åŸŸï¼ˆå¦‚ignition_theory, flame_propagationç­‰ï¼‰",
      "difficulty": 1-5,
      "type": "concept/reasoning/application/calculation",
      "question_text": "é—®é¢˜æ–‡æœ¬ï¼ˆä¸­æ–‡æˆ–è‹±æ–‡ï¼‰",
      "standard_answer": "è¯¦ç»†æ ‡å‡†ç­”æ¡ˆï¼ˆä¸­æ–‡æˆ–è‹±æ–‡ï¼‰"
    }}
  ]
}}
```

**é‡è¦**ï¼š
- åªè¿”å›JSONï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ã€è§£é‡Šæˆ–markdownä»£ç å—
- ç¡®ä¿ç”Ÿæˆæ­£å¥½20é“é¢˜
- æ ‡å‡†ç­”æ¡ˆå¿…é¡»è¶³å¤Ÿè¯¦ç»†ï¼ˆè‡³å°‘100å­—ï¼‰
"""
        return prompt
    
    def generate_questions_from_paper(self, paper_text: str) -> List[Dict[str, Any]]:
        """è°ƒç”¨LLMç”Ÿæˆé¢˜ç›®ï¼ˆä»…AIå­—æ®µï¼‰"""
        logger.info("=" * 60)
        logger.info("æ­¥éª¤1: è°ƒç”¨LLMç”Ÿæˆé¢˜ç›®")
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {self.model_name}")
        logger.info("=" * 60)
        
        prompt = self.build_generation_prompt(paper_text)
        
        # å‡†å¤‡æ¨¡å‹å›é€€åˆ—è¡¨
        model_fallbacks = [
            {'model': self.model_name},
            {'model': 'openai/deepseek-ai/DeepSeek-V3', 
             'api_base': 'https://api.siliconflow.cn/v1', 
             'api_key': os.environ.get('SILICONFLOW_API_KEY')}
        ]
        
        # å°è¯•æ‰€æœ‰æ¨¡å‹
        for attempt in range(2):
            for model_index, m in enumerate(model_fallbacks):
                try:
                    if attempt > 0 or model_index > 0:
                        logger.info(f"ç¬¬{attempt+1}è½®ï¼Œå°è¯•æ¨¡å‹: {m['model']}")
                    
                    # æ„å»ºcompletionå‚æ•°
                    completion_kwargs = {
                        "model": m['model'],
                        "messages": [{"role": "user", "content": prompt}],
                        "temperature": 0.8,
                        "max_tokens": 8000,
                        "timeout": 180
                    }
                    
                    # æ·»åŠ JSONæ ¼å¼å¼ºåˆ¶ï¼ˆå¯¹æ”¯æŒçš„æ¨¡å‹ï¼‰
                    if "gemini" in m['model'].lower() or "openai" in m['model']:
                        completion_kwargs["response_format"] = {"type": "json_object"}
                    
                    if 'api_base' in m and m['api_base']:
                        completion_kwargs['api_base'] = m['api_base']
                    if 'api_key' in m and m['api_key']:
                        completion_kwargs['api_key'] = m['api_key']
                    
                    # è°ƒç”¨LiteLLM
                    response = completion(**completion_kwargs)
                    
                    # æ£€æŸ¥å“åº”
                    if not response or not response.choices:
                        logger.warning(f"æ¨¡å‹ {m['model']} è¿”å›äº†ç©ºå“åº”")
                        continue
                    
                    message = response.choices[0].message
                    if not message.content:
                        logger.warning(f"æ¨¡å‹ {m['model']} è¿”å›å†…å®¹ä¸ºç©º")
                        continue
                    
                    response_text = message.content.strip()
                    
                    # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                    if response_text.startswith("```json"):
                        response_text = response_text[7:]
                    if response_text.startswith("```"):
                        response_text = response_text[3:]
                    if response_text.endswith("```"):
                        response_text = response_text[:-3]
                    
                    response_text = response_text.strip()
                    
                    # ä¿å­˜åŸå§‹å“åº”ç”¨äºè°ƒè¯•
                    debug_path = "data/milestone1_raw_response.txt"
                    os.makedirs(os.path.dirname(debug_path), exist_ok=True)
                    with open(debug_path, 'w', encoding='utf-8') as f:
                        f.write(response_text)
                    logger.info(f"åŸå§‹å“åº”å·²ä¿å­˜è‡³: {debug_path}")
                    
                    # è§£æJSON
                    try:
                        data = json.loads(response_text)
                    except json.JSONDecodeError as e:
                        logger.warning(f"JSONè§£æå¤±è´¥: {e}")
                        logger.warning(f"é”™è¯¯ä½ç½®: ç¬¬{e.lineno}è¡Œ, ç¬¬{e.colno}åˆ—")
                        
                        # å°è¯•ä¿®å¤å¸¸è§JSONé—®é¢˜
                        logger.info("å°è¯•è‡ªåŠ¨ä¿®å¤JSONæ ¼å¼...")
                        import re
                        # ç§»é™¤å¯èƒ½çš„å°¾éšé€—å·
                        response_text = re.sub(r',(\s*[}\]])', r'\1', response_text)
                        try:
                            data = json.loads(response_text)
                            logger.info("âœ“ JSONä¿®å¤æˆåŠŸ")
                        except:
                            logger.error(f"JSONä¿®å¤å¤±è´¥ï¼Œè·³è¿‡æ­¤æ¨¡å‹")
                            continue
                    
                    questions = data.get("questions", [])
                    
                    if not questions:
                        logger.warning(f"æ¨¡å‹ {m['model']} è¿”å›çš„questionsä¸ºç©º")
                        continue
                    
                    logger.info(f"âœ… ä½¿ç”¨æ¨¡å‹ {m['model']} æˆåŠŸç”Ÿæˆ {len(questions)} é“é¢˜ç›®")
                    return questions
                    
                except Exception as e:
                    logger.warning(f"âŒ æ¨¡å‹ {m['model']} ç¬¬{attempt+1}è½®å¤±è´¥: {e}")
                    import traceback
                    logger.debug(traceback.format_exc())
                    continue
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        raise RuntimeError("æ‰€æœ‰æ¨¡å‹å’Œé‡è¯•éƒ½å¤±è´¥äº†ï¼Œæ— æ³•ç”Ÿæˆé¢˜ç›®")
    
    def wrap_with_metadata(self, questions: List[Dict[str, Any]], paper_id: str = "PECS_2020_Vol85_p1", 
                          paper_title: str = "Combustion Theory Review") -> List[Dict[str, Any]]:
        """æ·»åŠ ç³»ç»Ÿmetadataå­—æ®µ"""
        logger.info("=" * 60)
        logger.info("æ­¥éª¤2: æ·»åŠ ç³»ç»Ÿmetadata")
        logger.info("=" * 60)
        
        wrapped_questions = []
        
        for q in questions:
            wrapped = {
                # ç³»ç»Ÿç”Ÿæˆå­—æ®µ
                "question_id": f"comb_qa_{str(uuid.uuid4())[:8]}",
                
                # AIç”Ÿæˆå­—æ®µï¼ˆä¿æŒåŸæ ·ï¼‰
                "question_text": q.get("question_text", ""),
                "standard_answer": q.get("standard_answer", ""),
                "type": q.get("type", "reasoning"),
                "difficulty": q.get("difficulty", 3),
                "topic": q.get("topic", "general_combustion"),
                
                # æ¥æºä¿¡æ¯
                "source": {
                    "type": "with_reference",
                    "paper_id": paper_id,
                    "paper_title": paper_title
                },
                
                # å…ƒæ•°æ®
                "metadata": {
                    "generation_model": self.model_name,
                    "created_at": datetime.now().isoformat(),
                    "milestone": "milestone_1"
                }
            }
            wrapped_questions.append(wrapped)
        
        logger.info(f"âœ“ æˆåŠŸåŒ…è£… {len(wrapped_questions)} é“é¢˜ç›®")
        return wrapped_questions
    
    def save_questions(self, questions: List[Dict[str, Any]]) -> None:
        """ä¿å­˜é¢˜ç›®åˆ°JSONLæ–‡ä»¶"""
        logger.info("=" * 60)
        logger.info("æ­¥éª¤3: ä¿å­˜é¢˜ç›®")
        logger.info("=" * 60)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)
        
        with open(self.output_path, 'w', encoding='utf-8') as f:
            for q in questions:
                f.write(json.dumps(q, ensure_ascii=False) + '\n')
        
        logger.info(f"âœ“ é¢˜ç›®å·²ä¿å­˜è‡³: {self.output_path}")
        logger.info(f"  å…± {len(questions)} é“é¢˜")
    
    def generate_quality_report(self, questions: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆè´¨é‡è¯„ä¼°æŠ¥å‘Š"""
        logger.info("=" * 60)
        logger.info("æ­¥éª¤4: ç”Ÿæˆè´¨é‡è¯„ä¼°æŠ¥å‘Š")
        logger.info("=" * 60)
        
        # ç»Ÿè®¡åˆ†æ
        total = len(questions)
        
        # ç±»å‹åˆ†å¸ƒ
        type_count = {}
        for q in questions:
            qtype = q.get("type", "unknown")
            type_count[qtype] = type_count.get(qtype, 0) + 1
        
        # éš¾åº¦åˆ†å¸ƒ
        difficulty_count = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
        for q in questions:
            diff = q.get("difficulty", 3)
            if diff in difficulty_count:
                difficulty_count[diff] += 1
        
        # ä¸»é¢˜åˆ†å¸ƒ
        topic_count = {}
        for q in questions:
            topic = q.get("topic", "unknown")
            topic_count[topic] = topic_count.get(topic, 0) + 1
        
        # ç­”æ¡ˆé•¿åº¦åˆ†æ
        answer_lengths = [len(q.get("standard_answer", "")) for q in questions]
        avg_length = sum(answer_lengths) / len(answer_lengths) if answer_lengths else 0
        min_length = min(answer_lengths) if answer_lengths else 0
        max_length = max(answer_lengths) if answer_lengths else 0
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report = f"""# Milestone 1 è´¨é‡è¯„ä¼°æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**ä½¿ç”¨æ¨¡å‹**: {self.model_name}  
**æ•°æ®æ¥æº**: {self.paper_path}

---

## ğŸ“Š æ•´ä½“ç»Ÿè®¡

- **æ€»é¢˜ç›®æ•°**: {total} é“
- **ç›®æ ‡æ•°é‡**: 20 é“
- **å®Œæˆç‡**: {total/20*100:.1f}%

---

## ğŸ¯ é¢˜ç›®ç±»å‹åˆ†å¸ƒ

| ç±»å‹ | æ•°é‡ | å æ¯” |
|------|------|------|
"""
        for qtype, count in sorted(type_count.items()):
            percentage = count / total * 100
            report += f"| {qtype} | {count} | {percentage:.1f}% |\n"
        
        report += f"""
---

## ğŸ“ˆ éš¾åº¦åˆ†å¸ƒ

| éš¾åº¦ç­‰çº§ | æ•°é‡ | å æ¯” | è¯´æ˜ |
|----------|------|------|------|
| 1 | {difficulty_count[1]} | {difficulty_count[1]/total*100:.1f}% | åŸºç¡€ |
| 2 | {difficulty_count[2]} | {difficulty_count[2]/total*100:.1f}% | ç®€å• |
| 3 | {difficulty_count[3]} | {difficulty_count[3]/total*100:.1f}% | ä¸­ç­‰ |
| 4 | {difficulty_count[4]} | {difficulty_count[4]/total*100:.1f}% | å›°éš¾ |
| 5 | {difficulty_count[5]} | {difficulty_count[5]/total*100:.1f}% | æéš¾ |

---

## ğŸ”¬ ä¸»é¢˜åˆ†å¸ƒ

"""
        for topic, count in sorted(topic_count.items(), key=lambda x: x[1], reverse=True):
            percentage = count / total * 100
            report += f"- **{topic}**: {count} é“ ({percentage:.1f}%)\n"
        
        report += f"""
---

## ğŸ“ ç­”æ¡ˆè´¨é‡åˆ†æ

- **å¹³å‡ç­”æ¡ˆé•¿åº¦**: {avg_length:.0f} å­—ç¬¦
- **æœ€çŸ­ç­”æ¡ˆ**: {min_length} å­—ç¬¦
- **æœ€é•¿ç­”æ¡ˆ**: {max_length} å­—ç¬¦

"""
        
        # è´¨é‡è¯„ä¼°ï¼ˆç®€å•è§„åˆ™ï¼‰
        short_answers = [i+1 for i, q in enumerate(questions) if len(q.get("standard_answer", "")) < 100]
        if short_answers:
            report += f"""
### âš ï¸ æ½œåœ¨é—®é¢˜

- **ç­”æ¡ˆè¿‡çŸ­çš„é¢˜ç›®** (< 100å­—ç¬¦): ç¬¬ {', '.join(map(str, short_answers[:5]))} é¢˜ç­‰ ({len(short_answers)} é“)
"""
        
        # å¯ç”¨æ€§ä¼°è®¡
        usable_count = sum(1 for q in questions if len(q.get("standard_answer", "")) >= 100)
        report += f"""
---

## âœ… éªŒæ”¶æ ‡å‡†æ£€æŸ¥

- [{'x' if total == 20 else ' '}] æˆåŠŸç”Ÿæˆ20é“å®Œæ•´çš„Q&A
- [{'x' if usable_count >= 15 else ' '}] è‡³å°‘15é“é¢˜ç¬¦åˆè´¨é‡æ ‡å‡†ï¼ˆç­”æ¡ˆâ‰¥100å­—ç¬¦ï¼‰
- [x] è¾“å‡ºä¸ºæ ‡å‡†JSONæ ¼å¼
- **é¢„ä¼°å¯ç”¨é¢˜ç›®**: {usable_count} / 20 é“

---

## ğŸ’¡ å»ºè®®

"""
        if usable_count >= 18:
            report += "âœ¨ **è´¨é‡ä¼˜ç§€**ï¼ç»å¤§å¤šæ•°é¢˜ç›®ç¬¦åˆæ ‡å‡†ï¼Œå¯ä»¥è¿›å…¥Milestone 2ã€‚\n"
        elif usable_count >= 15:
            report += "âœ“ **è´¨é‡åˆæ ¼**ã€‚å»ºè®®äººå·¥å®¡æ ¸ç­”æ¡ˆè¾ƒçŸ­çš„é¢˜ç›®ï¼Œç„¶åè¿›å…¥Milestone 2ã€‚\n"
        else:
            report += "âš ï¸ **éœ€è¦æ”¹è¿›**ã€‚å»ºè®®è°ƒæ•´promptæˆ–few-shotç¤ºä¾‹ï¼Œé‡æ–°ç”Ÿæˆã€‚\n"
        
        report += f"""
---

## ğŸ“‹ ç¤ºä¾‹é¢˜ç›®ï¼ˆå‰3é“ï¼‰

"""
        for i, q in enumerate(questions[:3], 1):
            report += f"""
### é¢˜ç›® {i}

- **ID**: `{q['question_id']}`
- **ç±»å‹**: {q['type']}
- **éš¾åº¦**: {q['difficulty']}/5
- **ä¸»é¢˜**: {q['topic']}

**é—®é¢˜**:  
{q['question_text']}

**æ ‡å‡†ç­”æ¡ˆ**:  
{q['standard_answer'][:200]}{'...' if len(q['standard_answer']) > 200 else ''}

---
"""
        
        return report
    
    def save_report(self, report: str) -> None:
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
        os.makedirs(os.path.dirname(self.report_path), exist_ok=True)
        
        with open(self.report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"âœ“ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜è‡³: {self.report_path}")
    
    def run(self) -> None:
        """è¿è¡ŒMilestone 1å®Œæ•´æµç¨‹"""
        logger.info("=" * 60)
        logger.info("ğŸš€ Milestone 1: åŸºäºPECSæ–‡çŒ®çš„è‡ªåŠ¨å‡ºé¢˜éªŒè¯")
        logger.info("=" * 60)
        
        try:
            # Step 1: åŠ è½½è®ºæ–‡
            paper_text = self.load_paper_text()
            
            # Step 2: ç”Ÿæˆé¢˜ç›®ï¼ˆAIå­—æ®µï¼‰
            ai_questions = self.generate_questions_from_paper(paper_text)
            
            # Step 3: åŒ…è£…metadata
            full_questions = self.wrap_with_metadata(
                ai_questions,
                paper_id="PECS_combustion_review",
                paper_title="Combustion Science Review"
            )
            
            # Step 4: ä¿å­˜é¢˜ç›®
            self.save_questions(full_questions)
            
            # Step 5: ç”ŸæˆæŠ¥å‘Š
            report = self.generate_quality_report(full_questions)
            self.save_report(report)
            
            # åœ¨æ§åˆ¶å°ä¹Ÿæ‰“å°æŠ¥å‘Š
            print("\n" + report)
            
            logger.info("=" * 60)
            logger.info("âœ… Milestone 1 å®Œæˆï¼")
            logger.info("=" * 60)
            logger.info(f"ğŸ“„ é¢˜ç›®æ–‡ä»¶: {self.output_path}")
            logger.info(f"ğŸ“Š æŠ¥å‘Šæ–‡ä»¶: {self.report_path}")
            
        except Exception as e:
            logger.error(f"âŒ Milestone 1 æ‰§è¡Œå¤±è´¥: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®
    config = load_config()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(
        log_level=config.get("log_level", "INFO"),
        log_file="logs/milestone1.log"
    )
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_env_variables(config.get("env_file_path", ".env"))
    
    # Milestone 1 é…ç½®
    m1_config = {
        "generation_model": config.get("generation_model", "gemini/gemini-2.5-flash"),
        "paper_path": "main.txt",
        "output_path": "data/milestone1_candidates.jsonl",
        "report_path": "data/milestone1_report.md"
    }
    
    # è¿è¡Œ
    generator = Milestone1Generator(m1_config)
    generator.run()


if __name__ == "__main__":
    main()
